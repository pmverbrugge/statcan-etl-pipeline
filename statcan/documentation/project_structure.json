{
  "generated_at": "2025-06-21T03:45:10.654841",
  "generator": "ETL Container Structure Scanner",
  "note": "Sensitive files and large data directories excluded",
  "structures": {
    "/app/statcan": {
      "documentation": {
        "type": "directory",
        "contents": {
          "complete_schema.sql": {
            "size": 33282,
            "modified": "2025-06-20T15:33:18.373485",
            "type": "file",
            "content": "pg_dump: last built-in OID is 16383\npg_dump: reading extensions\npg_dump: identifying extension members\npg_dump: reading schemas\npg_dump: reading user-defined tables\npg_dump: reading user-defined functions\npg_dump: reading user-defined types\npg_dump: reading procedural languages\npg_dump: reading user-defined aggregate functions\npg_dump: reading user-defined operators\npg_dump: reading user-defined access methods\npg_dump: reading user-defined operator classes\npg_dump: reading user-defined operator families\npg_dump: reading user-defined text search parsers\npg_dump: reading user-defined text search templates\npg_dump: reading user-defined text search dictionaries\npg_dump: reading user-defined text search configurations\npg_dump: reading user-defined foreign-data wrappers\npg_dump: reading user-defined foreign servers\npg_dump: reading default privileges\npg_dump: reading user-defined collations\npg_dump: reading user-defined conversions\npg_dump: reading type casts\npg_dump: reading transforms\npg_dump: reading table inheritance information\npg_dump: reading event triggers\npg_dump: finding extension tables\npg_dump: finding inheritance relationships\npg_dump: reading column info for interesting tables\npg_dump: finding table default expressions\npg_dump: finding table check constraints\npg_dump: flagging inherited columns in subtables\npg_dump: reading partitioning data\npg_dump: reading indexes\npg_dump: flagging indexes in partitioned tables\npg_dump: reading extended statistics\npg_dump: reading constraints\npg_dump: reading triggers\npg_dump: reading rewrite rules\npg_dump: reading policies\npg_dump: reading row-level security policies\npg_dump: reading publications\npg_dump: reading publication membership of tables\npg_dump: reading publication membership of schemas\npg_dump: reading subscriptions\npg_dump: reading dependency data\npg_dump: saving encoding = UTF8\npg_dump: saving standard_conforming_strings = on\npg_dump: saving search_path = \npg_dump: saving database definition\npg_dump: dropping DATABASE statcan\npg_dump: creating DATABASE \"statcan\"\npg_dump: connecting to new database \"statcan\"\npg_dump: creating SCHEMA \"cube\"\npg_dump: creating SCHEMA \"cube_data\"\npg_dump: creating SCHEMA \"dictionary\"\npg_dump: creating SCHEMA \"processing\"\npg_dump: creating SCHEMA \"raw_files\"\npg_dump: creating SCHEMA \"spine\"\npg_dump: creating EXTENSION \"pgcrypto\"\npg_dump: creating COMMENT \"EXTENSION pgcrypto\"\npg_dump: creating EXTENSION \"postgis\"\npg_dump: creating COMMENT \"EXTENSION postgis\"\npg_dump: creating EXTENSION \"uuid-ossp\"\npg_dump: creating COMMENT \"EXTENSION \"uuid-ossp\"\"\npg_dump: creating TABLE \"processing.dimension_set\"\npg_dump: creating COMMENT \"processing.TABLE dimension_set\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.dimension_hash\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.dimension_name_en\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.dimension_name_fr\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.has_uom\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.usage_count\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.is_tree\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set.is_hetero\"\npg_dump: creating TABLE \"processing.dimension_set_members\"\npg_dump: creating COMMENT \"processing.TABLE dimension_set_members\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set_members.dimension_hash\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set_members.member_name_en\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set_members.member_name_fr\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set_members.usage_count\"\npg_dump: creating COMMENT \"processing.COLUMN dimension_set_members.tree_level\"\npg_dump: creating TABLE \"processing.processed_dimensions\"\npg_dump: creating COMMENT \"processing.TABLE processed_dimensions\"\npg_dump: creating COMMENT \"processing.COLUMN processed_dimensions.dimension_hash\"\npg_dump: creating COMMENT \"processing.COLUMN processed_dimensions.has_uom\"\npg_dump: creating TABLE \"processing.processed_members\"\npg_dump: creating COMMENT \"processing.TABLE processed_members\"\npg_dump:--\n-- PostgreSQL database dump\n--\n\n-- Dumped from database version 15.8 (Debian 15.8-1.pgdg110+1)\n-- Dumped by pg_dump version 15.8 (Debian 15.8-1.pgdg110+1)\n\n-- Started on 2025-06-20 15:33:18 UTC\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\nDROP DATABASE IF EXISTS statcan;\n--\n-- TOC entry 4432 (class 1262 OID 16384)\n-- Name: statcan; Type: DATABASE; Schema: -; Owner: -\n--\n\nCREATE DATABASE statcan WITH TEMPLATE = template0 ENCODING = 'UTF8' LOCALE_PROVIDER = libc LOCALE = 'en_US.utf8';\n\n\n\\connect statcan\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\n--\n-- TOC entry 10 (class 2615 OID 17533)\n-- Name: cube; Type: SCHEMA; Schema: -; Owner: -\n--\n\nCREATE SCHEMA cube;\n\n\n--\n-- TOC entry 14 (class 2615 OID 26440)\n-- Name: cube_data; Type: SCHEMA; Schema: -; Owner: -\n--\n\nCREATE SCHEMA cube_data;\n\n\n--\n-- TOC entry 12 (class 2615 OID 17536)\n-- Name: dictionary; Type: SCHEMA; Schema: -; Owner: -\n--\n\nCREATE SCHEMA dictionary;\n\n\n--\n-- TOC entry 9 (class 2615 OID 17534)\n-- Name: processing; Type: SCHEMA; Schema: -; Owner: -\n--\n\nCREATE SCHEMA processing;\n\n\n--\n-- TOC entry 13 (class 2615 OID 17532)\n-- Name: raw_files; Type: SCHEMA; Schema: -; Owner: -\n--\n\nCREATE SCHEMA raw_files;\n\n\n--\n-- TOC entry 11 (class 2615 OID 17535)\n-- Name: spine; Type: SCHEMA; Schema: -; Owner: -\n--\n\nCREATE SCHEMA spine;\n\n\n--\n-- TOC entry 4 (class 3079 OID 17474)\n-- Name: pgcrypto; Type: EXTENSION; Schema: -; Owner: -\n--\n\nCREATE EXTENSION IF NOT EXISTS pgcrypto WITH SCHEMA public;\n\n\n--\n-- TOC entry 4433 (class 0 OID 0)\n-- Dependencies: 4\n-- Name: EXTENSION pgcrypto; Type: COMMENT; Schema: -; Owner: -\n--\n\nCOMMENT ON EXTENSION pgcrypto IS 'cryptographic functions';\n\n\n--\n-- TOC entry 2 (class 3079 OID 16385)\n-- Name: postgis; Type: EXTENSION; Schema: -; Owner: -\n--\n\nCREATE EXTENSION IF NOT EXISTS postgis WITH SCHEMA public;\n\n\n--\n-- TOC entry 4434 (class 0 OID 0)\n-- Dependencies: 2\n-- Name: EXTENSION postgis; Type: COMMENT; Schema: -; Owner: -\n--\n\nCOMMENT ON EXTENSION postgis IS 'PostGIS geometry and geography spatial types and functions';\n\n\n--\n-- TOC entry 3 (class 3079 OID 17463)\n-- Name: uuid-ossp; Type: EXTENSION; Schema: -; Owner: -\n--\n\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\" WITH SCHEMA public;\n\n\n--\n-- TOC entry 4435 (class 0 OID 0)\n-- Dependencies: 3\n-- Name: EXTENSION \"uuid-ossp\"; Type: COMMENT; Schema: -; Owner: -\n--\n\nCOMMENT ON EXTENSION \"uuid-ossp\" IS 'generate universally unique identifiers (UUIDs)';\n\n\nSET default_tablespace = '';\n\nSET default_table_access_method = heap;\n\n--\n-- TOC entry 242 (class 1259 OID 27237)\n-- Name: dimension_set; Type: TABLE; Schema: processing; Owner: -\n--\n\nCREATE TABLE processing.dimension_set (\n    dimension_hash text NOT NULL,\n    dimension_name_en text,\n    dimension_name_fr text,\n    dimension_name_en_slug text,\n    dimension_name_fr_slug text,\n    has_uom boolean DEFAULT false,\n    usage_count integer DEFAULT 0,\n    created_at timestamp with time zone DEFAULT now(),\n    updated_at timestamp with time zone DEFAULT now(),\n    is_tree boolean DEFAULT false,\n    is_hetero boolean DEFAULT false\n);\n\n\n--\n-- TOC entry 4436 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: TABLE dimension_set; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON TABLE processing.dimension_set IS 'Canonical dimension definitions with most common labels and characteristics. Built from processed_dimensions in script 12.';\n\n\n--\n-- TOC entry 4437 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.dimension_hash; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.dimension_hash IS '12-character SHA-256 hash identifying this unique dimension structure';\n\n\n--\n-- TOC entry 4438 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.dimension_name_en; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.dimension_name_en IS 'Most common English dimension name in title case';\n\n\n--\n-- TOC entry 4439 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.dimension_name_fr; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.dimension_name_fr IS 'Most common French dimension name in title case';\n\n\n--\n-- TOC entry 4440 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.has_uom; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.has_uom IS 'True if any instance of this dimension contains unit of measure information';\n\n\n--\n-- TOC entry 4441 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.usage_count; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.usage_count IS 'Number of (productid, dimension_position) instances using this dimension_hash';\n\n\n--\n-- TOC entry 4442 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.is_tree; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.is_tree IS 'True if any members in this dimension have parent-child relationships (hierarchical structure)';\n\n\n--\n-- TOC entry 4443 (class 0 OID 0)\n-- Dependencies: 242\n-- Name: COLUMN dimension_set.is_hetero; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set.is_hetero IS 'True if members in this dimension have varying units of measure (heterogeneous UOM)';\n\n\n--\n-- TOC entry 243 (class 1259 OID 27335)\n-- Name: dimension_set_members; Type: TABLE; Schema: processing; Owner: -\n--\n\nCREATE TABLE processing.dimension_set_members (\n    dimension_hash text NOT NULL,\n    member_id integer NOT NULL,\n    member_name_en text,\n    member_name_fr text,\n    parent_member_id integer,\n    member_uom_code text,\n    usage_count integer DEFAULT 0,\n    created_at timestamp with time zone DEFAULT now(),\n    tree_level integer\n);\n\n\n--\n-- TOC entry 4444 (class 0 OID 0)\n-- Dependencies: 243\n-- Name: TABLE dimension_set_members; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON TABLE processing.dimension_set_members IS 'Canonical member definitions within each dimension. Built from processed_members in script 13.';\n\n\n--\n-- TOC entry 4445 (class 0 OID 0)\n-- Dependencies: 243\n-- Name: COLUMN dimension_set_members.dimension_hash; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set_members.dimension_hash IS 'Reference to canonical dimension hash';\n\n\n--\n-- TOC entry 4446 (class 0 OID 0)\n-- Dependencies: 243\n-- Name: COLUMN dimension_set_members.member_name_en; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set_members.member_name_en IS 'Most common English member name across all cubes using this dimension';\n\n\n--\n-- TOC entry 4447 (class 0 OID 0)\n-- Dependencies: 243\n-- Name: COLUMN dimension_set_members.member_name_fr; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set_members.member_name_fr IS 'Most common French member name across all cubes using this dimension';\n\n\n--\n-- TOC entry 4448 (class 0 OID 0)\n-- Dependencies: 243\n-- Name: COLUMN dimension_set_members.usage_count; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set_members.usage_count IS 'Number of cube instances where this member appears in this dimension';\n\n\n--\n-- TOC entry 4449 (class 0 OID 0)\n-- Dependencies: 243\n-- Name: COLUMN dimension_set_members.tree_level; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.dimension_set_members.tree_level IS 'Hierarchical level: 1 for root nodes (no parent), 2 for children of root nodes, etc. NULL for non-hierarchical dimensions.';\n\n\n--\n-- TOC entry 241 (class 1259 OID 27224)\n-- Name: processed_dimensions; Type: TABLE; Schema: processing; Owner: -\n--\n\nCREATE TABLE processing.processed_dimensions (\n    productid bigint NOT NULL,\n    dimension_position integer NOT NULL,\n    dimension_hash text NOT NULL,\n    dimension_name_en text,\n    dimension_name_fr text,\n    has_uom boolean,\n    created_at timestamp with time zone DEFAULT now()\n);\n\n\n--\n-- TOC entry 4450 (class 0 OID 0)\n-- Dependencies: 241\n-- Name: TABLE processed_dimensions; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON TABLE processing.processed_dimensions IS 'Mapping of (productid, dimension_position) to dimension_hash with raw dimension metadata. Built from processed_members + raw_dimension in script 11.';\n\n\n--\n-- TOC entry 4451 (class 0 OID 0)\n-- Dependencies: 241\n-- Name: COLUMN processed_dimensions.dimension_hash; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.processed_dimensions.dimension_hash IS '12-character SHA-256 hash of concatenated member hashes (sorted by member_id) within this dimension';\n\n\n--\n-- TOC entry 4452 (class 0 OID 0)\n-- Dependencies: 241\n-- Name: COLUMN processed_dimensions.has_uom; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.processed_dimensions.has_uom IS 'Indicates if this dimension contains unit of measure information';\n\n\n--\n-- TOC entry 240 (class 1259 OID 27132)\n-- Name: processed_members; Type: TABLE; Schema: processing; Owner: -\n--\n\nCREATE TABLE processing.processed_members (\n    productid bigint NOT NULL,\n    dimension_position integer NOT NULL,\n    member_id integer NOT NULL,\n    member_hash text NOT NULL,\n    member_name_en text,\n    member_name_fr text,\n    parent_member_id integer,\n    member_uom_code text,\n    classification_code text,\n    classification_type_code text,\n    geo_level integer,\n    vintage integer,\n    terminated boolean,\n    member_label_norm text,\n    created_at timestamp with time zone DEFAULT now(),\n    dimension_hash text\n);\n\n\n--\n-- TOC entry 4453 (class 0 OID 0)\n-- Dependencies: 240\n-- Name: TABLE processed_members; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON TABLE processing.processed_members IS 'Raw member data with computed member-level hashes. Input for dimension registry building (script 11+).';\n\n\n--\n-- TOC entry 4454 (class 0 OID 0)\n-- Dependencies: 240\n-- Name: COLUMN processed_members.member_hash; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.processed_members.member_hash IS '12-character SHA-256 hash of member_id + normalized_label_en + parent_id + uom_code';\n\n\n--\n-- TOC entry 4455 (class 0 OID 0)\n-- Dependencies: 240\n-- Name: COLUMN processed_members.member_label_norm; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.processed_members.member_label_norm IS 'Normalized (lowercase, trimmed) version of member_name_en for hashing consistency';\n\n\n--\n-- TOC entry 4456 (class 0 OID 0)\n-- Dependencies: 240\n-- Name: COLUMN processed_members.dimension_hash; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.processed_members.dimension_hash IS '12-character dimension hash populated by script 13 from processed_dimensions';\n\n\n--\n-- TOC entry 244 (class 1259 OID 27365)\n-- Name: raw_dimension; Type: TABLE; Schema: processing; Owner: -\n--\n\nCREATE TABLE processing.raw_dimension (\n    productid bigint NOT NULL,\n    dimension_position integer NOT NULL,\n    dimension_name_en text,\n    dimension_name_fr text,\n    has_uom boolean,\n    created_at timestamp with time zone DEFAULT now()\n);\n\n\n creating COMMENT \"processing.COLUMN processed_members.member_hash\"\npg_dump: creating COMMENT \"processing.COLUMN processed_members.member_label_norm\"\npg_dump: creating COMMENT \"processing.COLUMN processed_members.dimension_hash\"\npg_dump: creating TABLE \"processing.raw_dimension\"\npg_dump: creating COMMENT \"processing.TABLE raw_dimension\"\npg_dump: creating COMMENT \"processing.COLUMN raw_dimension.created_at\"\npg_dump: creating TABLE \"processing.raw_member\"\n--\n-- TOC entry 4457 (class 0 OID 0)\n-- Dependencies: 244\n-- Name: TABLE raw_dimension; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON TABLE processing.raw_dimension IS 'Raw dimension metadata from Statistics Canada API - processing schema';\n\n\n--\n-- TOC entry 4458 (class 0 OID 0)\n-- Dependencies: 244\n-- Name: COLUMN raw_dimension.created_at; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.raw_dimension.created_at IS 'Timestamp when record was created';\n\n\n--\n-- TOC entry 245 (class 1259 OID 27373)\n-- Name: raw_member; Type: TABLE; Schema: processing; Owner: -\n--\n\nCREATE TABLE processing.raw_member (\n    productid bigint NOT NULL,\n    dimension_position integer NOT NULL,\n    member_id integer NOT NULL,\n    parent_member_id integer,\n    classification_code text,\n    classification_type_code text,\n    member_name_en text,\n    member_name_fr text,\n    member_uom_code text,\n    geo_level integer,\n    vintage integer,\n    terminated integer,\n    created_at timestamp with time zone DEFAULT now()\n);\n\n\n--\n-- TOC entry 4459 (class 0 OID 0)\n-- Dependencies: 245\n-- Name: TABLE raw_member; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON TABLE processing.raw_member IS 'Raw member metadata from Statistics Canada API - processing schema';\n\n\npg_dump: creating COMMENT \"processing.TABLE raw_member\"\npg_dump: creating COMMENT \"processing.COLUMN raw_member.created_at\"\npg_dump: creating TABLE \"raw_files.changed_cubes_log\"\npg_dump: creating TABLE \"raw_files.cube_status\"\npg_dump: creating TABLE \"raw_files.manage_cube_raw_files\"\n--\n-- TOC entry 4460 (class 0 OID 0)\n-- Dependencies: 245\n-- Name: COLUMN raw_member.created_at; Type: COMMENT; Schema: processing; Owner: -\n--\n\nCOMMENT ON COLUMN processing.raw_member.created_at IS 'Timestamp when record was created';\n\n\n--\n-- TOC entry 236 (class 1259 OID 25947)\n-- Name: changed_cubes_log; Type: TABLE; Schema: raw_files; Owner: -\n--\n\nCREATE TABLE raw_files.changed_cubes_log (\n    productid bigint NOT NULL,\n    change_date date NOT NULL\n);\n\n\n--\n-- TOC entry 235 (class 1259 OID 25864)\n-- Name: cube_status; Type: TABLE; Schema: raw_files; Owner: -\n--\n\nCREATE TABLE raw_files.cube_status (\n    productid bigint NOT NULL,\n    last_download timestamp with time zone,\n    download_pending boolean DEFAULT false NOT NULL,\n    last_file_hash text\n);\n\n\n--\n-- TOC entry 232 (class 1259 OID 25732)\n-- Name: manage_cube_raw_files; Type: TABLE; Schema: raw_files; Owner: -\n--\n\nCREATE TABLE raw_files.manage_cube_raw_files (\n    id integer NOT NULL,\n    productid bigint NOT NULL,\n    file_hash text NOT NULL,\n    date_download timestamp with time zone DEFAULT now() NOT NULL,\n    active boolean DEFAULT false NOT NULL,\n    storage_location text NOT NULL\n);\n\n\n--\n-- TOC entry 231 (class 1259 OID 25731)\n-- Name: manage_cube_raw_files_id_seq; Type: SEQUENCE; Schema: raw_files; Owner: -\n--\n\nCREATE SEQUENCE raw_files.manage_cube_raw_files_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\n--\n-- TOC entry 4461 (class 0 OID 0)\n-- Dependencies: 231\n-- Name: manage_cube_raw_files_id_seq; Type: SEQUENCE OWNED BY; Schema: raw_files; Owner: -\n--\n\nALTER SEQUENCE raw_files.manage_cube_raw_files_id_seq OWNED BY raw_files.manage_cube_raw_files.id;\n\n\npg_dump: creating SEQUENCE \"raw_files.manage_cube_raw_files_id_seq\"\npg_dump: creating SEQUENCE OWNED BY \"raw_files.manage_cube_raw_files_id_seq\"\npg_dump: creating TABLE \"raw_files.manage_metadata_raw_files\"\npg_dump: creating SEQUENCE \"raw_files.manage_metadata_raw_files_id_seq\"\n--\n-- TOC entry 239 (class 1259 OID 26011)\n-- Name: manage_metadata_raw_files; Type: TABLE; Schema: raw_files; Owner: -\n--\n\nCREATE TABLE raw_files.manage_metadata_raw_files (\n    id integer NOT NULL,\n    productid bigint NOT NULL,\n    file_hash text NOT NULL,\n    date_download timestamp with time zone DEFAULT now() NOT NULL,\n    active boolean DEFAULT true NOT NULL,\n    storage_location text NOT NULL\n);\n\n\n--\n-- TOC entry 238 (class 1259 OID 26010)\n-- Name: manage_metadata_raw_files_id_seq; Type: SEQUENCE; Schema: raw_files; Owner: -\n--\n\nCREATE SEQUENCE raw_files.manage_metadata_raw_files_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\n--\n-- TOC entry 4462 (class 0 OID 0)\n-- Dependencies: 238\n-- Name: manage_metadata_raw_files_id_seq; Type: SEQUENCE OWNED BY; Schema: raw_files; Owner: -\n--\n\nALTER SEQUENCE raw_files.manage_metadata_raw_files_id_seq OWNED BY raw_files.manage_metadata_raw_files.id;\n\n\npg_dump: creating SEQUENCE OWNED BY \"raw_files.manage_metadata_raw_files_id_seq\"\npg_dump: creating TABLE \"raw_files.manage_spine_raw_files\"\npg_dump: creating SEQUENCE \"raw_files.manage_spine_raw_files_id_seq\"\npg_dump: creating SEQUENCE OWNED BY \"raw_files.manage_spine_raw_files_id_seq\"\n--\n-- TOC entry 234 (class 1259 OID 25745)\n-- Name: manage_spine_raw_files; Type: TABLE; Schema: raw_files; Owner: -\n--\n\nCREATE TABLE raw_files.manage_spine_raw_files (\n    id integer NOT NULL,\n    file_hash text NOT NULL,\n    date_download timestamp with time zone DEFAULT now() NOT NULL,\n    active boolean DEFAULT false NOT NULL,\n    storage_location text NOT NULL\n);\n\n\n--\n-- TOC entry 233 (class 1259 OID 25744)\n-- Name: manage_spine_raw_files_id_seq; Type: SEQUENCE; Schema: raw_files; Owner: -\n--\n\nCREATE SEQUENCE raw_files.manage_spine_raw_files_id_seq\n    AS integer\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\n\n--\n-- TOC entry 4463 (class 0 OID 0)\n-- Dependencies: 233\n-- Name: manage_spine_raw_files_id_seq; Type: SEQUENCE OWNED BY; Schema: raw_files; Owner: -\n--\n\nALTER SEQUENCE raw_files.manage_spine_raw_files_id_seq OWNED BY raw_files.manage_spine_raw_files.id;\n\n\npg_dump: creating TABLE \"raw_files.metadata_status\"\npg_dump: creating TABLE \"spine.cube\"\npg_dump: creating TABLE \"spine.cube_subject\"\npg_dump: creating TABLE \"spine.cube_survey\"\npg_dump: creating DEFAULT \"raw_files.manage_cube_raw_files id\"\npg_dump: creating DEFAULT \"raw_files.manage_metadata_raw_files id\"\npg_dump: creating DEFAULT \"raw_files.manage_spine_raw_files id\"\n--\n-- TOC entry 237 (class 1259 OID 25992)\n-- Name: metadata_status; Type: TABLE; Schema: raw_files; Owner: -\n--\n\nCREATE TABLE raw_files.metadata_status (\n    productid bigint NOT NULL,\n    last_download timestamp with time zone,\n    download_pending boolean DEFAULT true NOT NULL,\n    last_file_hash text\n);\n\n\n--\n-- TOC entry 228 (class 1259 OID 17611)\n-- Name: cube; Type: TABLE; Schema: spine; Owner: -\n--\n\nCREATE TABLE spine.cube (\n    productid bigint NOT NULL,\n    cansimid text,\n    cubetitleen text,\n    cubetitlefr text,\n    cubestartdate date,\n    cubeenddate date,\n    releasetime date,\n    archived smallint,\n    frequencycode smallint,\n    issuedate date\n);\n\n\n--\n-- TOC entry 229 (class 1259 OID 17625)\n-- Name: cube_subject; Type: TABLE; Schema: spine; Owner: -\n--\n\nCREATE TABLE spine.cube_subject (\n    productid bigint NOT NULL,\n    subjectcode text NOT NULL\n);\n\n\n--\n-- TOC entry 230 (class 1259 OID 17632)\n-- Name: cube_survey; Type: TABLE; Schema: spine; Owner: -\n--\n\nCREATE TABLE spine.cube_survey (\n    productid bigint NOT NULL,\n    surveycode text NOT NULL\n);\n\n\n--\n-- TOC entry 4213 (class 2604 OID 25735)\n-- Name: manage_cube_raw_files id; Type: DEFAULT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.manage_cube_raw_files ALTER COLUMN id SET DEFAULT nextval('raw_files.manage_cube_raw_files_id_seq'::regclass);\n\n\n--\n-- TOC entry 4221 (class 2604 OID 26014)\n-- Name: manage_metadata_raw_files id; Type: DEFAULT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.manage_metadata_raw_files ALTER COLUMN id SET DEFAULT nextval('raw_files.manage_metadata_raw_files_id_seq'::regclass);\n\n\n--\n-- TOC entry 4216 (class 2604 OID 25748)\n-- Name: manage_spine_raw_files id; Type: DEFAULT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.manage_spine_raw_files ALTER COLUMN id SET DEFAULT nextval('raw_files.manage_spine_raw_files_id_seq'::regclass);\n\n\npg_dump: creating CONSTRAINT \"processing.dimension_set_members dimension_set_members_pkey\"\npg_dump: creating CONSTRAINT \"processing.dimension_set dimension_set_pkey\"\n--\n-- TOC entry 4271 (class 2606 OID 27343)\n-- Name: dimension_set_members dimension_set_members_pkey; Type: CONSTRAINT; Schema: processing; Owner: -\n--\n\nALTER TABLE ONLY processing.dimension_set_members\n    ADD CONSTRAINT dimension_set_members_pkey PRIMARY KEY (dimension_hash, member_id);\n\n\n--\n-- TOC entry 4269 (class 2606 OID 27247)\n-- Name: dimension_set dimension_set_pkey; Type: CONSTRAINT; Schema: processing; Owner: -\n--\n\nALTER TABLE ONLY processing.dimension_set\n    ADD CONSTRAINT dimension_set_pkey PRIMARY KEY (dimension_hash);\n\n\npg_dump: creating CONSTRAINT \"processing.processed_dimensions processed_dimensions_pkey\"\n--\n-- TOC entry 4267 (class 2606 OID 27231)\n-- Name: processed_dimensions processed_dimensions_pkey; Type: CONSTRAINT; Schema: processing; Owner: -\n--\n\nALTER TABLE ONLY processing.processed_dimensions\n    ADD CONSTRAINT processed_dimensions_pkey PRIMARY KEY (productid, dimension_position);\n\n\npg_dump: creating CONSTRAINT \"processing.processed_members processed_members_pkey\"\n--\n-- TOC entry 4264 (class 2606 OID 27139)\n-- Name: processed_members processed_members_pkey; Type: CONSTRAINT; Schema: processing; Owner: -\n--\n\nALTER TABLE ONLY processing.processed_members\n    ADD CONSTRAINT processed_members_pkey PRIMARY KEY (productid, dimension_position, member_id);\n\n\npg_dump: creating CONSTRAINT \"processing.raw_dimension processing_raw_dimension_pkey\"\n--\n-- TOC entry 4275 (class 2606 OID 27372)\n-- Name: raw_dimension processing_raw_dimension_pkey; Type: CONSTRAINT; Schema: processing; Owner: -\n--\n\nALTER TABLE ONLY processing.raw_dimension\n    ADD CONSTRAINT processing_raw_dimension_pkey PRIMARY KEY (productid, dimension_position);\n\n\npg_dump: creating CONSTRAINT \"processing.raw_member processing_raw_member_pkey\"\npg_dump: creating CONSTRAINT \"raw_files.changed_cubes_log changed_cubes_log_pkey\"\npg_dump: creating CONSTRAINT \"raw_files.cube_status cube_status_pkey\"\npg_dump: creating CONSTRAINT \"raw_files.manage_cube_raw_files manage_cube_raw_files_pkey\"\npg_dump: creating CONSTRAINT \"raw_files.manage_metadata_raw_files manage_metadata_raw_files_pkey\"\n--\n-- TOC entry 4279 (class 2606 OID 27380)\n-- Name: raw_member processing_raw_member_pkey; Type: CONSTRAINT; Schema: processing; Owner: -\n--\n\nALTER TABLE ONLY processing.raw_member\n    ADD CONSTRAINT processing_raw_member_pkey PRIMARY KEY (productid, dimension_position, member_id);\n\n\n--\n-- TOC entry 4255 (class 2606 OID 25951)\n-- Name: changed_cubes_log changed_cubes_log_pkey; Type: CONSTRAINT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.changed_cubes_log\n    ADD CONSTRAINT changed_cubes_log_pkey PRIMARY KEY (productid, change_date);\n\n\n--\n-- TOC entry 4253 (class 2606 OID 25871)\n-- Name: cube_status cube_status_pkey; Type: CONSTRAINT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.cube_status\n    ADD CONSTRAINT cube_status_pkey PRIMARY KEY (productid);\n\n\n--\n-- TOC entry 4246 (class 2606 OID 25741)\n-- Name: manage_cube_raw_files manage_cube_raw_files_pkey; Type: CONSTRAINT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.manage_cube_raw_files\n    ADD CONSTRAINT manage_cube_raw_files_pkey PRIMARY KEY (id);\n\n\n--\n-- TOC entry 4259 (class 2606 OID 26020)\n-- Name: manage_metadata_raw_files manage_metadata_raw_files_pkey; Type: CONSTRAINT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.manage_metadata_raw_files\n    ADD CONSTRAINT manage_metadata_raw_files_pkey PRIMARY KEY (id);\n\n\n--\n-- TOC entry 4251 (class 2606 OID 25754)\n-- Name: manage_spine_raw_files manage_spine_raw_files_pkey; Type: CONSTRAINT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.manage_spine_raw_files\n    ADD CONSTRAINT manage_spine_raw_files_pkey PRIMARY KEY (id);\n\n\n--\n-- TOC entry 4257 (class 2606 OID 26002)\n-- Name: metadata_status metadata_status_pkey; Type: CONSTRAINT; Schema: raw_files; Owner: -\n--\n\nALTER TABLE ONLY raw_files.metadata_status\n    ADD CONSTRAINT metadata_status_pkey PRIMARY KEY (productid);\n\n\npg_dump: creating CONSTRAINT \"raw_files.manage_spine_raw_files manage_spine_raw_files_pkey\"\npg_dump: creating CONSTRAINT \"raw_files.metadata_status metadata_status_pkey\"\npg_dump: creating CONSTRAINT \"spine.cube cube_pkey\"\npg_dump: creating CONSTRAINT \"spine.cube_subject cube_subject_pkey\"\n--\n-- TOC entry 4240 (class 2606 OID 17617)\n-- Name: cube cube_pkey; Type: CONSTRAINT; Schema: spine; Owner: -\n--\n\nALTER TABLE ONLY spine.cube\n    ADD CONSTRAINT cube_pkey PRIMARY KEY (productid);\n\n\n--\n-- TOC entry 4242 (class 2606 OID 17631)\n-- Name: cube_subject cube_subject_pkey; Type: CONSTRAINT; Schema: spine; Owner: -\n--\n\nALTER TABLE ONLY spine.cube_subject\n    ADD CONSTRAINT cube_subject_pkey PRIMARY KEY (productid, subjectcode);\n\n\npg_dump: creating CONSTRAINT \"spine.cube_survey cube_survey_pkey\"\npg_dump: creating INDEX \"processing.idx_dimension_set_members_member_id\"\n--\n-- TOC entry 4244 (class 2606 OID 17638)\n-- Name: cube_survey cube_survey_pkey; Type: CONSTRAINT; Schema: spine; Owner: -\n--\n\nALTER TABLE ONLY spine.cube_survey\n    ADD CONSTRAINT cube_survey_pkey PRIMARY KEY (productid, surveycode);\n\n\npg_dump: creating INDEX \"processing.idx_processed_dimensions_dimension_hash\"\npg_dump:--\n-- TOC entry 4272 (class 1259 OID 27344)\n-- Name: idx_dimension_set_members_member_id; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_dimension_set_members_member_id ON processing.dimension_set_members USING btree (member_id);\n\n\n--\n-- TOC entry 4265 (class 1259 OID 27232)\n-- Name: idx_processed_dimensions_dimension_hash; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processed_dimensions_dimension_hash ON processing.processed_dimensions USING btree (dimension_hash);\n\n\n creating INDEX \"processing.idx_processed_members_dimension_hash\"\npg_dump: creating INDEX \"processing.idx_processed_members_member_hash\"\npg_dump: creating INDEX \"processing.idx_processed_members_productid_pos\"\npg_dump: creating INDEX \"processing.idx_processing_raw_dimension_productid\"\n--\n-- TOC entry 4260 (class 1259 OID 27334)\n-- Name: idx_processed_members_dimension_hash; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processed_members_dimension_hash ON processing.processed_members USING btree (dimension_hash);\n\n\n--\n-- TOC entry 4261 (class 1259 OID 27141)\n-- Name: idx_processed_members_member_hash; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processed_members_member_hash ON processing.processed_members USING btree (member_hash);\n\n\n--\n-- TOC entry 4262 (class 1259 OID 27140)\n-- Name: idx_processed_members_productid_pos; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processed_members_productid_pos ON processing.processed_members USING btree (productid, dimension_position);\n\n\n--\n-- TOC entry 4273 (class 1259 OID 27381)\n-- Name: idx_processing_raw_dimension_productid; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processing_raw_dimension_productid ON processing.raw_dimension USING btree (productid);\n\n\npg_dump: creating INDEX \"processing.idx_processing_raw_member_classification\"\npg_dump: creating INDEX \"processing.idx_processing_raw_member_productid\"\npg_dump: creating INDEX \"raw_files.manage_cube_raw_files_productid_file_hash_idx\"\npg_dump: creating INDEX \"raw_files.manage_cube_raw_files_productid_idx\"\npg_dump: creating INDEX \"raw_files.manage_spine_raw_files_file_hash_idx\"\n--\n-- TOC entry 4276 (class 1259 OID 27383)\n-- Name: idx_processing_raw_member_classification; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processing_raw_member_classification ON processing.raw_member USING btree (classification_code);\n\n\n--\n-- TOC entry 4277 (class 1259 OID 27382)\n-- Name: idx_processing_raw_member_productid; Type: INDEX; Schema: processing; Owner: -\n--\n\nCREATE INDEX idx_processing_raw_member_productid ON processing.raw_member USING btree (productid);\n\n\n--\n-- TOC entry 4247 (class 1259 OID 25743)\n-- Name: manage_cube_raw_files_productid_file_hash_idx; Type: INDEX; Schema: raw_files; Owner: -\n--\n\nCREATE UNIQUE INDEX manage_cube_raw_files_productid_file_hash_idx ON raw_files.manage_cube_raw_files USING btree (productid, file_hash);\n\n\n--\n-- TOC entry 4248 (class 1259 OID 25742)\n-- Name: manage_cube_raw_files_productid_idx; Type: INDEX; Schema: raw_files; Owner: -\n--\n\nCREATE INDEX manage_cube_raw_files_productid_idx ON raw_files.manage_cube_raw_files USING btree (productid);\n\n\n--\n-- TOC entry 4249 (class 1259 OID 25755)\n-- Name: manage_spine_raw_files_file_hash_idx; Type: INDEX; Schema: raw_files; Owner: -\n--\n\nCREATE UNIQUE INDEX manage_spine_raw_files_file_hash_idx ON raw_files.manage_spine_raw_files USING btree (file_hash);\n\n\n-- Completed on 2025-06-20 15:33:18 UTC\n\n--\n-- PostgreSQL database dump complete\n--\n\n"
          },
          "ddl.sh": {
            "size": 1886,
            "modified": "2025-06-19T04:17:12.144168",
            "type": "file"
          },
          "generate_project_structure.py": {
            "size": 5738,
            "modified": "2025-06-21T03:44:45.730199",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatcan Public Data ETL Pipeline\nScript: generate_project_structure.py\nDate: 2025-06-21\nAuthor: Paul Verbrugge with Claude 4 Sonnet (Anthropic)\n\nGenerate documentation of project structure for development reference.\nExcludes sensitive files and focuses on publicly shareable structure.\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Files/directories to exclude for security\nEXCLUDE_PATTERNS = [\n    '__pycache__',\n    '.pyc',\n    '.env',\n    'config.py',  # May contain credentials\n    '.git',\n    'logs',  # May contain sensitive data\n    '.log',\n    'raw',   # Contains downloaded data\n    'staging',\n    'warehouse',\n    'postgres',  # Database files\n    'backups'\n]\n\n# File extensions to include content for\nINCLUDE_CONTENT_EXTENSIONS = [\n    '.py', '.sql', '.md', '.txt', '.yml', '.yaml', '.dockerfile'\n]\n\ndef should_exclude(path_str):\n    \"\"\"Check if path should be excluded for security reasons\"\"\"\n    return any(pattern in path_str.lower() for pattern in EXCLUDE_PATTERNS)\n\ndef get_file_info(file_path):\n    \"\"\"Get basic file information\"\"\"\n    try:\n        stat = file_path.stat()\n        return {\n            'size': stat.st_size,\n            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),\n            'type': 'file'\n        }\n    except:\n        return {'type': 'file', 'error': 'Cannot access'}\n\ndef should_include_content(file_path):\n    \"\"\"Determine if file content should be included\"\"\"\n    return (file_path.suffix.lower() in INCLUDE_CONTENT_EXTENSIONS and \n            file_path.stat().st_size < 50000)  # Max 50KB files\n\ndef scan_directory(base_path, max_depth=3, current_depth=0):\n    \"\"\"Recursively scan directory structure\"\"\"\n    if current_depth > max_depth:\n        return {}\n    \n    structure = {}\n    \n    try:\n        for item in sorted(base_path.iterdir()):\n            if should_exclude(str(item)):\n                continue\n                \n            if item.is_dir():\n                structure[item.name] = {\n                    'type': 'directory',\n                    'contents': scan_directory(item, max_depth, current_depth + 1)\n                }\n            else:\n                file_info = get_file_info(item)\n                \n                # Include content for small, relevant files\n                if should_include_content(item):\n                    try:\n                        with open(item, 'r', encoding='utf-8') as f:\n                            file_info['content'] = f.read()\n                    except:\n                        file_info['content_error'] = 'Cannot read file'\n                \n                structure[item.name] = file_info\n                \n    except PermissionError:\n        structure['_error'] = 'Permission denied'\n    \n    return structure\n\ndef generate_tree_view(structure, prefix=\"\", is_last=True):\n    \"\"\"Generate tree-like text representation\"\"\"\n    lines = []\n    items = list(structure.items())\n    \n    for i, (name, info) in enumerate(items):\n        is_last_item = (i == len(items) - 1)\n        \n        # Tree formatting\n        current_prefix = \"└── \" if is_last_item else \"├── \"\n        lines.append(f\"{prefix}{current_prefix}{name}\")\n        \n        # Recurse for directories\n        if info.get('type') == 'directory' and 'contents' in info:\n            next_prefix = prefix + (\"    \" if is_last_item else \"│   \")\n            lines.extend(generate_tree_view(info['contents'], next_prefix, is_last_item))\n    \n    return lines\n\ndef main():\n    \"\"\"Generate project structure documentation\"\"\"\n    \n    # Base paths to scan (from ETL container perspective)\n    scan_paths = [\n        Path('/app/statcan'),\n        Path('/app/db/statcan_schema') if Path('/app/db/statcan_schema').exists() else None\n    ]\n    \n    # Remove None values\n    scan_paths = [p for p in scan_paths if p and p.exists()]\n    \n    documentation = {\n        'generated_at': datetime.now().isoformat(),\n        'generator': 'ETL Container Structure Scanner',\n        'note': 'Sensitive files and large data directories excluded',\n        'structures': {}\n    }\n    \n    for base_path in scan_paths:\n        print(f\"Scanning {base_path}...\")\n        structure = scan_directory(base_path)\n        documentation['structures'][str(base_path)] = structure\n    \n    # Output both JSON and tree formats\n    output_dir = Path('.')  # Current directory (documentation folder)\n    \n    # JSON format for programmatic use\n    json_file = output_dir / 'project_structure.json'\n    with open(json_file, 'w', encoding='utf-8') as f:\n        json.dump(documentation, f, indent=2, ensure_ascii=False)\n    \n    # Tree format for human reading\n    tree_file = output_dir / 'project_structure.txt'\n    with open(tree_file, 'w', encoding='utf-8') as f:\n        f.write(f\"Project Structure - Generated {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n        \n        for path_str, structure in documentation['structures'].items():\n            f.write(f\"{path_str}/\\n\")\n            tree_lines = generate_tree_view(structure)\n            f.write('\\n'.join(tree_lines))\n            f.write('\\n\\n')\n    \n    print(f\"Documentation generated:\")\n    print(f\"  JSON: {json_file}\")\n    print(f\"  Tree: {tree_file}\")\n    \n    # Show summary\n    total_files = sum(len([k for k, v in struct.items() if v.get('type') == 'file']) \n                     for struct in documentation['structures'].values())\n    total_dirs = sum(len([k for k, v in struct.items() if v.get('type') == 'directory']) \n                    for struct in documentation['structures'].values())\n    \n    print(f\"\\nScanned: {total_files} files, {total_dirs} directories\")\n\nif __name__ == \"__main__\":\n    main()\n"
          }
        }
      },
      "scripts": {
        "type": "directory",
        "contents": {
          "02_spine_load_to_db.py": {
            "size": 13382,
            "modified": "2025-06-20T03:46:39.741956",
            "type": "file",
            "content": "\"\"\"\nEnhanced Spine ETL Script - Statistics Canada ETL Pipeline\n==========================================================\n\nThis script loads validated spine metadata from archived files into the PostgreSQL\ndata warehouse. It implements pre-load validation and safety checks to prevent\ndatabase corruption from invalid or incomplete data files.\n\nKey Features:\n- Loads most recent validated spine file from archive\n- Stages data in DuckDB for validation before database changes\n- Implements comprehensive pre-load validation checks\n- Uses atomic transactions to prevent partial updates\n- Maintains referential integrity across spine schema tables\n\nProcess Flow:\n1. Retrieve path of most recent active spine file\n2. Load JSON data into DuckDB memory database\n3. Create normalized views (cube, cube_subject, cube_survey)\n4. Validate staged data completeness and quality\n5. Compare against existing spine data for sanity checks\n6. Atomically replace spine tables with validated data\n\nProtection Mechanisms:\n- Pre-load validation prevents truncating good data\n- Size and content checks ensure data completeness\n- Atomic transactions with rollback on failure\n- Comparison with existing data for sanity checks\n- Detailed logging for audit trail\n\nDependencies:\n- Requires validated spine files from 01_spine_fetch_raw.py\n- Uses DuckDB for in-memory staging and validation\n- Connects to PostgreSQL spine schema tables\n\nLast Updated: June 2025\nAuthor: Paul Verbrugge with Claude 3.5 Sonnet (v20241022)e\n\"\"\"\n\nimport os\nimport json\nimport tempfile\nimport duckdb\nimport pandas as pd\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\n# Add file logging\nlogger.add(\"/app/logs/spine_etl.log\", rotation=\"10 MB\", retention=\"7 days\")\n\n# Constants\nRAW_DIR = \"/app/raw/statcan/metadata\"\nMIN_EXPECTED_CUBES = 1000\nMIN_SUBJECTS_RATIO = 0.8  # Expect 80%+ of cubes to have subjects\nMAX_SIZE_VARIANCE = 0.1   # Allow 10% variance from existing data\n\n# Initialize DuckDB connection\ncon = duckdb.connect(\":memory:\")\n\n\ndef get_active_file_path() -> str:\n    \"\"\"Fetch the file path of the active spine metadata file from Postgres.\"\"\"\n    query = \"\"\"\n        SELECT storage_location\n        FROM raw_files.manage_spine_raw_files\n        WHERE active = true\n        ORDER BY date_download DESC\n        LIMIT 1\n    \"\"\"\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            cur.execute(query)\n            result = cur.fetchone()\n            if not result:\n                raise RuntimeError(\"❌ No active spine metadata file found.\")\n            file_path = result[0]\n            logger.info(f\"📁 Active spine file: {file_path}\")\n            return file_path\n\n\ndef get_existing_spine_stats(cur) -> dict:\n    \"\"\"Get statistics about current spine data for comparison\"\"\"\n    stats = {}\n    \n    # Count existing cubes\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube\")\n    stats['cube_count'] = cur.fetchone()[0]\n    \n    # Count cube subjects\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube_subject\")\n    stats['subject_count'] = cur.fetchone()[0]\n    \n    # Count cube surveys\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube_survey\")\n    stats['survey_count'] = cur.fetchone()[0]\n    \n    # Count archived cubes\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube WHERE archived = 1\")\n    stats['archived_count'] = cur.fetchone()[0]\n    \n    logger.info(f\"📊 Existing spine stats: {stats['cube_count']} cubes, {stats['subject_count']} subjects, {stats['survey_count']} surveys, {stats['archived_count']} archived\")\n    return stats\n\n\ndef normalize_datetime(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Convert datetime columns to string format for PostgreSQL insertion.\"\"\"\n    df = df.replace({pd.NaT: None, \"NaT\": None})\n    for col in df.select_dtypes(include=[\"datetime64[ns]\"]):\n        df[col] = df[col].apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\") if pd.notnull(x) else None)\n    return df\n\n\ndef stage_data(file_path: str) -> str:\n    \"\"\"Read the JSON metadata into DuckDB and stage views for target tables.\"\"\"\n    logger.info(f\"📥 Loading spine file into DuckDB: {file_path}\")\n    \n    # Verify file exists and is readable\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"❌ Spine file not found: {file_path}\")\n    \n    file_size = os.path.getsize(file_path)\n    if file_size < 100000:  # Less than 100KB is suspicious\n        raise ValueError(f\"❌ Spine file too small: {file_size} bytes\")\n    \n    logger.info(f\"📏 File size: {file_size:,} bytes\")\n    \n    # Install and load JSON extension\n    con.sql(\"INSTALL json; LOAD json;\")\n\n    # Read and validate JSON file\n    try:\n        with open(file_path, \"rb\") as f:\n            json_bytes = f.read()\n        \n        # Verify it's valid JSON\n        json_data = json.loads(json_bytes.decode(\"utf-8\"))\n        if not isinstance(json_data, list):\n            raise ValueError(\"❌ JSON file does not contain a list\")\n        \n        logger.info(f\"✅ JSON validated: {len(json_data)} records\")\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"❌ Invalid JSON in spine file: {e}\")\n\n    # Create temporary file for DuckDB\n    with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".json\", delete=False) as tmp:\n        tmp.write(json_bytes.decode(\"utf-8\"))\n        tmp.flush()\n        json_path = tmp.name\n\n    try:\n        # Create base view from JSON\n        con.execute(f\"CREATE OR REPLACE VIEW base_cube AS SELECT * FROM read_json_auto('{json_path}')\")\n        \n        # Check that we can read the base data\n        base_count = con.sql(\"SELECT COUNT(*) FROM base_cube\").fetchone()[0]\n        logger.info(f\"📊 Base cube records loaded: {base_count}\")\n\n        # Create normalized views\n        con.sql(\"\"\"\n            CREATE OR REPLACE VIEW cube AS \n            SELECT DISTINCT\n                productId, cansimId, cubeTitleEn, cubeTitleFr,\n                CAST(cubeStartDate AS DATE) AS cubeStartDate,\n                CAST(cubeEndDate AS DATE) AS cubeEndDate,\n                CAST(releaseTime AS DATE) AS releaseTime,\n                CAST(archived AS SMALLINT) AS archived,\n                CAST(frequencyCode AS SMALLINT) AS frequencyCode,\n                CAST(issueDate AS DATE) AS issueDate\n            FROM base_cube;\n        \"\"\")\n\n        con.sql(\"\"\"\n            CREATE OR REPLACE VIEW cube_subject AS \n            SELECT productId, UNNEST(subjectCode) AS subjectCode \n            FROM base_cube \n            WHERE subjectCode IS NOT NULL AND len(subjectCode) > 0;\n        \"\"\")\n\n        con.sql(\"\"\"\n            CREATE OR REPLACE VIEW cube_survey AS \n            SELECT productId, UNNEST(surveyCode) AS surveyCode \n            FROM base_cube \n            WHERE surveyCode IS NOT NULL AND len(surveyCode) > 0;\n        \"\"\")\n\n        logger.success(\"✅ DuckDB views created successfully\")\n        return json_path\n        \n    except Exception as e:\n        # Clean up temp file on error\n        if os.path.exists(json_path):\n            os.remove(json_path)\n        raise RuntimeError(f\"❌ Failed to create DuckDB views: {e}\")\n\n\ndef validate_staged_data(existing_stats: dict):\n    \"\"\"Comprehensive validation of staged data before database update\"\"\"\n    logger.info(\"🔍 Validating staged data...\")\n    \n    # Get counts from staged views\n    cube_count = con.sql(\"SELECT COUNT(*) FROM cube\").fetchone()[0]\n    subject_count = con.sql(\"SELECT COUNT(*) FROM cube_subject\").fetchone()[0]\n    survey_count = con.sql(\"SELECT COUNT(*) FROM cube_survey\").fetchone()[0]\n    \n    logger.info(f\"📊 Staged data: {cube_count} cubes, {subject_count} subjects, {survey_count} surveys\")\n    \n    # Validation 1: Minimum cube count\n    if cube_count < MIN_EXPECTED_CUBES:\n        raise ValueError(f\"❌ Too few cubes staged: {cube_count} < {MIN_EXPECTED_CUBES}\")\n    \n    # Validation 2: Required fields not null\n    null_product_ids = con.sql(\"SELECT COUNT(*) FROM cube WHERE productId IS NULL\").fetchone()[0]\n    if null_product_ids > 0:\n        raise ValueError(f\"❌ {null_product_ids} cubes have NULL productId\")\n    \n    null_titles = con.sql(\"SELECT COUNT(*) FROM cube WHERE cubeTitleEn IS NULL OR trim(cubeTitleEn) = ''\").fetchone()[0]\n    if null_titles > 0:\n        raise ValueError(f\"❌ {null_titles} cubes have NULL or empty English titles\")\n    \n    # Validation 3: Product ID uniqueness\n    duplicate_ids = con.sql(\"SELECT COUNT(*) - COUNT(DISTINCT productId) FROM cube\").fetchone()[0]\n    if duplicate_ids > 0:\n        raise ValueError(f\"❌ {duplicate_ids} duplicate product IDs found\")\n    \n    # Validation 4: Subject coverage (most cubes should have subjects)\n    cubes_with_subjects = con.sql(\"SELECT COUNT(DISTINCT productId) FROM cube_subject\").fetchone()[0]\n    subject_ratio = cubes_with_subjects / cube_count if cube_count > 0 else 0\n    if subject_ratio < MIN_SUBJECTS_RATIO:\n        raise ValueError(f\"❌ Too few cubes with subjects: {subject_ratio:.1%} < {MIN_SUBJECTS_RATIO:.1%}\")\n    \n    # Validation 5: Compare with existing data (if exists)\n    if existing_stats['cube_count'] > 0:\n        size_ratio = cube_count / existing_stats['cube_count']\n        if size_ratio < (1 - MAX_SIZE_VARIANCE) or size_ratio > (1 + MAX_SIZE_VARIANCE):\n            logger.warning(f\"⚠️  Large size change: {existing_stats['cube_count']} → {cube_count} ({size_ratio:.1%})\")\n            if size_ratio < 0.5:  # More than 50% reduction is suspicious\n                raise ValueError(f\"❌ Suspicious data reduction: {size_ratio:.1%} of original size\")\n    \n    # Validation 6: Data type consistency\n    invalid_archived = con.sql(\"SELECT COUNT(*) FROM cube WHERE archived NOT IN (0, 1, NULL)\").fetchone()[0]\n    if invalid_archived > 0:\n        raise ValueError(f\"❌ {invalid_archived} cubes have invalid archived values\")\n    \n    logger.success(\"✅ Staged data validation passed\")\n    logger.info(f\"📈 Subject coverage: {subject_ratio:.1%}\")\n    \n    return {\n        'cube_count': cube_count,\n        'subject_count': subject_count,\n        'survey_count': survey_count,\n        'subject_ratio': subject_ratio\n    }\n\n\ndef insert_into_postgres(staged_stats: dict):\n    \"\"\"Insert the validated staged data into the spine schema in PostgreSQL.\"\"\"\n    target_tables = [\"cube\", \"cube_subject\", \"cube_survey\"]\n\n    with psycopg2.connect(**DB_CONFIG) as pg:\n        with pg.cursor() as cur:\n            try:\n                logger.info(\"🔄 Starting atomic spine update...\")\n                \n                # Begin explicit transaction\n                cur.execute(\"BEGIN\")\n                \n                # Truncate tables (within transaction)\n                cur.execute(\"TRUNCATE TABLE spine.cube, spine.cube_subject, spine.cube_survey CASCADE\")\n                logger.info(\"🗑️  Spine tables truncated\")\n\n                # Insert data for each table\n                for table in target_tables:\n                    logger.info(f\"📥 Inserting {table} data...\")\n                    \n                    df = con.sql(f\"SELECT * FROM {table}\").fetchdf()\n                    if df.empty:\n                        logger.warning(f\"⚠️  No data for {table}, skipping...\")\n                        continue\n\n                    df = normalize_datetime(df)\n                    columns = ','.join(df.columns)\n                    values = [tuple(row) for row in df.itertuples(index=False)]\n                    \n                    sql = f\"INSERT INTO spine.{table} ({columns}) VALUES %s\"\n                    execute_values(cur, sql, values)\n                    \n                    logger.info(f\"✅ Inserted {len(values)} rows into spine.{table}\")\n\n                # Commit transaction\n                cur.execute(\"COMMIT\")\n                logger.success(\"✅ Spine update committed successfully\")\n                \n                # Log final summary\n                logger.info(\"📊 Update summary:\")\n                logger.info(f\"   Cubes: {staged_stats['cube_count']}\")\n                logger.info(f\"   Subjects: {staged_stats['subject_count']}\")\n                logger.info(f\"   Surveys: {staged_stats['survey_count']}\")\n\n            except Exception as e:\n                # Rollback on any error\n                cur.execute(\"ROLLBACK\")\n                logger.error(\"🔄 Transaction rolled back due to error\")\n                raise RuntimeError(f\"❌ Failed to update spine tables: {e}\")\n\n\ndef main():\n    json_path = None\n    try:\n        logger.info(\"🚀 Starting enhanced spine ETL...\")\n\n        # Get active file and existing stats\n        file_path = get_active_file_path()\n        \n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                existing_stats = get_existing_spine_stats(cur)\n\n        # Stage and validate data\n        json_path = stage_data(file_path)\n        staged_stats = validate_staged_data(existing_stats)\n        \n        # Update database with validated data\n        insert_into_postgres(staged_stats)\n\n        logger.success(\"✅ Enhanced spine ETL completed successfully\")\n\n    except Exception as e:\n        logger.exception(\"❌ Enhanced spine ETL failed\")\n        raise\n    finally:\n        con.close()\n        if json_path and os.path.exists(json_path):\n            os.remove(json_path)\n            logger.info(\"🧹 Temporary files cleaned up\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "03_cube_status_init.py": {
            "size": 10994,
            "modified": "2025-06-20T03:47:05.356922",
            "type": "file",
            "content": "\"\"\"\nEnhanced Cube Status Initialization Script - Statistics Canada ETL Pipeline\n===========================================================================\n\nThis script initializes the cube_status table with entries for all product IDs\npresent in the spine.cube table that don't already have download tracking records.\nIt implements safety checks to ensure spine data integrity before creating\ndownload tracking entries.\n\nKey Features:\n- Identifies new product IDs from validated spine data\n- Creates download tracking entries with download_pending = TRUE\n- Validates spine data completeness before processing\n- Implements safety checks against corrupted spine data\n- Preserves existing cube_status records (no updates to existing entries)\n\nProcess Flow:\n1. Validate spine.cube table has reasonable data\n2. Identify product IDs missing from cube_status table\n3. Insert new entries with download_pending = TRUE flag\n4. Log summary of changes for audit trail\n\nProtection Mechanisms:\n- Pre-flight validation of spine data integrity\n- Sanity checks on product ID formats and counts\n- Safe INSERT with conflict handling (no overwrites)\n- Detailed logging for monitoring and debugging\n\nUse Cases:\n- Initial pipeline setup (populate cube_status for first time)\n- Recovery after spine updates (catch new cubes for download)\n- Regular maintenance (ensure all cubes are tracked)\n\nDependencies:\n- Requires validated spine.cube table from 02_spine_load_to_db.py\n- Uses raw_files.cube_status table for download tracking\n\nLast Updated: June 2025\nAuthor: Paul Verbrugge with Claude 3.5 Sonnet (v20241022)\n\"\"\"\n\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\n# Add file logging\nlogger.add(\"/app/logs/populate_cube_status.log\", rotation=\"1 MB\", retention=\"7 days\")\n\n# Validation constants\nMIN_SPINE_CUBES = 1000  # Expect at least 1000 cubes in spine\nMIN_PRODUCT_ID = 10000000  # StatCan uses 8-digit product IDs\nMAX_PRODUCT_ID = 99999999\n\n\ndef validate_spine_integrity(cur) -> dict:\n    \"\"\"Validate spine.cube table has reasonable data before processing\"\"\"\n    logger.info(\"🔍 Validating spine data integrity...\")\n    \n    # Check if spine.cube table exists and has data\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube\")\n    cube_count = cur.fetchone()[0]\n    \n    if cube_count == 0:\n        raise ValueError(\"❌ spine.cube table is empty - cannot initialize cube_status\")\n    \n    if cube_count < MIN_SPINE_CUBES:\n        raise ValueError(f\"❌ Too few cubes in spine: {cube_count} < {MIN_SPINE_CUBES}\")\n    \n    # Check for NULL product IDs\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube WHERE productid IS NULL\")\n    null_ids = cur.fetchone()[0]\n    if null_ids > 0:\n        raise ValueError(f\"❌ {null_ids} cubes have NULL product IDs in spine\")\n    \n    # Check product ID format (should be 8-digit numbers)\n    cur.execute(\"\"\"\n        SELECT COUNT(*) FROM spine.cube \n        WHERE productid < %s OR productid > %s\n    \"\"\", (MIN_PRODUCT_ID, MAX_PRODUCT_ID))\n    invalid_ids = cur.fetchone()[0]\n    if invalid_ids > 0:\n        logger.warning(f\"⚠️  {invalid_ids} cubes have non-standard product ID format\")\n        if invalid_ids > cube_count * 0.1:  # More than 10% is suspicious\n            raise ValueError(f\"❌ Too many invalid product IDs: {invalid_ids}/{cube_count}\")\n    \n    # Check for duplicate product IDs\n    cur.execute(\"\"\"\n        SELECT COUNT(*) - COUNT(DISTINCT productid) FROM spine.cube\n    \"\"\")\n    duplicates = cur.fetchone()[0]\n    if duplicates > 0:\n        raise ValueError(f\"❌ {duplicates} duplicate product IDs found in spine\")\n    \n    # Get additional statistics\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube WHERE archived = 1\")\n    archived_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT MIN(productid), MAX(productid) FROM spine.cube\")\n    min_id, max_id = cur.fetchone()\n    \n    stats = {\n        'total_cubes': cube_count,\n        'archived_cubes': archived_count,\n        'active_cubes': cube_count - archived_count,\n        'min_product_id': min_id,\n        'max_product_id': max_id,\n        'invalid_ids': invalid_ids\n    }\n    \n    logger.success(\"✅ Spine data validation passed\")\n    logger.info(f\"📊 Spine stats: {cube_count} total, {archived_count} archived, {cube_count - archived_count} active\")\n    logger.info(f\"🔢 Product ID range: {min_id} to {max_id}\")\n    \n    return stats\n\n\ndef get_existing_status_stats(cur) -> dict:\n    \"\"\"Get current cube_status table statistics\"\"\"\n    logger.info(\"📊 Analyzing existing cube_status data...\")\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.cube_status\")\n    existing_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.cube_status WHERE download_pending = TRUE\")\n    pending_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.cube_status WHERE last_download IS NOT NULL\")\n    downloaded_count = cur.fetchone()[0]\n    \n    stats = {\n        'existing_count': existing_count,\n        'pending_count': pending_count,\n        'downloaded_count': downloaded_count,\n        'never_downloaded': existing_count - downloaded_count\n    }\n    \n    logger.info(f\"📈 Current status: {existing_count} tracked, {pending_count} pending, {downloaded_count} downloaded\")\n    return stats\n\n\ndef identify_missing_cubes(cur) -> list:\n    \"\"\"Find product IDs in spine that are missing from cube_status\"\"\"\n    logger.info(\"🔍 Identifying cubes missing from status tracking...\")\n    \n    cur.execute(\"\"\"\n        SELECT c.productid\n        FROM spine.cube c\n        LEFT JOIN raw_files.cube_status cs ON c.productid = cs.productid\n        WHERE cs.productid IS NULL\n        ORDER BY c.productid\n    \"\"\")\n    \n    missing_cubes = [row[0] for row in cur.fetchall()]\n    logger.info(f\"📋 Found {len(missing_cubes)} cubes missing from cube_status\")\n    \n    if missing_cubes:\n        # Log some examples for verification\n        sample_size = min(5, len(missing_cubes))\n        logger.info(f\"📝 Sample missing product IDs: {missing_cubes[:sample_size]}\")\n        \n        # Check if we're missing a reasonable number\n        cur.execute(\"SELECT COUNT(*) FROM spine.cube\")\n        total_cubes = cur.fetchone()[0]\n        missing_ratio = len(missing_cubes) / total_cubes\n        \n        if missing_ratio > 0.5:  # More than 50% missing is suspicious for existing pipeline\n            logger.warning(f\"⚠️  Large number of missing cubes: {missing_ratio:.1%} of total\")\n    \n    return missing_cubes\n\n\ndef insert_missing_cubes(cur, missing_cubes: list) -> int:\n    \"\"\"Insert missing cube entries into cube_status table\"\"\"\n    if not missing_cubes:\n        logger.info(\"ℹ️  No missing cubes to insert\")\n        return 0\n    \n    logger.info(f\"📥 Inserting {len(missing_cubes)} new cube_status entries...\")\n    \n    # Use INSERT with explicit conflict handling for safety\n    insert_sql = \"\"\"\n        INSERT INTO raw_files.cube_status (productid, download_pending)\n        SELECT unnest(%s::bigint[]), TRUE\n        ON CONFLICT (productid) DO NOTHING\n    \"\"\"\n    \n    try:\n        cur.execute(insert_sql, (missing_cubes,))\n        inserted_count = cur.rowcount\n        \n        if inserted_count != len(missing_cubes):\n            logger.warning(f\"⚠️  Expected to insert {len(missing_cubes)} but inserted {inserted_count}\")\n            logger.info(\"ℹ️  Some cubes may have been added concurrently\")\n        \n        logger.success(f\"✅ Successfully inserted {inserted_count} cube_status entries\")\n        return inserted_count\n        \n    except Exception as e:\n        logger.error(f\"❌ Failed to insert cube_status entries: {e}\")\n        raise\n\n\ndef validate_post_insert(cur, spine_stats: dict, pre_insert_stats: dict, inserted_count: int):\n    \"\"\"Validate the cube_status table after insertion\"\"\"\n    logger.info(\"🔍 Validating post-insertion state...\")\n    \n    # Get updated statistics\n    cur.execute(\"SELECT COUNT(*) FROM raw_files.cube_status\")\n    final_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.cube_status WHERE download_pending = TRUE\")\n    final_pending = cur.fetchone()[0]\n    \n    # Verify counts make sense\n    expected_count = pre_insert_stats['existing_count'] + inserted_count\n    if final_count != expected_count:\n        logger.warning(f\"⚠️  Unexpected final count: {final_count} vs expected {expected_count}\")\n    \n    # Check that we now track all spine cubes\n    cur.execute(\"\"\"\n        SELECT COUNT(*) FROM spine.cube c\n        LEFT JOIN raw_files.cube_status cs ON c.productid = cs.productid\n        WHERE cs.productid IS NULL\n    \"\"\")\n    still_missing = cur.fetchone()[0]\n    \n    if still_missing > 0:\n        logger.error(f\"❌ Still missing {still_missing} cubes after insertion!\")\n        raise ValueError(f\"Post-insertion validation failed: {still_missing} cubes still missing\")\n    \n    logger.success(\"✅ Post-insertion validation passed\")\n    logger.info(f\"📊 Final stats: {final_count} total tracked, {final_pending} pending download\")\n\n\ndef main():\n    logger.info(\"🚀 Starting enhanced cube_status initialization...\")\n    \n    try:\n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                # Validate spine data integrity first\n                spine_stats = validate_spine_integrity(cur)\n                \n                # Get current cube_status statistics\n                pre_insert_stats = get_existing_status_stats(cur)\n                \n                # Find cubes missing from cube_status\n                missing_cubes = identify_missing_cubes(cur)\n                \n                if not missing_cubes:\n                    logger.success(\"✅ All spine cubes already tracked in cube_status\")\n                    return\n                \n                # Insert missing cubes\n                inserted_count = insert_missing_cubes(cur, missing_cubes)\n                \n                if inserted_count > 0:\n                    # Validate final state\n                    validate_post_insert(cur, spine_stats, pre_insert_stats, inserted_count)\n                    \n                    # Commit changes\n                    conn.commit()\n                    logger.success(f\"✅ Successfully initialized {inserted_count} new cube_status entries\")\n                    \n                    # Log summary\n                    logger.info(\"📋 Summary:\")\n                    logger.info(f\"   Spine cubes: {spine_stats['total_cubes']}\")\n                    logger.info(f\"   Previously tracked: {pre_insert_stats['existing_count']}\")\n                    logger.info(f\"   Newly added: {inserted_count}\")\n                    logger.info(f\"   Total now tracked: {pre_insert_stats['existing_count'] + inserted_count}\")\n                else:\n                    logger.info(\"ℹ️  No new entries were needed\")\n\n    except Exception as e:\n        logger.exception(f\"❌ Cube status initialization failed: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "04_cube_status_update.py": {
            "size": 16382,
            "modified": "2025-06-20T03:47:28.454891",
            "type": "file",
            "content": "\"\"\"\nEnhanced Cube Status Update Script - Statistics Canada ETL Pipeline\n====================================================================\n\nThis script monitors StatCan data cubes for updates by querying the getChangedCubeList API\nendpoint for each day since the last checked date. It implements comprehensive API response\nvalidation and safety checks to ensure reliable change detection and download flagging.\n\nKey Features:\n- Tracks change detection progress via changed_cubes_log table\n- Queries StatCan API for changed cubes on each unchecked day\n- Validates API responses for completeness and sanity\n- Records all changes with deduplication handling\n- Updates cube_status.download_pending for cubes needing re-download\n- Handles StatCan's 8:30 AM EST release schedule\n\nProcess Flow:\n1. Determine last checked date from change log\n2. Calculate effective StatCan date (accounting for release time)\n3. For each unchecked date: query API, validate response, record changes\n4. Update cube_status flags based on accumulated change log\n5. Provide summary statistics and audit trail\n\nProtection Mechanisms:\n- API response validation (structure, status codes, data quality)\n- Graceful handling of individual date failures\n- Rate limiting and timeout protection\n- Sanity checks on change volumes and patterns\n- Atomic database operations with rollback capability\n\nAPI Behavior:\n- getChangedCubeList(date) returns cubes that changed on specific date\n- Response includes responseStatusCode (0 = success, others = various errors)\n- Empty responses are normal (no changes on that date)\n- API failures are logged but don't stop overall process\n\nDependencies:\n- Requires spine.cube table for product ID validation\n- Uses raw_files.changed_cubes_log for change tracking\n- Updates raw_files.cube_status for download management\n\nLast Updated: June 2025\nAuthor: Paul Verbrugge with Claude 3.5 Sonnet (v20241022)\n\"\"\"\n\nfrom datetime import date, datetime, time, timedelta, timezone\nimport requests\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom statcan.tools.config import DB_CONFIG\nfrom loguru import logger\nfrom time import sleep \n\n# Add file logging\nlogger.add(\"/app/logs/update_cube_status.log\", rotation=\"10 MB\", retention=\"7 days\")\n\n# API and safety constants\nWDS_CHANGE_URL = \"https://www150.statcan.gc.ca/t1/wds/rest/getChangedCubeList/{}\"\nSLEEP_SECONDS = 2\nNO_CHANGES_MARKER = -1  # Special productid to mark dates with no changes\nAPI_TIMEOUT = 30  # seconds\nMAX_CHANGES_PER_DAY = 1000  # Sanity check - more than this seems suspicious\nMIN_PRODUCT_ID = 10000000  # StatCan 8-digit product IDs\nMAX_PRODUCT_ID = 99999999\n\n\ndef validate_change_tracking_setup(cur) -> dict:\n    \"\"\"Validate that change tracking tables are properly set up\"\"\"\n    logger.info(\"🔍 Validating change tracking setup...\")\n    \n    # Check that changed_cubes_log table exists and is accessible\n    try:\n        cur.execute(\"SELECT COUNT(*) FROM raw_files.changed_cubes_log\")\n        log_count = cur.fetchone()[0]\n    except Exception as e:\n        raise RuntimeError(f\"❌ Cannot access changed_cubes_log table: {e}\")\n    \n    # Check that cube_status table exists and is accessible\n    try:\n        cur.execute(\"SELECT COUNT(*) FROM raw_files.cube_status\")\n        status_count = cur.fetchone()[0]\n    except Exception as e:\n        raise RuntimeError(f\"❌ Cannot access cube_status table: {e}\")\n    \n    # Get some basic statistics\n    cur.execute(\"SELECT MIN(change_date), MAX(change_date) FROM raw_files.changed_cubes_log WHERE productid != %s\", (NO_CHANGES_MARKER,))\n    date_range = cur.fetchone()\n    min_date, max_date = date_range if date_range[0] else (None, None)\n    \n    stats = {\n        'log_entries': log_count,\n        'status_entries': status_count,\n        'min_change_date': min_date,\n        'max_change_date': max_date\n    }\n    \n    logger.success(\"✅ Change tracking setup validated\")\n    logger.info(f\"📊 Change log: {log_count} entries, Status tracking: {status_count} cubes\")\n    if min_date and max_date:\n        logger.info(f\"📅 Change date range: {min_date} to {max_date}\")\n    \n    return stats\n\n\ndef get_last_checked_date(cur) -> date:\n    \"\"\"Get the most recent date we've checked for changes in the log\"\"\"\n    cur.execute(\"SELECT MAX(change_date) FROM raw_files.changed_cubes_log\")\n    result = cur.fetchone()[0]\n    \n    if result is None:\n        # If no changes have been logged, start from a reasonable baseline\n        baseline_date = date(2024, 1, 1)\n        logger.info(f\"ℹ️  No previous change checks found, starting from {baseline_date}\")\n        return baseline_date\n    \n    logger.info(f\"📅 Last checked date found in log: {result}\")\n    return result\n\n\ndef date_already_checked(cur, check_date: date) -> bool:\n    \"\"\"Check if we've already processed this date\"\"\"\n    cur.execute(\"\"\"\n        SELECT 1 FROM raw_files.changed_cubes_log \n        WHERE change_date = %s \n        LIMIT 1\n    \"\"\", (check_date,))\n    result = cur.fetchone() is not None\n    if result:\n        logger.debug(f\"📅 Date {check_date} already processed\")\n    return result\n\n\ndef validate_api_response(response_data: dict, check_date: date) -> bool:\n    \"\"\"Validate API response structure and content\"\"\"\n    logger.debug(f\"🔍 Validating API response for {check_date}\")\n    \n    # Check basic response structure\n    if not isinstance(response_data, dict):\n        logger.error(f\"❌ API response is not a dictionary: {type(response_data)}\")\n        return False\n    \n    if 'status' not in response_data:\n        logger.error(\"❌ API response missing 'status' field\")\n        return False\n    \n    if response_data['status'] != 'SUCCESS':\n        logger.warning(f\"⚠️  API returned non-success status: {response_data.get('status')}\")\n        return False\n    \n    # Get the object array\n    objects = response_data.get('object', [])\n    if not isinstance(objects, list):\n        logger.error(f\"❌ API response 'object' is not a list: {type(objects)}\")\n        return False\n    \n    # Validate individual change records\n    valid_changes = 0\n    for i, entry in enumerate(objects):\n        if not isinstance(entry, dict):\n            logger.warning(f\"⚠️  Change entry {i} is not a dictionary\")\n            continue\n            \n        # Check required fields\n        if 'productId' not in entry or 'responseStatusCode' not in entry:\n            logger.warning(f\"⚠️  Change entry {i} missing required fields\")\n            continue\n            \n        # Validate product ID\n        try:\n            product_id = int(entry['productId'])\n            if not (MIN_PRODUCT_ID <= product_id <= MAX_PRODUCT_ID):\n                logger.warning(f\"⚠️  Invalid product ID format: {product_id}\")\n                continue\n        except (ValueError, TypeError):\n            logger.warning(f\"⚠️  Non-numeric product ID: {entry.get('productId')}\")\n            continue\n            \n        # Check response status code (0 = success)\n        status_code = entry.get('responseStatusCode')\n        if status_code != 0:\n            logger.debug(f\"ℹ️  Entry {i} has non-zero status code: {status_code}\")\n            continue  # This is normal - just means the entry has some issue\n            \n        valid_changes += 1\n    \n    # Sanity check on change volume\n    if valid_changes > MAX_CHANGES_PER_DAY:\n        logger.warning(f\"⚠️  Unusually high change count: {valid_changes} for {check_date}\")\n        logger.warning(\"⚠️  Proceeding but this may indicate API issues\")\n    \n    logger.debug(f\"✅ Validated {valid_changes} valid changes for {check_date}\")\n    return True\n\n\ndef fetch_changed_cubes(for_date: date) -> list[tuple[int, date]]:\n    \"\"\"Fetch and validate changed cubes for a specific date from StatCan API\"\"\"\n    url = WDS_CHANGE_URL.format(for_date.isoformat())\n    logger.info(f\"🔍 Checking changes for {for_date}\")\n    \n    try:\n        resp = requests.get(url, timeout=API_TIMEOUT)\n        resp.raise_for_status()\n        data = resp.json()\n        \n        # Validate response before processing\n        if not validate_api_response(data, for_date):\n            logger.error(f\"❌ Invalid API response for {for_date}\")\n            return []\n        \n        # Extract valid changes (only status code 0)\n        changes = []\n        objects = data.get(\"object\", [])\n        \n        for entry in objects:\n            try:\n                product_id = int(entry[\"productId\"])\n                status_code = entry.get(\"responseStatusCode\", -1)\n                \n                if status_code == 0:  # Only include successful entries\n                    changes.append((product_id, for_date))\n                else:\n                    logger.debug(f\"ℹ️  Skipping product {product_id} with status code {status_code}\")\n                    \n            except (ValueError, TypeError, KeyError) as e:\n                logger.warning(f\"⚠️  Skipping invalid change entry: {e}\")\n                continue\n        \n        logger.info(f\"📅 {for_date}: Found {len(changes)} valid changed cubes\")\n        return changes\n        \n    except requests.exceptions.Timeout:\n        logger.error(f\"❌ API timeout for {for_date} after {API_TIMEOUT} seconds\")\n        return []\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"❌ API request failed for {for_date}: {e}\")\n        return []\n    except Exception as e:\n        logger.error(f\"❌ Unexpected error fetching changes for {for_date}: {e}\")\n        return []\n\n\ndef record_changes_for_date(cur, for_date: date, changes: list[tuple[int, date]]):\n    \"\"\"Record changes for a date, including a marker for dates with no changes\"\"\"\n    try:\n        if changes:\n            # Record actual changes with conflict handling\n            execute_values(cur, \"\"\"\n                INSERT INTO raw_files.changed_cubes_log (productid, change_date)\n                VALUES %s\n                ON CONFLICT (productid, change_date) DO NOTHING;\n            \"\"\", changes)\n            logger.info(f\"📝 Recorded {len(changes)} changes for {for_date}\")\n        else:\n            # Record a marker for dates with no changes\n            cur.execute(\"\"\"\n                INSERT INTO raw_files.changed_cubes_log (productid, change_date)\n                VALUES (%s, %s)\n                ON CONFLICT (productid, change_date) DO NOTHING;\n            \"\"\", (NO_CHANGES_MARKER, for_date))\n            logger.info(f\"📝 Recorded no-changes marker for {for_date}\")\n            \n    except Exception as e:\n        logger.error(f\"❌ Failed to record changes for {for_date}: {e}\")\n        raise\n\n\ndef count_pending_updates(cur) -> int:\n    \"\"\"Count cubes that need downloading based on logged changes\"\"\"\n    cur.execute(\"\"\"\n        SELECT COUNT(DISTINCT log.productid)\n        FROM raw_files.changed_cubes_log log\n        JOIN raw_files.cube_status cs ON log.productid = cs.productid\n        WHERE log.change_date > COALESCE(cs.last_download::date, '2020-01-01'::date)\n          AND log.productid != %s  -- Exclude no-changes markers\n    \"\"\", (NO_CHANGES_MARKER,))\n    return cur.fetchone()[0]\n\n\ndef update_cube_status_from_log(cur):\n    \"\"\"Update cube_status based on all logged changes\"\"\"\n    logger.info(\"🔄 Updating cube download status from change log...\")\n    \n    try:\n        cur.execute(\"\"\"\n            UPDATE raw_files.cube_status\n            SET download_pending = TRUE\n            WHERE productid IN (\n                SELECT DISTINCT log.productid\n                FROM raw_files.changed_cubes_log log\n                JOIN raw_files.cube_status cs ON log.productid = cs.productid\n                WHERE log.change_date > COALESCE(cs.last_download::date, '2020-01-01'::date)\n                  AND log.productid != %s  -- Exclude no-changes markers\n            );\n        \"\"\", (NO_CHANGES_MARKER,))\n        \n        updated_count = cur.rowcount\n        logger.success(f\"✅ Marked {updated_count} cubes as pending download\")\n        return updated_count\n        \n    except Exception as e:\n        logger.error(f\"❌ Failed to update cube status: {e}\")\n        raise\n\n\ndef get_effective_statcan_date() -> date:\n    \"\"\"\n    StatCan releases data at 8:30 AM EST (13:30 UTC).\n    If it's before 13:30 UTC, we consider yesterday as the effective date.\n    \"\"\"\n    now = datetime.now(timezone.utc)\n    cutoff = time(13, 30)  # 08:30 EST == 13:30 UTC\n    \n    if now.time() >= cutoff:\n        effective_date = now.date()\n    else:\n        effective_date = now.date() - timedelta(days=1)\n    \n    logger.info(f\"🕐 Current UTC time: {now.strftime('%H:%M')}, effective StatCan date: {effective_date}\")\n    return effective_date\n\n\ndef main():\n    logger.info(\"🚀 Starting enhanced change detection...\")\n    \n    try:\n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                # Validate setup\n                setup_stats = validate_change_tracking_setup(cur)\n                \n                # Determine date range to check\n                last_checked = get_last_checked_date(cur)\n                today = get_effective_statcan_date()\n\n                logger.info(f\"📅 Last checked date: {last_checked}\")\n                logger.info(f\"📅 Checking through: {today}\")\n\n                if last_checked >= today:\n                    logger.success(\"✅ Already up to date - no new dates to check\")\n                else:\n                    # Process each unchecked date\n                    start_date = last_checked + timedelta(days=1)\n                    current_date = start_date\n                    \n                    total_days = (today - start_date).days + 1\n                    logger.info(f\"🗓️ Checking {total_days} days from {start_date} to {today}\")\n\n                    processed_days = 0\n                    failed_days = 0\n                    total_changes = 0\n                    \n                    while current_date <= today:\n                        day_number = processed_days + 1\n                        \n                        # Skip if already checked (safety check)\n                        if date_already_checked(cur, current_date):\n                            logger.info(f\"⏭️  Day {day_number}/{total_days}: {current_date} - already checked, skipping\")\n                            current_date += timedelta(days=1)\n                            continue\n                        \n                        try:\n                            # Fetch and record changes for this date\n                            changes = fetch_changed_cubes(current_date)\n                            record_changes_for_date(cur, current_date, changes)\n                            \n                            total_changes += len(changes)\n                            processed_days += 1\n                            \n                            logger.success(f\"✅ Day {day_number}/{total_days}: {current_date} - completed ({len(changes)} changes)\")\n                            \n                            # Commit each day's changes\n                            conn.commit()\n                            \n                            # Rate limiting\n                            if current_date < today:  # Don't sleep after last request\n                                sleep(SLEEP_SECONDS)\n                                \n                        except Exception as e:\n                            failed_days += 1\n                            logger.error(f\"❌ Day {day_number}/{total_days}: Failed to process {current_date}: {e}\")\n                            conn.rollback()\n                        \n                        current_date += timedelta(days=1)\n                    \n                    logger.info(f\"📊 Processing summary: {processed_days} successful, {failed_days} failed, {total_changes} total changes\")\n\n                # Update cube status based on all logged changes\n                updated_cubes = update_cube_status_from_log(cur)\n                conn.commit()\n                \n                # Final summary\n                pending_count = count_pending_updates(cur)\n                logger.success(f\"✅ Enhanced change detection complete\")\n                logger.info(f\"📋 Summary: {updated_cubes} cubes flagged, {pending_count} total pending download\")\n\n    except Exception as e:\n        logger.exception(f\"❌ Enhanced change detection failed: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "05_cube_download.py": {
            "size": 10110,
            "modified": "2025-06-20T03:50:41.569637",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Cube Data Download Manager\n===========================================\n\nScript:     05_cube_download.py\nPurpose:    Automated download and tracking of StatCan data cube ZIP files\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script manages the systematic download of Statistics Canada data cubes using \nthe Web Data Service (WDS) API. It processes cubes marked as 'download_pending' in \nthe cube_status table, downloads compressed CSV files via the getFullTableDownloadCSV \nendpoint, and maintains comprehensive file tracking with hash-based deduplication.\n\nKey Features:\n------------\n• Granular status tracking with progressive updates during download process\n• Content-based deduplication using SHA-256 file hashing\n• Robust error handling with per-cube failure isolation\n• Polite API usage with 2-second delays between requests\n• Comprehensive logging with emoji indicators for easy monitoring\n• Atomic database operations with rollback capability\n• File versioning and archival of superseded downloads\n\nData Flow:\n---------\n1. Query raw_files.cube_status for cubes with download_pending = TRUE\n2. For each pending cube:\n   a. Mark download initiation timestamp\n   b. Retrieve download URL from WDS API\n   c. Download and hash file content\n   d. Check for duplicate files using hash comparison\n   e. Save file to /app/raw/cubes/ with hash-based naming\n   f. Update raw_files.manage_cube_raw_files with file metadata\n   g. Mark cube_status as complete (download_pending = FALSE)\n\nDatabase Tables Modified:\n------------------------\n• raw_files.cube_status - Download completion tracking\n• raw_files.manage_cube_raw_files - File inventory and metadata\n\nAPI Endpoints Used:\n------------------\n• GET /t1/wds/rest/getFullTableDownloadCSV/{productid}/en\n\nFile Storage:\n------------\n• Location: /app/raw/cubes/\n• Naming: {productid}_{hash_prefix}.zip\n• Retention: Managed by file verification script (06_cube_verify_files.py)\n\nError Handling:\n--------------\n• Network timeouts: 5-minute download timeout per file\n• API failures: Logged with retry capability maintained\n• Disk I/O errors: Isolated per-cube with batch continuation\n• Database errors: Transactional rollback with error logging\n\nIntegration:\n-----------\n• Preceded by: 04_cube_status_update.py (change detection)\n• Followed by: 06_cube_verify_files.py (integrity verification)\n• Monitored via: /app/logs/fetch_cubes.log\n\nPerformance Notes:\n-----------------\n• Processes all pending cubes in single batch\n• 2-second delay between downloads for API politeness\n• Duplicate detection prevents redundant storage\n• Progressive status updates enable restart capability\n\nUsage:\n------\npython 05_cube_download.py\n\nEnvironment Requirements:\n------------------------\n• Network access to Statistics Canada WDS API\n• Write permissions to /app/raw/cubes/ directory\n• PostgreSQL connection via statcan.tools.config.DB_CONFIG\n• Sufficient disk space for compressed cube files (typically 1KB-100MB each)\n\"\"\"\n\nimport os\nimport requests\nimport hashlib\nimport psycopg2\nfrom pathlib import Path\nfrom loguru import logger\nfrom datetime import datetime\nfrom statcan.tools.config import DB_CONFIG\nimport time\n\n\nWDS_URL_TEMPLATE = \"https://www150.statcan.gc.ca/t1/wds/rest/getFullTableDownloadCSV/{}/en\"\nDOWNLOAD_DIR = Path(\"/app/raw/cubes\")\nMAX_CUBES = None\n\nlogger.add(\"/app/logs/fetch_cubes.log\", rotation=\"10 MB\", retention=\"7 days\")\n\n\ndef get_pending_cubes(limit=MAX_CUBES):\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT productid FROM raw_files.cube_status\n                WHERE download_pending = TRUE\n                ORDER BY productid\n                LIMIT %s;\n            \"\"\", (limit,))\n            return [row[0] for row in cur.fetchall()]\n\n\ndef mark_download_started(productid: int):\n    \"\"\"Mark that download has started for a productid\"\"\"\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                UPDATE raw_files.cube_status\n                SET last_download = now()\n                WHERE productid = %s;\n            \"\"\", (productid,))\n            conn.commit()\n            logger.info(f\"🚀 Marked download started for {productid}\")\n\n\ndef mark_download_failed(productid: int, error_msg: str):\n    \"\"\"Mark that download failed for a productid\"\"\"\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            # Keep download_pending = TRUE so it gets retried\n            # Update last_download to track the attempt\n            cur.execute(\"\"\"\n                UPDATE raw_files.cube_status\n                SET last_download = now()\n                WHERE productid = %s;\n            \"\"\", (productid,))\n            conn.commit()\n            logger.error(f\"❌ Marked download failed for {productid}: {error_msg}\")\n\n\ndef get_download_url(productid: int) -> str:\n    url = WDS_URL_TEMPLATE.format(productid)\n    resp = requests.get(url)\n    resp.raise_for_status()\n    result = resp.json()\n    return result[\"object\"]\n\n\ndef hash_bytes(b: bytes) -> str:\n    return hashlib.sha256(b).hexdigest()\n\n\ndef save_file(productid: int, file_hash: str, content: bytes) -> str:\n    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n    filename = f\"{productid}_{file_hash[:16]}.zip\"\n    file_path = DOWNLOAD_DIR / filename\n    with open(file_path, \"wb\") as f:\n        f.write(content)\n    return str(file_path)\n\n\ndef file_exists(cur, file_hash: str) -> bool:\n    cur.execute(\"SELECT 1 FROM raw_files.manage_cube_raw_files WHERE file_hash = %s\", (file_hash,))\n    return cur.fetchone() is not None\n\n\ndef deactivate_existing(cur, productid: int):\n    cur.execute(\"\"\"\n        UPDATE raw_files.manage_cube_raw_files\n        SET active = FALSE\n        WHERE productid = %s AND active = TRUE\n    \"\"\", (productid,))\n\n\ndef insert_log(cur, productid: int, file_hash: str, file_path: str):\n    deactivate_existing(cur, productid)\n    cur.execute(\"\"\"\n        INSERT INTO raw_files.manage_cube_raw_files (\n            productid, file_hash, date_download, active, storage_location\n        ) VALUES (%s, %s, now(), TRUE, %s)\n    \"\"\", (productid, file_hash, file_path))\n\n\ndef update_status_complete(cur, productid: int):\n    \"\"\"Mark download as successfully completed\"\"\"\n    cur.execute(\"\"\"\n        UPDATE raw_files.cube_status\n        SET download_pending = FALSE, \n            last_download = now(), \n            last_file_hash = (\n                SELECT file_hash FROM raw_files.manage_cube_raw_files\n                WHERE productid = %s AND active = TRUE\n            )\n        WHERE productid = %s;\n    \"\"\", (productid, productid))\n\n\ndef download_and_log(productid: int):\n    logger.info(f\"🔽 Starting download for cube {productid}...\")\n    \n    # Step 1: Mark download started\n    try:\n        mark_download_started(productid)\n    except Exception as e:\n        logger.error(f\"❌ Failed to mark download started for {productid}: {e}\")\n        return\n    \n    # Step 2: Get download URL\n    try:\n        url = get_download_url(productid)\n        logger.info(f\"📡 Got download URL for {productid}\")\n    except Exception as e:\n        mark_download_failed(productid, f\"Failed to get download URL: {e}\")\n        return\n    \n    # Step 3: Download file content\n    try:\n        resp = requests.get(url, timeout=300)  # 5 minute timeout\n        resp.raise_for_status()\n        file_bytes = resp.content\n        file_hash = hash_bytes(file_bytes)\n        logger.info(f\"⬇️ Downloaded {len(file_bytes)} bytes for {productid}, hash: {file_hash[:16]}\")\n    except Exception as e:\n        mark_download_failed(productid, f\"Failed to download file: {e}\")\n        return\n    \n    # Step 4: Check for duplicates and save/log\n    try:\n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                if file_exists(cur, file_hash):\n                    logger.warning(f\"⚠️ Duplicate file for {productid} (hash: {file_hash[:16]}), updating status only\")\n                    update_status_complete(cur, productid)\n                    conn.commit()\n                    logger.success(f\"✅ Updated status for duplicate {productid}\")\n                    return\n                \n                # Save file to disk\n                file_path = save_file(productid, file_hash, file_bytes)\n                logger.info(f\"💾 Saved file for {productid} to {file_path}\")\n                \n                # Log file in database\n                insert_log(cur, productid, file_hash, file_path)\n                logger.info(f\"📝 Logged file for {productid} in database\")\n                \n                # Mark download complete\n                update_status_complete(cur, productid)\n                conn.commit()\n                logger.success(f\"✅ Completed download and logging for {productid}\")\n                \n    except Exception as e:\n        mark_download_failed(productid, f\"Failed to save/log file: {e}\")\n        return\n\n\ndef main():\n    logger.info(\"🚀 Starting cube fetch script...\")\n    try:\n        product_ids = get_pending_cubes()\n        if not product_ids:\n            logger.info(\"🎉 No cubes pending download.\")\n            return\n        \n        logger.info(f\"📋 Found {len(product_ids)} cubes pending download\")\n        \n        for i, pid in enumerate(product_ids, 1):\n            logger.info(f\"🔄 Processing cube {i}/{len(product_ids)}: {pid}\")\n            try:\n                download_and_log(pid)\n                time.sleep(2)  # polite pause\n            except Exception as e:\n                logger.error(f\"❌ Unexpected error processing cube {pid}: {e}\")\n                # Continue with next cube rather than stopping entire batch\n                continue\n                \n        logger.info(\"✅ Batch download complete.\")\n    except Exception as e:\n        logger.exception(f\"❌ Download pipeline failed: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "06_cube_verify_files.py": {
            "size": 6885,
            "modified": "2025-06-20T03:52:48.541471",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Cube File Integrity Verification System\n=========================================================\n\nScript:     06_cube_verify_files.py\nPurpose:    Validate integrity of downloaded cube files and trigger re-downloads when needed\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis critical data integrity script validates that all active cube files in the raw file \ninventory exist on disk and match their recorded SHA-256 hashes. When files are missing \nor corrupted, the script automatically cleans up the database records and triggers \nre-downloads by resetting the download_pending flag in cube_status.\n\nKey Features:\n------------\n• Complete file integrity verification using SHA-256 hash validation\n• Automatic cleanup of missing or corrupted files\n• Self-healing architecture that triggers re-downloads for failed files\n• Atomic database operations with immediate commits per file\n• Comprehensive logging with emoji indicators for monitoring\n• Fail-fast approach with individual file error isolation\n\nVerification Process:\n-------------------\n1. Query all active cube files from raw_files.manage_cube_raw_files\n2. For each recorded file:\n   a. Check physical file existence at storage_location\n   b. Calculate SHA-256 hash of file contents\n   c. Compare calculated hash with recorded file_hash\n   d. If file missing or hash mismatch:\n      - Delete physical file (if corrupted)\n      - Remove database record from manage_cube_raw_files\n      - Set download_pending = TRUE in cube_status for re-download\n   e. Log verification result\n\nDatabase Tables Modified:\n------------------------\n• raw_files.manage_cube_raw_files - Remove invalid file records\n• raw_files.cube_status - Reset download_pending flag for failed files\n\nFile Operations:\n---------------\n• Read verification: All active cube files in /app/raw/cubes/\n• Delete operation: Corrupted files that fail hash validation\n• No file creation or modification (read-only verification)\n\nError Handling:\n--------------\n• Missing files: Database cleanup and re-download trigger\n• Hash mismatches: File deletion, database cleanup, re-download trigger\n• File deletion failures: Logged but don't prevent database cleanup\n• Database errors: Individual file processing continues on error\n\nSelf-Healing Behavior:\n---------------------\nThe script implements automatic recovery by:\n• Removing invalid database records to maintain consistency\n• Triggering re-downloads via download_pending flag\n• Deleting corrupted files to free disk space\n• Logging all actions for audit trail\n\nIntegration Points:\n------------------\n• Follows: 05_cube_download.py (initial file download)\n• Triggers: Re-execution of 05_cube_download.py for failed files\n• Monitored via: /app/logs/verify_raw_files.log\n• Scheduled: Should run after download batches or on schedule\n\nPerformance Characteristics:\n---------------------------\n• I/O intensive: Reads entire content of each file for hashing\n• Memory efficient: Processes files individually\n• Database efficient: Single query to get file list, individual updates\n• Scale: Processes all active files in single run\n\nUsage Scenarios:\n---------------\n• Post-download verification after batch cube downloads\n• Scheduled integrity checks (daily/weekly)\n• Diagnostic runs when data quality issues suspected\n• Recovery operations after disk/network issues\n\nUsage:\n------\npython 06_cube_verify_files.py\n\nEnvironment Requirements:\n------------------------\n• Read access to /app/raw/cubes/ directory\n• Write/delete permissions for cube files\n• PostgreSQL connection via statcan.tools.config.DB_CONFIG\n• Sufficient I/O capacity for hash calculation of large files\n\nMonitoring:\n----------\n• Success indicator: \"✅ Verified {productid}: {filename}\" messages\n• Failure indicators: \"❌ File missing\" or \"⚠️ Hash mismatch\" messages\n• Cleanup actions: \"🗑️ Corrupted file deleted\" messages\n• Overall status: \"🎯 Verification complete\" on successful run\n\nData Quality Assurance:\n----------------------\nThis script is essential for maintaining data integrity in the ETL pipeline.\nIt ensures that downstream processing (cube ingestion, analysis) operates on\nvalidated, uncorrupted source files. The self-healing aspect reduces manual\nintervention and improves pipeline reliability.\n\"\"\"\n\nimport os\nimport hashlib\nimport psycopg2\nfrom pathlib import Path\nfrom statcan.tools.config import DB_CONFIG\nfrom loguru import logger\n\nDOWNLOAD_DIR = Path(\"/app/raw/cubes\")\nlogger.add(\"/app/logs/verify_raw_files.log\", rotation=\"10 MB\", retention=\"7 days\")\n\ndef hash_file(file_path: Path) -> str:\n    with open(file_path, \"rb\") as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef verify_files():\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT productid, file_hash, storage_location\n                FROM raw_files.manage_cube_raw_files\n                WHERE active = TRUE\n            \"\"\")\n            rows = cur.fetchall()\n\n            for productid, file_hash, file_path in rows:\n                p = Path(file_path)\n                if not p.exists():\n                    logger.warning(f\"❌ File missing for {productid}: {file_path}\")\n                    cur.execute(\"DELETE FROM raw_files.manage_cube_raw_files WHERE productid = %s AND file_hash = %s\", (productid, file_hash))\n                    cur.execute(\"UPDATE raw_files.cube_status SET download_pending = TRUE WHERE productid = %s\", (productid,))\n                    conn.commit()\n                    continue\n\n                actual_hash = hash_file(p)\n                if actual_hash != file_hash:\n                    logger.error(f\"⚠️ Hash mismatch for {productid}: expected {file_hash}, got {actual_hash}\")\n                    try:\n                        os.remove(p)\n                        logger.warning(f\"🗑️ Corrupted file deleted: {file_path}\")\n                    except Exception as e:\n                        logger.exception(f\"💥 Failed to delete corrupted file: {file_path}\")\n                    cur.execute(\"DELETE FROM raw_files.manage_cube_raw_files WHERE productid = %s AND file_hash = %s\", (productid, file_hash))\n                    cur.execute(\"UPDATE raw_files.cube_status SET download_pending = TRUE WHERE productid = %s\", (productid,))\n                    conn.commit()\n                else:\n                    logger.info(f\"✅ Verified {productid}: {p.name}\")\n\ndef main():\n    logger.info(\"🔍 Starting raw file verification...\")\n    try:\n        verify_files()\n        logger.success(\"🎯 Verification complete.\")\n    except Exception as e:\n        logger.exception(f\"💥 Verification failed: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n\n"
          },
          "07_metadata_status_init.py": {
            "size": 11430,
            "modified": "2025-06-20T03:53:30.509416",
            "type": "file",
            "content": "\"\"\"\nEnhanced Metadata Status Initialization Script - Statistics Canada ETL Pipeline\n===============================================================================\n\nThis script initializes the metadata_status table with entries for all product IDs\npresent in the spine.cube table that don't already have metadata download tracking \nrecords. It implements comprehensive safety checks to ensure spine data integrity \nbefore creating metadata tracking entries.\n\nKey Features:\n- Identifies new product IDs from validated spine data\n- Creates metadata tracking entries with download_pending = TRUE\n- Validates spine data completeness before processing\n- Implements safety checks against corrupted spine data\n- Preserves existing metadata_status records (no updates to existing entries)\n- Provides detailed audit trail of initialization process\n\nProcess Flow:\n1. Validate spine.cube table has reasonable data\n2. Analyze existing metadata_status tracking coverage\n3. Identify product IDs missing from metadata_status table\n4. Insert new entries with download_pending = TRUE flag\n5. Validate final state and provide summary statistics\n\nProtection Mechanisms:\n- Pre-flight validation of spine data integrity\n- Sanity checks on product ID formats and counts\n- Safe INSERT with conflict handling (no overwrites)\n- Post-insertion validation ensures complete coverage\n- Detailed logging for monitoring and debugging\n\nUse Cases:\n- Initial pipeline setup (populate metadata_status for first time)\n- Recovery after spine updates (catch new cubes for metadata download)\n- Regular maintenance (ensure all cubes have metadata tracking)\n\nDependencies:\n- Requires validated spine.cube table from 02_spine_load_to_db.py\n- Uses raw_files.metadata_status table for download tracking\n\nLast Updated: June 2025\nAuthor: Paul Verbrugge with Claude Sonnet 3.5 (Anthropic)\n\"\"\"\n\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\n# Add file logging\nlogger.add(\"/app/logs/populate_metadata_status.log\", rotation=\"1 MB\", retention=\"7 days\")\n\n# Validation constants\nMIN_SPINE_CUBES = 1000  # Expect at least 1000 cubes in spine\nMIN_PRODUCT_ID = 10000000  # StatCan uses 8-digit product IDs\nMAX_PRODUCT_ID = 99999999\n\n\ndef validate_spine_integrity(cur) -> dict:\n    \"\"\"Validate spine.cube table has reasonable data before processing\"\"\"\n    logger.info(\"🔍 Validating spine data integrity...\")\n    \n    # Check if spine.cube table exists and has data\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube\")\n    cube_count = cur.fetchone()[0]\n    \n    if cube_count == 0:\n        raise ValueError(\"❌ spine.cube table is empty - cannot initialize metadata_status\")\n    \n    if cube_count < MIN_SPINE_CUBES:\n        raise ValueError(f\"❌ Too few cubes in spine: {cube_count} < {MIN_SPINE_CUBES}\")\n    \n    # Check for NULL product IDs\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube WHERE productid IS NULL\")\n    null_ids = cur.fetchone()[0]\n    if null_ids > 0:\n        raise ValueError(f\"❌ {null_ids} cubes have NULL product IDs in spine\")\n    \n    # Check product ID format (should be 8-digit numbers)\n    cur.execute(\"\"\"\n        SELECT COUNT(*) FROM spine.cube \n        WHERE productid < %s OR productid > %s\n    \"\"\", (MIN_PRODUCT_ID, MAX_PRODUCT_ID))\n    invalid_ids = cur.fetchone()[0]\n    if invalid_ids > 0:\n        logger.warning(f\"⚠️  {invalid_ids} cubes have non-standard product ID format\")\n        if invalid_ids > cube_count * 0.1:  # More than 10% is suspicious\n            raise ValueError(f\"❌ Too many invalid product IDs: {invalid_ids}/{cube_count}\")\n    \n    # Check for duplicate product IDs\n    cur.execute(\"\"\"\n        SELECT COUNT(*) - COUNT(DISTINCT productid) FROM spine.cube\n    \"\"\")\n    duplicates = cur.fetchone()[0]\n    if duplicates > 0:\n        raise ValueError(f\"❌ {duplicates} duplicate product IDs found in spine\")\n    \n    # Get additional statistics\n    cur.execute(\"SELECT COUNT(*) FROM spine.cube WHERE archived = 1\")\n    archived_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT MIN(productid), MAX(productid) FROM spine.cube\")\n    min_id, max_id = cur.fetchone()\n    \n    stats = {\n        'total_cubes': cube_count,\n        'archived_cubes': archived_count,\n        'active_cubes': cube_count - archived_count,\n        'min_product_id': min_id,\n        'max_product_id': max_id,\n        'invalid_ids': invalid_ids\n    }\n    \n    logger.success(\"✅ Spine data validation passed\")\n    logger.info(f\"📊 Spine stats: {cube_count} total, {archived_count} archived, {cube_count - archived_count} active\")\n    logger.info(f\"🔢 Product ID range: {min_id} to {max_id}\")\n    \n    return stats\n\n\ndef get_existing_metadata_stats(cur) -> dict:\n    \"\"\"Get current metadata_status table statistics\"\"\"\n    logger.info(\"📊 Analyzing existing metadata_status data...\")\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status\")\n    existing_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status WHERE download_pending = TRUE\")\n    pending_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status WHERE last_download IS NOT NULL\")\n    downloaded_count = cur.fetchone()[0]\n    \n    stats = {\n        'existing_count': existing_count,\n        'pending_count': pending_count,\n        'downloaded_count': downloaded_count,\n        'never_downloaded': existing_count - downloaded_count\n    }\n    \n    logger.info(f\"📈 Current metadata status: {existing_count} tracked, {pending_count} pending, {downloaded_count} downloaded\")\n    return stats\n\n\ndef identify_missing_metadata_cubes(cur) -> list:\n    \"\"\"Find product IDs in spine that are missing from metadata_status\"\"\"\n    logger.info(\"🔍 Identifying cubes missing from metadata status tracking...\")\n    \n    cur.execute(\"\"\"\n        SELECT c.productid\n        FROM spine.cube c\n        LEFT JOIN raw_files.metadata_status ms ON c.productid = ms.productid\n        WHERE ms.productid IS NULL\n        ORDER BY c.productid\n    \"\"\")\n    \n    missing_cubes = [row[0] for row in cur.fetchall()]\n    logger.info(f\"📋 Found {len(missing_cubes)} cubes missing from metadata_status\")\n    \n    if missing_cubes:\n        # Log some examples for verification\n        sample_size = min(5, len(missing_cubes))\n        logger.info(f\"📝 Sample missing product IDs: {missing_cubes[:sample_size]}\")\n        \n        # Check if we're missing a reasonable number\n        cur.execute(\"SELECT COUNT(*) FROM spine.cube\")\n        total_cubes = cur.fetchone()[0]\n        missing_ratio = len(missing_cubes) / total_cubes\n        \n        if missing_ratio > 0.5:  # More than 50% missing is suspicious for existing pipeline\n            logger.warning(f\"⚠️  Large number of missing cubes: {missing_ratio:.1%} of total\")\n    \n    return missing_cubes\n\n\ndef insert_missing_metadata_entries(cur, missing_cubes: list) -> int:\n    \"\"\"Insert missing cube entries into metadata_status table\"\"\"\n    if not missing_cubes:\n        logger.info(\"ℹ️  No missing cubes to insert\")\n        return 0\n    \n    logger.info(f\"📥 Inserting {len(missing_cubes)} new metadata_status entries...\")\n    \n    # Use INSERT with explicit conflict handling for safety\n    insert_sql = \"\"\"\n        INSERT INTO raw_files.metadata_status (productid, download_pending)\n        SELECT unnest(%s::bigint[]), TRUE\n        ON CONFLICT (productid) DO NOTHING\n    \"\"\"\n    \n    try:\n        cur.execute(insert_sql, (missing_cubes,))\n        inserted_count = cur.rowcount\n        \n        if inserted_count != len(missing_cubes):\n            logger.warning(f\"⚠️  Expected to insert {len(missing_cubes)} but inserted {inserted_count}\")\n            logger.info(\"ℹ️  Some cubes may have been added concurrently\")\n        \n        logger.success(f\"✅ Successfully inserted {inserted_count} metadata_status entries\")\n        return inserted_count\n        \n    except Exception as e:\n        logger.error(f\"❌ Failed to insert metadata_status entries: {e}\")\n        raise\n\n\ndef validate_post_insert(cur, spine_stats: dict, pre_insert_stats: dict, inserted_count: int):\n    \"\"\"Validate the metadata_status table after insertion\"\"\"\n    logger.info(\"🔍 Validating post-insertion state...\")\n    \n    # Get updated statistics\n    cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status\")\n    final_count = cur.fetchone()[0]\n    \n    cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status WHERE download_pending = TRUE\")\n    final_pending = cur.fetchone()[0]\n    \n    # Verify counts make sense\n    expected_count = pre_insert_stats['existing_count'] + inserted_count\n    if final_count != expected_count:\n        logger.warning(f\"⚠️  Unexpected final count: {final_count} vs expected {expected_count}\")\n    \n    # Check that we now track all spine cubes for metadata\n    cur.execute(\"\"\"\n        SELECT COUNT(*) FROM spine.cube c\n        LEFT JOIN raw_files.metadata_status ms ON c.productid = ms.productid\n        WHERE ms.productid IS NULL\n    \"\"\")\n    still_missing = cur.fetchone()[0]\n    \n    if still_missing > 0:\n        logger.error(f\"❌ Still missing {still_missing} cubes after insertion!\")\n        raise ValueError(f\"Post-insertion validation failed: {still_missing} cubes still missing\")\n    \n    logger.success(\"✅ Post-insertion validation passed\")\n    logger.info(f\"📊 Final metadata stats: {final_count} total tracked, {final_pending} pending download\")\n\n\ndef main():\n    logger.info(\"🚀 Starting enhanced metadata_status initialization...\")\n    \n    try:\n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                # Validate spine data integrity first\n                spine_stats = validate_spine_integrity(cur)\n                \n                # Get current metadata_status statistics\n                pre_insert_stats = get_existing_metadata_stats(cur)\n                \n                # Find cubes missing from metadata_status\n                missing_cubes = identify_missing_metadata_cubes(cur)\n                \n                if not missing_cubes:\n                    logger.success(\"✅ All spine cubes already tracked in metadata_status\")\n                    return\n                \n                # Insert missing cubes\n                inserted_count = insert_missing_metadata_entries(cur, missing_cubes)\n                \n                if inserted_count > 0:\n                    # Validate final state\n                    validate_post_insert(cur, spine_stats, pre_insert_stats, inserted_count)\n                    \n                    # Commit changes\n                    conn.commit()\n                    logger.success(f\"✅ Successfully initialized {inserted_count} new metadata_status entries\")\n                    \n                    # Log summary\n                    logger.info(\"📋 Summary:\")\n                    logger.info(f\"   Spine cubes: {spine_stats['total_cubes']}\")\n                    logger.info(f\"   Previously tracked: {pre_insert_stats['existing_count']}\")\n                    logger.info(f\"   Newly added: {inserted_count}\")\n                    logger.info(f\"   Total now tracked: {pre_insert_stats['existing_count'] + inserted_count}\")\n                else:\n                    logger.info(\"ℹ️  No new entries were needed\")\n\n    except Exception as e:\n        logger.exception(f\"❌ Metadata status initialization failed: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "08_metadata_download.py": {
            "size": 16608,
            "modified": "2025-06-20T03:54:06.535368",
            "type": "file",
            "content": "\"\"\"\nEnhanced Metadata Download Script - Statistics Canada ETL Pipeline\n===================================================================\n\nThis script downloads bilingual metadata for StatCan data cubes using the getCubeMetadata\nAPI endpoint. It implements comprehensive API response validation, file integrity checks,\nand safety mechanisms to ensure reliable metadata acquisition and storage.\n\nKey Features:\n- Downloads detailed cube metadata in JSON format from StatCan API\n- Validates API responses for completeness and data quality\n- Implements hash-based deduplication to prevent duplicate downloads\n- Manages file storage with content verification and integrity checks\n- Updates tracking tables with atomic operations and rollback protection\n- Rate-limited API calls to respect StatCan's service limits\n\nProcess Flow:\n1. Query metadata_status table for cubes pending download\n2. For each pending cube: call getCubeMetadata API endpoint\n3. Validate API response structure and content quality\n4. Calculate content hash for deduplication checking\n5. Save validated metadata to versioned file storage\n6. Update tracking tables atomically with download status\n7. Implement rate limiting between API calls\n\nProtection Mechanisms:\n- API response validation (structure, content, error handling)\n- Content integrity verification before storage\n- Hash-based deduplication prevents duplicate processing\n- Atomic database operations with rollback capability\n- Timeout protection and retry logic for network issues\n- File system error handling and cleanup\n\nAPI Behavior:\n- getCubeMetadata endpoint returns bilingual metadata in JSON format\n- Includes dimension definitions, member hierarchies, and cube properties\n- Response can be large (several MB for complex cubes)\n- API has rate limits - script implements polite delays\n\nDependencies:\n- Requires metadata_status entries from 07_metadata_status_init.py\n- Uses raw_files.manage_metadata_raw_files for file tracking\n- Updates raw_files.metadata_status for download management\n\nLast Updated: June 2025\nAuthor: Paul Verbrugge with Claude Sonnet 3.5 (Anthropic)\n\"\"\"\n\nimport os\nimport requests\nimport hashlib\nimport psycopg2\nimport json\nfrom pathlib import Path\nfrom loguru import logger\nfrom datetime import datetime\nfrom statcan.tools.config import DB_CONFIG\nimport time\n\n# Add file logging\nlogger.add(\"/app/logs/fetch_metadata.log\", rotation=\"10 MB\", retention=\"7 days\")\n\n# API and validation constants\nWDS_METADATA_URL = \"https://www150.statcan.gc.ca/t1/wds/rest/getCubeMetadata\"\nDOWNLOAD_DIR = Path(\"/app/raw/metadata\")\nAPI_TIMEOUT = 120  # seconds\nRATE_LIMIT_DELAY = 1  # seconds between requests\nMAX_BATCH_SIZE = None  # Set to limit for testing\nMIN_METADATA_SIZE = 1000  # Minimum expected metadata size in bytes\nMAX_METADATA_SIZE = 50 * 1024 * 1024  # 50MB maximum (very large cubes)\n\n\ndef validate_metadata_tracking_setup(cur) -> dict:\n    \"\"\"Validate that metadata tracking tables are properly set up\"\"\"\n    logger.info(\"🔍 Validating metadata tracking setup...\")\n    \n    # Check that metadata_status table exists and is accessible\n    try:\n        cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status\")\n        status_count = cur.fetchone()[0]\n    except Exception as e:\n        raise RuntimeError(f\"❌ Cannot access metadata_status table: {e}\")\n    \n    # Check that manage_metadata_raw_files table exists\n    try:\n        cur.execute(\"SELECT COUNT(*) FROM raw_files.manage_metadata_raw_files\")\n        files_count = cur.fetchone()[0]\n    except Exception as e:\n        raise RuntimeError(f\"❌ Cannot access manage_metadata_raw_files table: {e}\")\n    \n    # Get pending download statistics\n    cur.execute(\"SELECT COUNT(*) FROM raw_files.metadata_status WHERE download_pending = TRUE\")\n    pending_count = cur.fetchone()[0]\n    \n    stats = {\n        'total_status_entries': status_count,\n        'total_file_entries': files_count,\n        'pending_downloads': pending_count\n    }\n    \n    logger.success(\"✅ Metadata tracking setup validated\")\n    logger.info(f\"📊 Status entries: {status_count}, File entries: {files_count}, Pending: {pending_count}\")\n    \n    return stats\n\n\ndef get_pending_metadata(cur, limit=MAX_BATCH_SIZE) -> list:\n    \"\"\"Get list of product IDs that need metadata download\"\"\"\n    logger.info(\"📋 Fetching pending metadata downloads...\")\n    \n    sql = \"\"\"\n        SELECT productid \n        FROM raw_files.metadata_status\n        WHERE download_pending = TRUE\n        ORDER BY productid\n    \"\"\"\n    \n    if limit:\n        sql += f\" LIMIT {limit}\"\n    \n    cur.execute(sql)\n    product_ids = [row[0] for row in cur.fetchall()]\n    \n    logger.info(f\"📥 Found {len(product_ids)} cubes pending metadata download\")\n    if product_ids:\n        sample_size = min(5, len(product_ids))\n        logger.info(f\"📝 Sample product IDs: {product_ids[:sample_size]}\")\n    \n    return product_ids\n\n\ndef validate_api_response(response_data: dict, productid: int) -> bool:\n    \"\"\"Validate API response structure and content\"\"\"\n    logger.debug(f\"🔍 Validating API response for product {productid}\")\n    \n    # Check if response is a list (getCubeMetadata returns array)\n    if not isinstance(response_data, list):\n        logger.error(f\"❌ API response is not a list for {productid}: {type(response_data)}\")\n        return False\n    \n    if len(response_data) == 0:\n        logger.error(f\"❌ Empty response array for {productid}\")\n        return False\n    \n    # Check first element structure\n    first_element = response_data[0]\n    if not isinstance(first_element, dict):\n        logger.error(f\"❌ First response element is not a dictionary for {productid}\")\n        return False\n    \n    # Check for required top-level fields\n    if 'status' not in first_element:\n        logger.error(f\"❌ Response missing 'status' field for {productid}\")\n        return False\n    \n    if first_element.get('status') != 'SUCCESS':\n        logger.warning(f\"⚠️  API returned non-success status for {productid}: {first_element.get('status')}\")\n        return False\n    \n    # Check for object field containing actual metadata\n    if 'object' not in first_element:\n        logger.error(f\"❌ Response missing 'object' field for {productid}\")\n        return False\n    \n    metadata_obj = first_element.get('object', {})\n    if not isinstance(metadata_obj, dict):\n        logger.error(f\"❌ Metadata object is not a dictionary for {productid}\")\n        return False\n    \n    # Validate essential metadata fields\n    essential_fields = ['productId', 'cubeTitleEn']\n    missing_fields = [field for field in essential_fields if field not in metadata_obj]\n    if missing_fields:\n        logger.error(f\"❌ Metadata missing essential fields for {productid}: {missing_fields}\")\n        return False\n    \n    # Validate product ID matches\n    response_product_id = metadata_obj.get('productId')\n    try:\n        if int(response_product_id) != productid:\n            logger.error(f\"❌ Product ID mismatch for {productid}: got {response_product_id}\")\n            return False\n    except (ValueError, TypeError):\n        logger.error(f\"❌ Invalid product ID in response for {productid}: {response_product_id}\")\n        return False\n    \n    # Check for dimension data (main content we need)\n    dimensions = metadata_obj.get('dimension', [])\n    if not isinstance(dimensions, list):\n        logger.warning(f\"⚠️  Dimensions field is not a list for {productid}\")\n    elif len(dimensions) == 0:\n        logger.warning(f\"⚠️  No dimensions found for {productid}\")\n    \n    logger.debug(f\"✅ API response validation passed for {productid}\")\n    return True\n\n\ndef get_metadata_json(productid: int) -> bytes:\n    \"\"\"Fetch metadata from StatCan API with validation and error handling\"\"\"\n    logger.info(f\"🔽 Downloading metadata for product {productid}...\")\n    \n    try:\n        # Prepare request payload\n        payload = [{\"productId\": productid}]\n        headers = {\"Content-Type\": \"application/json\"}\n        \n        # Make API request with timeout\n        response = requests.post(\n            WDS_METADATA_URL,\n            json=payload,\n            headers=headers,\n            timeout=API_TIMEOUT\n        )\n        response.raise_for_status()\n        \n        # Get response content\n        content = response.content\n        content_size = len(content)\n        \n        # Validate content size\n        if content_size < MIN_METADATA_SIZE:\n            raise ValueError(f\"Response too small: {content_size} bytes (minimum {MIN_METADATA_SIZE})\")\n        \n        if content_size > MAX_METADATA_SIZE:\n            raise ValueError(f\"Response too large: {content_size} bytes (maximum {MAX_METADATA_SIZE})\")\n        \n        # Validate JSON structure\n        try:\n            response_data = json.loads(content.decode('utf-8'))\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON response: {e}\")\n        \n        # Validate API response content\n        if not validate_api_response(response_data, productid):\n            raise ValueError(\"API response validation failed\")\n        \n        logger.success(f\"✅ Successfully downloaded metadata for {productid} ({content_size:,} bytes)\")\n        return content\n        \n    except requests.exceptions.Timeout:\n        logger.error(f\"❌ API timeout for {productid} after {API_TIMEOUT} seconds\")\n        raise\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"❌ API request failed for {productid}: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"❌ Metadata download failed for {productid}: {e}\")\n        raise\n\n\ndef hash_bytes(content: bytes) -> str:\n    \"\"\"Generate SHA-256 hash of content\"\"\"\n    return hashlib.sha256(content).hexdigest()\n\n\ndef save_metadata_file(productid: int, file_hash: str, content: bytes) -> str:\n    \"\"\"Save metadata content to file with integrity verification\"\"\"\n    logger.debug(f\"💾 Saving metadata file for product {productid}\")\n    \n    try:\n        # Ensure download directory exists\n        DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n        \n        # Generate filename with hash for deduplication\n        filename = f\"{productid}_{file_hash[:16]}.json\"\n        file_path = DOWNLOAD_DIR / filename\n        \n        # Write content to file\n        with open(file_path, \"wb\") as f:\n            f.write(content)\n        \n        # Verify file was written correctly\n        if not file_path.exists():\n            raise IOError(f\"File was not created: {file_path}\")\n        \n        actual_size = file_path.stat().st_size\n        if actual_size != len(content):\n            raise IOError(f\"File size mismatch: expected {len(content)}, got {actual_size}\")\n        \n        # Verify content integrity\n        with open(file_path, \"rb\") as f:\n            file_content = f.read()\n        \n        file_hash_verify = hash_bytes(file_content)\n        if file_hash_verify != file_hash:\n            raise IOError(f\"File hash mismatch: expected {file_hash}, got {file_hash_verify}\")\n        \n        logger.success(f\"💾 Metadata file saved: {filename}\")\n        return str(file_path)\n        \n    except Exception as e:\n        logger.error(f\"❌ Failed to save metadata file for {productid}: {e}\")\n        # Clean up partial file if it exists\n        if 'file_path' in locals() and file_path.exists():\n            try:\n                file_path.unlink()\n                logger.info(f\"🧹 Cleaned up partial file: {filename}\")\n            except:\n                pass\n        raise\n\n\ndef file_already_exists(cur, file_hash: str) -> bool:\n    \"\"\"Check if file with this hash already exists\"\"\"\n    cur.execute(\"SELECT 1 FROM raw_files.manage_metadata_raw_files WHERE file_hash = %s\", (file_hash,))\n    return cur.fetchone() is not None\n\n\ndef deactivate_existing_metadata(cur, productid: int):\n    \"\"\"Deactivate existing metadata files for this product\"\"\"\n    cur.execute(\"\"\"\n        UPDATE raw_files.manage_metadata_raw_files\n        SET active = FALSE\n        WHERE productid = %s AND active = TRUE\n    \"\"\", (productid,))\n    deactivated = cur.rowcount\n    if deactivated > 0:\n        logger.info(f\"🔄 Deactivated {deactivated} existing metadata file(s) for {productid}\")\n\n\ndef insert_metadata_log(cur, productid: int, file_hash: str, file_path: str):\n    \"\"\"Insert metadata file record into tracking table\"\"\"\n    deactivate_existing_metadata(cur, productid)\n    cur.execute(\"\"\"\n        INSERT INTO raw_files.manage_metadata_raw_files (\n            productid, file_hash, date_download, active, storage_location\n        ) VALUES (%s, %s, now(), TRUE, %s)\n    \"\"\", (productid, file_hash, file_path))\n    logger.debug(f\"📝 Registered metadata file for {productid}: {file_hash[:16]}\")\n\n\ndef update_metadata_status(cur, productid: int, file_hash: str):\n    \"\"\"Update metadata_status table with download completion\"\"\"\n    cur.execute(\"\"\"\n        UPDATE raw_files.metadata_status\n        SET download_pending = FALSE, \n            last_download = now(), \n            last_file_hash = %s\n        WHERE productid = %s\n    \"\"\", (file_hash, productid))\n    \n    if cur.rowcount == 0:\n        logger.warning(f\"⚠️  No metadata_status record updated for {productid}\")\n    else:\n        logger.debug(f\"📊 Updated metadata_status for {productid}\")\n\n\ndef download_and_process_metadata(productid: int) -> bool:\n    \"\"\"Download and process metadata for a single product ID\"\"\"\n    try:\n        # Download metadata from API\n        content = get_metadata_json(productid)\n        file_hash = hash_bytes(content)\n        \n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                # Check for duplicate\n                if file_already_exists(cur, file_hash):\n                    logger.info(f\"ℹ️  Duplicate metadata for {productid} (hash: {file_hash[:16]}), updating status only\")\n                    update_metadata_status(cur, productid, file_hash)\n                    conn.commit()\n                    return True\n                \n                # Save file and update tracking\n                file_path = save_metadata_file(productid, file_hash, content)\n                insert_metadata_log(cur, productid, file_hash, file_path)\n                update_metadata_status(cur, productid, file_hash)\n                \n                conn.commit()\n                logger.success(f\"✅ Completed metadata processing for {productid}\")\n                return True\n                \n    except Exception as e:\n        logger.error(f\"❌ Failed to process metadata for {productid}: {e}\")\n        return False\n\n\ndef main():\n    logger.info(\"🚀 Starting enhanced metadata download...\")\n    \n    try:\n        with psycopg2.connect(**DB_CONFIG) as conn:\n            with conn.cursor() as cur:\n                # Validate setup\n                setup_stats = validate_metadata_tracking_setup(cur)\n                \n                # Get pending downloads\n                product_ids = get_pending_metadata(cur)\n                \n                if not product_ids:\n                    logger.success(\"✅ No metadata downloads pending\")\n                    return\n                \n                # Process each product ID\n                total_products = len(product_ids)\n                successful = 0\n                failed = 0\n                \n                logger.info(f\"📥 Processing {total_products} metadata downloads...\")\n                \n                for i, productid in enumerate(product_ids, 1):\n                    logger.info(f\"🔄 Processing {i}/{total_products}: Product {productid}\")\n                    \n                    if download_and_process_metadata(productid):\n                        successful += 1\n                    else:\n                        failed += 1\n                    \n                    # Rate limiting (except for last request)\n                    if i < total_products:\n                        time.sleep(RATE_LIMIT_DELAY)\n                \n                # Final summary\n                logger.success(f\"✅ Enhanced metadata download complete\")\n                logger.info(f\"📊 Summary: {successful} successful, {failed} failed, {total_products} total\")\n                \n                if failed > 0:\n                    failure_rate = failed / total_products\n                    if failure_rate > 0.1:  # More than 10% failure rate\n                        logger.warning(f\"⚠️  High failure rate: {failure_rate:.1%}\")\n                \n    except Exception as e:\n        logger.exception(f\"❌ Enhanced metadata download failed: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "10_process_dimension_members.py": {
            "size": 9164,
            "modified": "2025-06-20T12:38:23.255240",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Dimension Member Processing and Normalization\n==============================================================\n\nScript:     10_process_dimension_members.py\nPurpose:    Process and normalize raw dimension members with hash-based deduplication\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script processes raw dimension members from Statistics Canada metadata,\ncreating normalized, hash-based identifiers for deduplication and harmonization.\nIt generates member-level hashes and dimension-level hashes for the registry system.\n\nKey Changes:\n- Updated to read from processing.raw_member (instead of dictionary.raw_member)\n- Enhanced error handling and validation\n- TimescaleDB optimization support\n\nThe output feeds into 11_process_dimension.py for final registry construction.\n\nKey Operations:\n--------------\n• Hash generation for member identity (code + label + parent + UOM)\n• Dimension-level hash creation from sorted member hashes\n• Label normalization and most-common selection\n• Metadata flag computation (is_total, etc.)\n• Results stored in processing.processed_members table\n\nProcessing Pipeline:\n-------------------\n1. Load raw_member and raw_dimension data FROM PROCESSING SCHEMA\n2. Generate member_hash for each unique code-label-parent-UOM combination\n3. Create dimension_hash by aggregating member hashes per dimension\n4. Select most common English/French labels for each member\n5. Compute metadata flags (is_total based on label content)\n6. Store processed results for registry building stage\n\"\"\"\n\nimport hashlib\nimport pandas as pd\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/process_dimension_members.log\", rotation=\"1 MB\", retention=\"7 days\")\n\ndef normalize(text):\n    \"\"\"Normalize text for consistent hashing\"\"\"\n    return str(text or \"\").strip().lower()\n\ndef hash_member_identity(member_id, label_en, parent_id=None, uom_code=None):\n    \"\"\"Create deterministic hash for member identity based on key attributes\n    \n    Core member identity based on:\n    - member_id: Core member identifier\n    - label_en: Normalized English label \n    - parent_id: Hierarchical parent relationship\n    - uom_code: Unit of measure code\n    \n    Note: Excludes classification_code, classification_type_code, geo_level,\n    vintage, and terminated from hash to focus on structural identity.\n    \"\"\"\n    key = f\"{normalize(member_id)}|{normalize(label_en)}|{normalize(parent_id)}|{normalize(uom_code)}\"\n    full_hash = hashlib.sha256(key.encode(\"utf-8\")).hexdigest()\n    # Truncate to 12 characters for display convenience\n    # Collision probability ~0.001% for 300k members\n    return full_hash[:12]\n\ndef hash_dimension_identity(member_hashes):\n    \"\"\"Create deterministic hash for dimension based on sorted member hashes\"\"\"\n    sorted_hashes = sorted(member_hashes)\n    full_hash = hashlib.sha256(\"|\".join(sorted_hashes).encode(\"utf-8\")).hexdigest()\n    # Truncate to 12 characters for display convenience  \n    # Collision probability ~0.0003% for 100k dimensions\n    return full_hash[:12]\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef check_required_tables():\n    \"\"\"Verify required tables exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        required_tables = [\n            ('processing', 'raw_member'),\n            ('processing', 'processed_members')\n        ]\n        \n        for schema, table_name in required_tables:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = %s AND table_name = %s\n                )\n            \"\"\", (schema, table_name))\n            \n            if not cur.fetchone()[0]:\n                raise Exception(\n                    f\"❌ Required table {schema}.{table_name} does not exist! \"\n                    \"Please run the DDL script to create it first.\"\n                )\n        \n        logger.info(\"✅ All required tables exist\")\n\ndef process_members():\n    \"\"\"Main member processing function - creates member-level hashes only\"\"\"\n    logger.info(\"🚀 Starting dimension member processing...\")\n    \n    check_required_tables()\n    \n    with get_db_conn() as conn:\n        # Load raw member data FROM PROCESSING SCHEMA\n        logger.info(\"📥 Loading raw member data from processing schema...\")\n        raw_member = pd.read_sql(\"SELECT * FROM processing.raw_member\", conn)\n        \n        logger.info(f\"📊 Processing {len(raw_member)} raw member records\")\n        \n        # Step 1: Minimal label normalization\n        logger.info(\"🔨 Applying minimal label normalization...\")\n        raw_member[\"member_name_en_norm\"] = raw_member[\"member_name_en\"].apply(normalize)\n        raw_member[\"member_name_fr_norm\"] = raw_member[\"member_name_fr\"].apply(normalize)\n        \n        # Step 2: Generate member-level hashes\n        logger.info(\"🔨 Generating member identity hashes...\")\n        raw_member[\"member_hash\"] = raw_member.apply(\n            lambda row: hash_member_identity(\n                row[\"member_id\"],\n                row[\"member_name_en_norm\"],\n                row[\"parent_member_id\"],\n                row[\"member_uom_code\"]\n            ), axis=1\n        )\n        \n        # Step 3: Store all member data with hashes\n        logger.info(\"💾 Storing processed member data...\")\n        \n        # Prepare data for insertion - keep ALL original fields plus computed hash\n        processed_data = raw_member[[\n            \"productid\", \"dimension_position\", \"member_id\", \"member_hash\",\n            \"member_name_en\", \"member_name_fr\", \"member_name_en_norm\", \"member_name_fr_norm\",\n            \"parent_member_id\", \"member_uom_code\", \"classification_code\", \n            \"classification_type_code\", \"geo_level\", \"vintage\", \"terminated\"\n        ]].copy()\n        \n        # Convert integer boolean to proper boolean\n        processed_data[\"terminated\"] = processed_data[\"terminated\"].astype(bool)\n        \n        # Debug: Check for large integer values that might cause overflow\n        logger.info(\"🔍 Checking for integer overflow issues...\")\n        for col in [\"productid\", \"dimension_position\", \"member_id\", \"parent_member_id\", \"geo_level\", \"vintage\"]:\n            if col in processed_data.columns:\n                max_val = processed_data[col].max()\n                min_val = processed_data[col].min()\n                logger.info(f\"   {col}: min={min_val}, max={max_val}\")\n                \n                # PostgreSQL integer range is -2,147,483,648 to 2,147,483,647\n                if pd.notna(max_val) and max_val > 2147483647:\n                    logger.warning(f\"⚠️ {col} has values exceeding integer range: {max_val}\")\n                if pd.notna(min_val) and min_val < -2147483648:\n                    logger.warning(f\"⚠️ {col} has values below integer range: {min_val}\")\n        \n        # Insert into processing table\n        cur = conn.cursor()\n        \n        # Clear existing data for fresh processing\n        cur.execute(\"TRUNCATE TABLE processing.processed_members\")\n        \n        for _, row in processed_data.iterrows():\n            cur.execute(\"\"\"\n                INSERT INTO processing.processed_members (\n                    productid, dimension_position, member_id, member_hash,\n                    member_name_en, member_name_fr, parent_member_id, member_uom_code,\n                    classification_code, classification_type_code, geo_level, vintage,\n                    terminated, member_label_norm\n                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n            \"\"\", (\n                int(row[\"productid\"]) if pd.notna(row[\"productid\"]) else None,\n                int(row[\"dimension_position\"]) if pd.notna(row[\"dimension_position\"]) else None,\n                int(row[\"member_id\"]) if pd.notna(row[\"member_id\"]) else None,\n                row[\"member_hash\"],\n                row[\"member_name_en\"], row[\"member_name_fr\"], \n                int(row[\"parent_member_id\"]) if pd.notna(row[\"parent_member_id\"]) else None,\n                row[\"member_uom_code\"], row[\"classification_code\"], row[\"classification_type_code\"],\n                int(row[\"geo_level\"]) if pd.notna(row[\"geo_level\"]) else None,\n                int(row[\"vintage\"]) if pd.notna(row[\"vintage\"]) else None,\n                row[\"terminated\"], row[\"member_name_en_norm\"]\n            ))\n        \n        conn.commit()\n        \n        # Summary statistics\n        unique_members = len(processed_data)\n        unique_hashes = len(processed_data[\"member_hash\"].unique())\n        \n        logger.success(f\"✅ Processed {unique_members:,} member records\")\n        logger.success(f\"📈 Generated {unique_hashes:,} unique member hashes\")\n        logger.info(\"🎯 Member processing complete - ready for dimension registry building\")\n\ndef main():\n    try:\n        process_members()\n    except Exception as e:\n        logger.exception(f\"❌ Member processing failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "11_process_dimension.py": {
            "size": 7198,
            "modified": "2025-06-20T12:39:43.636289",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Dimension Hash Generator\n=========================================\n\nScript:     11_process_dimensions.py\nPurpose:    Generate dimension hashes from processed member data\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script creates dimension-level hashes by concatenating member hashes\nwithin each (productid, dimension_position) group, sorted by member_id.\n\nKey Changes:\n- Updated to read from processing.raw_dimension (instead of dictionary.raw_dimension)\n- Enhanced error handling and validation\n- TimescaleDB optimization support\n\nRequires: 10_process_dimension_members.py to have run successfully first.\n\nKey Operations:\n--------------\n• Group processed members by (productid, dimension_position)\n• Sort by member_id within each group\n• Concatenate member hashes in sorted order\n• Hash the concatenated string and truncate to 12 characters\n• Add raw dimension metadata FROM PROCESSING SCHEMA\n• Store in processing.processed_dimensions table\n\nProcessing Pipeline:\n-------------------\n1. Load processed members from script 10\n2. Load raw dimension metadata FROM PROCESSING SCHEMA\n3. Group by (productid, dimension_position)  \n4. Sort by member_id within each group\n5. Concatenate member hashes\n6. Create dimension_hash (12-char truncated SHA-256)\n7. Merge with dimension metadata\n8. Store (productid, dimension_position, dimension_hash, names, has_uom)\n\"\"\"\n\nimport hashlib\nimport pandas as pd\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/build_dimension_registry.log\", rotation=\"1 MB\", retention=\"7 days\")\n\ndef hash_dimension_identity(member_hashes_concatenated):\n    \"\"\"Create deterministic hash for dimension from concatenated member hashes\"\"\"\n    full_hash = hashlib.sha256(member_hashes_concatenated.encode(\"utf-8\")).hexdigest()\n    # Truncate to 12 characters for display convenience  \n    return full_hash[:12]\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef check_required_tables():\n    \"\"\"Verify required tables exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        required_tables = [\n            ('processing', 'processed_members'),\n            ('processing', 'raw_dimension'),\n            ('processing', 'processed_dimensions')\n        ]\n        \n        for schema, table_name in required_tables:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = %s AND table_name = %s\n                )\n            \"\"\", (schema, table_name))\n            \n            if not cur.fetchone()[0]:\n                raise Exception(\n                    f\"❌ Required table {schema}.{table_name} does not exist! \"\n                    \"Please run the DDL script to create it first.\"\n                )\n        \n        logger.info(\"✅ All required tables exist\")\n\ndef process_dimensions():\n    \"\"\"Main dimension hash generation function\"\"\"\n    logger.info(\"🚀 Starting dimension hash generation...\")\n    \n    check_required_tables()\n    \n    with get_db_conn() as conn:\n        # Load processed member data\n        logger.info(\"📥 Loading processed member data...\")\n        processed_members = pd.read_sql(\"\"\"\n            SELECT productid, dimension_position, member_id, member_hash \n            FROM processing.processed_members\n            ORDER BY productid, dimension_position, member_id\n        \"\"\", conn)\n        \n        # Load raw dimension metadata FROM PROCESSING SCHEMA\n        logger.info(\"📥 Loading raw dimension metadata from processing schema...\")\n        raw_dimensions = pd.read_sql(\"\"\"\n            SELECT productid, dimension_position, dimension_name_en, dimension_name_fr, has_uom\n            FROM processing.raw_dimension\n        \"\"\", conn)\n        \n        logger.info(f\"📊 Processing {len(processed_members)} member records across {len(raw_dimensions)} dimensions\")\n        \n        # Group by (productid, dimension_position) and concatenate member hashes\n        logger.info(\"🔨 Generating dimension hashes...\")\n        \n        def create_dimension_hash(group):\n            \"\"\"Create dimension hash from sorted member hashes\"\"\"\n            # Sort by member_id (should already be sorted from SQL, but ensure consistency)\n            group_sorted = group.sort_values('member_id')\n            \n            # Concatenate member hashes in order\n            member_hashes_concat = ''.join(group_sorted['member_hash'])\n            \n            # Create dimension hash\n            dimension_hash = hash_dimension_identity(member_hashes_concat)\n            \n            return pd.Series({'dimension_hash': dimension_hash})\n        \n        # Group and generate hashes\n        dimension_hashes = (\n            processed_members.groupby(['productid', 'dimension_position'])\n            .apply(create_dimension_hash, include_groups=False)\n            .reset_index()\n        )\n        \n        logger.info(f\"📈 Generated {len(dimension_hashes)} dimension hashes\")\n        \n        # Add raw dimension metadata\n        logger.info(\"🏷️ Adding raw dimension metadata...\")\n        dimension_data = dimension_hashes.merge(\n            raw_dimensions,\n            on=['productid', 'dimension_position'],\n            how='left'\n        )\n        \n        # Show deduplication statistics\n        unique_hashes = len(dimension_data['dimension_hash'].unique())\n        total_dimensions = len(dimension_data)\n        deduplication_rate = ((total_dimensions - unique_hashes) / total_dimensions * 100)\n        \n        logger.info(f\"🎯 Unique dimension hashes: {unique_hashes:,}\")\n        logger.info(f\"🎯 Total dimensions: {total_dimensions:,}\")\n        logger.info(f\"🎯 Deduplication rate: {deduplication_rate:.1f}%\")\n        \n        # Store in processing table\n        logger.info(\"💾 Storing dimension data...\")\n        \n        cur = conn.cursor()\n        \n        # Clear existing data for fresh processing\n        cur.execute(\"TRUNCATE TABLE processing.processed_dimensions\")\n        \n        for _, row in dimension_data.iterrows():\n            cur.execute(\"\"\"\n                INSERT INTO processing.processed_dimensions (\n                    productid, dimension_position, dimension_hash,\n                    dimension_name_en, dimension_name_fr, has_uom\n                ) VALUES (%s, %s, %s, %s, %s, %s)\n            \"\"\", (\n                int(row['productid']), \n                int(row['dimension_position']), \n                row['dimension_hash'],\n                row['dimension_name_en'],\n                row['dimension_name_fr'], \n                row['has_uom'] if pd.notna(row['has_uom']) else None\n            ))\n        \n        conn.commit()\n        \n        logger.success(f\"✅ Stored {len(dimension_data):,} dimension records with metadata\")\n        logger.info(\"🎯 Dimension processing complete\")\n\ndef main():\n    try:\n        process_dimensions()\n    except Exception as e:\n        logger.exception(f\"❌ Dimension hash generation failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "12_create_dimension_set.py": {
            "size": 8857,
            "modified": "2025-06-20T12:40:54.744331",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Canonical Dimension Registry Builder\n=====================================================\n\nScript:     12_build_canonical_registry.py\nPurpose:    Build canonical dimension registry from processed dimensions\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script creates the canonical dimension registry by summarizing processed\ndimensions by dimension_hash and selecting the most common labels with proper\nformatting for presentation.\n\nKey Changes:\n- All references now point to processing schema (no dictionary schema dependencies)\n- Enhanced error handling and validation\n- TimescaleDB optimization support\n- Improved slugification using python-slugify\n\nRequires: 11_process_dimensions.py to have run successfully first.\n\nKey Operations:\n--------------\n• Aggregate processed_dimensions by dimension_hash\n• Select most common English/French dimension names\n• Apply title case formatting to canonical names\n• Create slugified versions of names\n• Build processing.dimension_set (canonical definitions)\n\nProcessing Pipeline:\n-------------------\n1. Load processed dimensions from script 11\n2. Group by dimension_hash and select most common labels\n3. Apply title case and create slugs\n4. Store canonical definitions in processing.dimension_set\n5. Generate summary statistics\n\nNote: Cube-to-dimension mapping remains in processing.processed_dimensions\nfor now. Final registry mapping will be handled in later scripts.\n\"\"\"\n\nimport pandas as pd\nimport psycopg2\nfrom slugify import slugify\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/build_canonical_registry.log\", rotation=\"1 MB\", retention=\"7 days\")\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef title_case(text):\n    \"\"\"Apply title case to text\"\"\"\n    if pd.isna(text) or text is None:\n        return None\n    return str(text).title()\n\ndef create_slug(text):\n    \"\"\"Create URL-friendly slug from text\"\"\"\n    if pd.isna(text) or text is None:\n        return None\n    return slugify(text, separator=\"_\").lower()\n\ndef check_required_tables():\n    \"\"\"Verify required tables exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        required_tables = [\n            ('processing', 'processed_dimensions'),\n            ('processing', 'dimension_set')\n        ]\n        \n        for schema, table_name in required_tables:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = %s AND table_name = %s\n                )\n            \"\"\", (schema, table_name))\n            \n            if not cur.fetchone()[0]:\n                raise Exception(\n                    f\"❌ Required table {schema}.{table_name} does not exist! \"\n                    \"Please run the DDL script to create it first.\"\n                )\n        \n        logger.info(\"✅ All required tables exist\")\n\ndef build_canonical_dimensions():\n    \"\"\"Build canonical dimension definitions from processed dimensions\"\"\"\n    logger.info(\"🚀 Starting canonical dimension registry build...\")\n    \n    check_required_tables()\n    \n    with get_db_conn() as conn:\n        # Load processed dimensions\n        logger.info(\"📥 Loading processed dimension data...\")\n        processed_dims = pd.read_sql(\"\"\"\n            SELECT dimension_hash, dimension_name_en, dimension_name_fr, has_uom,\n                   productid, dimension_position\n            FROM processing.processed_dimensions\n        \"\"\", conn)\n        \n        logger.info(f\"📊 Processing {len(processed_dims)} dimension instances\")\n        \n        # Group by dimension_hash and select most common labels\n        logger.info(\"🔨 Building canonical dimension definitions...\")\n        \n        def select_canonical_labels(group):\n            \"\"\"Select most common labels for a dimension_hash\"\"\"\n            # Count frequency of each English label\n            en_counts = group['dimension_name_en'].value_counts()\n            most_common_en = en_counts.index[0] if len(en_counts) > 0 else None\n            \n            # Count frequency of each French label  \n            fr_counts = group['dimension_name_fr'].value_counts()\n            most_common_fr = fr_counts.index[0] if len(fr_counts) > 0 else None\n            \n            # Take maximum value of has_uom (TRUE wins over FALSE)\n            max_has_uom = group['has_uom'].max() if group['has_uom'].notna().any() else False\n            \n            # Count usage\n            usage_count = len(group)\n            \n            return pd.Series({\n                'dimension_name_en': most_common_en,\n                'dimension_name_fr': most_common_fr,\n                'has_uom': max_has_uom,\n                'usage_count': usage_count\n            })\n        \n        # Create canonical dimension set\n        canonical_dims = (\n            processed_dims.groupby('dimension_hash')\n            .apply(select_canonical_labels, include_groups=False)\n            .reset_index()\n        )\n        \n        # Apply title case and create slugs\n        logger.info(\"🎨 Applying title case and creating slugs...\")\n        canonical_dims['dimension_name_en'] = canonical_dims['dimension_name_en'].apply(title_case)\n        canonical_dims['dimension_name_fr'] = canonical_dims['dimension_name_fr'].apply(title_case)\n        \n        canonical_dims['dimension_name_en_slug'] = canonical_dims['dimension_name_en'].apply(create_slug)\n        canonical_dims['dimension_name_fr_slug'] = canonical_dims['dimension_name_fr'].apply(create_slug)\n        \n        logger.info(f\"📈 Created {len(canonical_dims)} canonical dimension definitions\")\n        \n        # Store canonical dimensions\n        logger.info(\"💾 Storing canonical dimension definitions...\")\n        cur = conn.cursor()\n        \n        # Clear existing data\n        cur.execute(\"TRUNCATE TABLE processing.dimension_set\")\n        \n        # Insert canonical definitions\n        for _, row in canonical_dims.iterrows():\n            cur.execute(\"\"\"\n                INSERT INTO processing.dimension_set (\n                    dimension_hash, dimension_name_en, dimension_name_fr,\n                    dimension_name_en_slug, dimension_name_fr_slug,\n                    has_uom, usage_count\n                ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n            \"\"\", (\n                row['dimension_hash'],\n                row['dimension_name_en'],\n                row['dimension_name_fr'], \n                row['dimension_name_en_slug'],\n                row['dimension_name_fr_slug'],\n                row['has_uom'],\n                int(row['usage_count'])\n            ))\n        \n        conn.commit()\n        logger.success(f\"✅ Stored {len(canonical_dims)} canonical dimension definitions\")\n\ndef generate_summary_stats():\n    \"\"\"Generate and log summary statistics\"\"\"\n    logger.info(\"📊 Generating registry statistics...\")\n    \n    with get_db_conn() as conn:\n        # Canonical dimensions\n        result = pd.read_sql(\"SELECT COUNT(*) as count FROM processing.dimension_set\", conn)\n        canonical_count = result.iloc[0]['count']\n        \n        # Total dimension instances\n        result = pd.read_sql(\"SELECT COUNT(*) as count FROM processing.processed_dimensions\", conn)\n        total_instances = result.iloc[0]['count']\n        \n        # Deduplication rate\n        deduplication_rate = ((total_instances - canonical_count) / total_instances * 100) if total_instances > 0 else 0\n        \n        # Most used dimensions\n        top_dims = pd.read_sql(\"\"\"\n            SELECT dimension_name_en, usage_count \n            FROM processing.dimension_set \n            ORDER BY usage_count DESC \n            LIMIT 5\n        \"\"\", conn)\n        \n        logger.success(f\"📈 Registry Summary:\")\n        logger.success(f\"   • {canonical_count:,} canonical dimensions\")\n        logger.success(f\"   • {total_instances:,} total dimension instances\")\n        logger.success(f\"   • {deduplication_rate:.1f}% deduplication rate\")\n        \n        logger.info(\"🏆 Top 5 most used dimensions:\")\n        for _, dim in top_dims.iterrows():\n            logger.info(f\"   • {dim['dimension_name_en']}: {dim['usage_count']:,} uses\")\n\ndef main():\n    \"\"\"Main registry building function\"\"\"\n    try:\n        # Build canonical dimensions\n        build_canonical_dimensions()\n        \n        # Generate summary\n        generate_summary_stats()\n        \n        logger.success(\"🎉 Canonical dimension registry build complete!\")\n        logger.info(\"📝 Cube-to-dimension mappings remain in processing.processed_dimensions\")\n        \n    except Exception as e:\n        logger.exception(f\"❌ Registry build failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "13_create_dimension_set_members.py": {
            "size": 9428,
            "modified": "2025-06-20T10:32:01.335339",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Dimension Set Members Builder\n==============================================\n\nScript:     13_create_dimension_set_members.py\nPurpose:    Create canonical member definitions for each dimension\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script creates canonical member definitions by joining processed members\nwith their dimension hashes and summarizing to select the most common member\nattributes within each dimension.\n\nRequires: 10_process_dimension_members.py and 11_process_dimensions.py to have run first.\n\nKey Operations:\n--------------\n• Join processed_members with processed_dimensions on (productid, dimension_position)\n• Group by (dimension_hash, member_id) \n• Select most common member names (English and French)\n• Preserve parent_member_id and member_uom_code\n• Store canonical member definitions in processing.dimension_set_members\n\nProcessing Pipeline:\n-------------------\n1. Load processed members and processed dimensions\n2. Join on (productid, dimension_position) to get dimension_hash for each member\n3. Group by (dimension_hash, member_id) and select most common attributes\n4. Store canonical member definitions\n5. Generate summary statistics\n\"\"\"\n\nimport pandas as pd\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/create_dimension_set_members.log\", rotation=\"1 MB\", retention=\"7 days\")\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef check_required_tables():\n    \"\"\"Verify required tables exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        required_tables = [\n            ('processing', 'processed_members'),\n            ('processing', 'processed_dimensions'),\n            ('processing', 'dimension_set_members')\n        ]\n        \n        for schema, table_name in required_tables:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = %s AND table_name = %s\n                )\n            \"\"\", (schema, table_name))\n            \n            if not cur.fetchone()[0]:\n                raise Exception(\n                    f\"❌ Required table {schema}.{table_name} does not exist! \"\n                    \"Please run the DDL script to create it first.\"\n                )\n        \n        logger.info(\"✅ All required tables exist\")\n\ndef populate_dimension_hashes():\n    \"\"\"Populate dimension_hash in processed_members from processed_dimensions\"\"\"\n    logger.info(\"🔗 Populating dimension hashes in processed_members...\")\n    \n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        # Update processed_members with dimension_hash from processed_dimensions\n        cur.execute(\"\"\"\n            UPDATE processing.processed_members \n            SET dimension_hash = pd.dimension_hash\n            FROM processing.processed_dimensions pd\n            WHERE processing.processed_members.productid = pd.productid\n            AND processing.processed_members.dimension_position = pd.dimension_position\n        \"\"\")\n        \n        updated_rows = cur.rowcount\n        conn.commit()\n        \n        logger.success(f\"✅ Updated {updated_rows:,} member records with dimension hashes\")\n\ndef create_dimension_set_members():\n    \"\"\"Create canonical member definitions for each dimension\"\"\"\n    logger.info(\"🚀 Starting dimension set members creation...\")\n    \n    check_required_tables()\n    \n    # First populate dimension hashes\n    populate_dimension_hashes()\n    \n    with get_db_conn() as conn:\n        # Load processed members with dimension hashes\n        logger.info(\"📥 Loading processed members with dimension hashes...\")\n        \n        members_with_dimensions = pd.read_sql(\"\"\"\n            SELECT \n                dimension_hash,\n                member_id,\n                member_name_en,\n                member_name_fr,\n                parent_member_id,\n                member_uom_code\n            FROM processing.processed_members\n            WHERE dimension_hash IS NOT NULL\n        \"\"\", conn)\n        \n        logger.info(f\"📊 Processing {len(members_with_dimensions)} member instances\")\n        \n        # Group by dimension_hash and member_id to create canonical definitions\n        logger.info(\"🔨 Creating canonical member definitions...\")\n        \n        def select_canonical_member_attributes(group):\n            \"\"\"Select most common member attributes within a dimension\"\"\"\n            # Count frequency of each English name\n            en_counts = group['member_name_en'].value_counts()\n            most_common_en = en_counts.index[0] if len(en_counts) > 0 else None\n            \n            # Count frequency of each French name\n            fr_counts = group['member_name_fr'].value_counts()\n            most_common_fr = fr_counts.index[0] if len(fr_counts) > 0 else None\n            \n            # For parent_member_id and member_uom_code, take the most common non-null value\n            parent_counts = group['parent_member_id'].value_counts(dropna=False)\n            most_common_parent = parent_counts.index[0] if len(parent_counts) > 0 else None\n            \n            uom_counts = group['member_uom_code'].value_counts(dropna=False)\n            most_common_uom = uom_counts.index[0] if len(uom_counts) > 0 else None\n            \n            # Count usage across cube instances\n            usage_count = len(group)\n            \n            return pd.Series({\n                'member_name_en': most_common_en,\n                'member_name_fr': most_common_fr,\n                'parent_member_id': most_common_parent if pd.notna(most_common_parent) else None,\n                'member_uom_code': most_common_uom if pd.notna(most_common_uom) else None,\n                'usage_count': usage_count\n            })\n        \n        # Create canonical member definitions\n        canonical_members = (\n            members_with_dimensions.groupby(['dimension_hash', 'member_id'])\n            .apply(select_canonical_member_attributes, include_groups=False)\n            .reset_index()\n        )\n        \n        logger.info(f\"📈 Created {len(canonical_members)} canonical member definitions\")\n        \n        # Store canonical member definitions\n        logger.info(\"💾 Storing canonical member definitions...\")\n        cur = conn.cursor()\n        \n        # Clear existing data\n        cur.execute(\"TRUNCATE TABLE processing.dimension_set_members\")\n        \n        # Insert canonical member definitions\n        for _, row in canonical_members.iterrows():\n            cur.execute(\"\"\"\n                INSERT INTO processing.dimension_set_members (\n                    dimension_hash, member_id, member_name_en, member_name_fr,\n                    parent_member_id, member_uom_code, usage_count\n                ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n            \"\"\", (\n                row['dimension_hash'],\n                int(row['member_id']),\n                row['member_name_en'],\n                row['member_name_fr'],\n                int(row['parent_member_id']) if pd.notna(row['parent_member_id']) else None,\n                row['member_uom_code'],\n                int(row['usage_count'])\n            ))\n        \n        conn.commit()\n        logger.success(f\"✅ Stored {len(canonical_members)} canonical member definitions\")\n\ndef generate_summary_stats():\n    \"\"\"Generate and log summary statistics\"\"\"\n    logger.info(\"📊 Generating member statistics...\")\n    \n    with get_db_conn() as conn:\n        # Total canonical members\n        result = pd.read_sql(\"SELECT COUNT(*) as count FROM processing.dimension_set_members\", conn)\n        total_members = result.iloc[0]['count']\n        \n        # Members per dimension statistics\n        result = pd.read_sql(\"\"\"\n            SELECT \n                COUNT(*) as member_count,\n                dimension_hash\n            FROM processing.dimension_set_members \n            GROUP BY dimension_hash \n            ORDER BY member_count DESC \n            LIMIT 5\n        \"\"\", conn)\n        \n        # Most used members\n        top_members = pd.read_sql(\"\"\"\n            SELECT member_name_en, usage_count \n            FROM processing.dimension_set_members \n            ORDER BY usage_count DESC \n            LIMIT 5\n        \"\"\", conn)\n        \n        logger.success(f\"📈 Member Summary:\")\n        logger.success(f\"   • {total_members:,} canonical members across all dimensions\")\n        \n        logger.info(\"🏆 Top 5 largest dimensions (by member count):\")\n        for _, dim in result.iterrows():\n            logger.info(f\"   • {dim['dimension_hash']}: {dim['member_count']:,} members\")\n            \n        logger.info(\"🏆 Top 5 most used members:\")\n        for _, member in top_members.iterrows():\n            logger.info(f\"   • {member['member_name_en']}: {member['usage_count']:,} uses\")\n\ndef main():\n    \"\"\"Main member definition creation function\"\"\"\n    try:\n        # Create canonical member definitions\n        create_dimension_set_members()\n        \n        # Generate summary\n        generate_summary_stats()\n        \n        logger.success(\"🎉 Dimension set members creation complete!\")\n        \n    except Exception as e:\n        logger.exception(f\"❌ Member creation failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "14_add_dimension_metadata_flags.py": {
            "size": 10832,
            "modified": "2025-06-20T14:11:24.371345",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Dimension Metadata Flag Generator\n==================================================\n\nScript:     14_add_dimension_metadata_flags.py\nPurpose:    Calculate and populate metadata flags for dimension registry\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script analyzes the members within each dimension to calculate metadata flags:\n- is_tree: True if any members have parent-child relationships (hierarchical structure)\n- is_hetero: True if members have varying units of measure (heterogeneous UOM)\n\nThese flags provide important metadata about dimension structure for downstream analysis\nand help categorize dimensions by their organizational patterns.\n\nRequires: Scripts 10-13 to have run successfully first.\n\nKey Operations:\n--------------\n• Analyze parent_member_id relationships to detect hierarchical structures (is_tree)\n• Analyze member_uom_code variation to detect heterogeneous units (is_hetero)\n• Update processing.dimension_set with calculated flags\n• Generate summary statistics on dimension characteristics\n\nProcessing Pipeline:\n-------------------\n1. Load dimension_set_members data from script 13\n2. Group by dimension_hash and analyze member relationships\n3. Calculate is_tree flag based on parent_member_id presence\n4. Calculate is_hetero flag based on member_uom_code variation\n5. Update processing.dimension_set with calculated flags\n6. Generate summary statistics and validation reports\n\"\"\"\n\nimport pandas as pd\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/add_dimension_metadata_flags.log\", rotation=\"1 MB\", retention=\"7 days\")\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef check_required_tables():\n    \"\"\"Verify required tables exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        required_tables = [\n            ('processing', 'dimension_set'),\n            ('processing', 'dimension_set_members')\n        ]\n        \n        for schema, table_name in required_tables:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = %s AND table_name = %s\n                )\n            \"\"\", (schema, table_name))\n            \n            if not cur.fetchone()[0]:\n                raise Exception(\n                    f\"❌ Required table {schema}.{table_name} does not exist! \"\n                    \"Please run scripts 10-13 first.\"\n                )\n        \n        # Check if the new columns exist\n        cur.execute(\"\"\"\n            SELECT column_name \n            FROM information_schema.columns \n            WHERE table_schema = 'processing' \n            AND table_name = 'dimension_set'\n            AND column_name IN ('is_tree', 'is_hetero')\n        \"\"\")\n        \n        existing_columns = [row[0] for row in cur.fetchall()]\n        missing_columns = set(['is_tree', 'is_hetero']) - set(existing_columns)\n        \n        if missing_columns:\n            raise Exception(\n                f\"❌ Missing columns in processing.dimension_set: {missing_columns}. \"\n                \"Please add these columns first.\"\n            )\n        \n        logger.info(\"✅ All required tables and columns exist\")\n\ndef calculate_metadata_flags():\n    \"\"\"Calculate is_tree and is_hetero flags for each dimension\"\"\"\n    logger.info(\"🚀 Starting dimension metadata flag calculation...\")\n    \n    check_required_tables()\n    \n    with get_db_conn() as conn:\n        # Load dimension members data\n        logger.info(\"📥 Loading dimension members data...\")\n        \n        members_data = pd.read_sql(\"\"\"\n            SELECT \n                dimension_hash,\n                member_id,\n                parent_member_id,\n                member_uom_code\n            FROM processing.dimension_set_members\n            ORDER BY dimension_hash, member_id\n        \"\"\", conn)\n        \n        logger.info(f\"📊 Analyzing {len(members_data)} member records across dimensions\")\n        \n        # Calculate flags by dimension\n        logger.info(\"🔨 Calculating metadata flags...\")\n        \n        def calculate_dimension_flags(group):\n            \"\"\"Calculate is_tree and is_hetero flags for a dimension group\"\"\"\n            # is_tree: True if any member has a parent (non-null parent_member_id)\n            has_parents = group['parent_member_id'].notna().any()\n            is_tree = bool(has_parents)\n            \n            # is_hetero: True if there are multiple distinct non-null UOM codes\n            uom_codes = group['member_uom_code'].dropna().unique()\n            is_hetero = len(uom_codes) > 1\n            \n            return pd.Series({\n                'is_tree': is_tree,\n                'is_hetero': is_hetero,\n                'member_count': len(group),\n                'parent_count': group['parent_member_id'].notna().sum(),\n                'uom_code_count': len(uom_codes),\n                'uom_codes': list(uom_codes) if len(uom_codes) <= 5 else list(uom_codes[:5]) + ['...']\n            })\n        \n        # Group by dimension and calculate flags\n        dimension_flags = (\n            members_data.groupby('dimension_hash')\n            .apply(calculate_dimension_flags, include_groups=False)\n            .reset_index()\n        )\n        \n        logger.info(f\"📈 Calculated flags for {len(dimension_flags)} dimensions\")\n        \n        # Update dimension_set table with flags\n        logger.info(\"💾 Updating dimension_set with metadata flags...\")\n        \n        cur = conn.cursor()\n        update_count = 0\n        \n        for _, row in dimension_flags.iterrows():\n            cur.execute(\"\"\"\n                UPDATE processing.dimension_set \n                SET is_tree = %s, is_hetero = %s\n                WHERE dimension_hash = %s\n            \"\"\", (\n                row['is_tree'],\n                row['is_hetero'],\n                row['dimension_hash']\n            ))\n            update_count += cur.rowcount\n        \n        conn.commit()\n        logger.success(f\"✅ Updated {update_count} dimension records with metadata flags\")\n        \n        return dimension_flags\n\ndef generate_summary_statistics(dimension_flags):\n    \"\"\"Generate and log summary statistics\"\"\"\n    logger.info(\"📊 Generating metadata flag statistics...\")\n    \n    total_dimensions = len(dimension_flags)\n    tree_dimensions = dimension_flags['is_tree'].sum()\n    hetero_dimensions = dimension_flags['is_hetero'].sum()\n    both_flags = ((dimension_flags['is_tree']) & (dimension_flags['is_hetero'])).sum()\n    \n    tree_percentage = (tree_dimensions / total_dimensions * 100) if total_dimensions > 0 else 0\n    hetero_percentage = (hetero_dimensions / total_dimensions * 100) if total_dimensions > 0 else 0\n    both_percentage = (both_flags / total_dimensions * 100) if total_dimensions > 0 else 0\n    \n    logger.success(f\"📈 Metadata Flag Summary:\")\n    logger.success(f\"   • Total dimensions: {total_dimensions:,}\")\n    logger.success(f\"   • Hierarchical (is_tree=true): {tree_dimensions:,} ({tree_percentage:.1f}%)\")\n    logger.success(f\"   • Heterogeneous UOM (is_hetero=true): {hetero_dimensions:,} ({hetero_percentage:.1f}%)\")\n    logger.success(f\"   • Both tree and hetero: {both_flags:,} ({both_percentage:.1f}%)\")\n    \n    # Show examples of each type\n    tree_examples = dimension_flags[dimension_flags['is_tree']].head(3)\n    hetero_examples = dimension_flags[dimension_flags['is_hetero']].head(3)\n    \n    if len(tree_examples) > 0:\n        logger.info(\"🌳 Examples of hierarchical dimensions (is_tree=true):\")\n        for _, dim in tree_examples.iterrows():\n            logger.info(f\"   • {dim['dimension_hash']}: {dim['member_count']} members, {dim['parent_count']} with parents\")\n    \n    if len(hetero_examples) > 0:\n        logger.info(\"🔀 Examples of heterogeneous UOM dimensions (is_hetero=true):\")\n        for _, dim in hetero_examples.iterrows():\n            uom_display = ', '.join(map(str, dim['uom_codes'])) if dim['uom_codes'] else 'None'\n            logger.info(f\"   • {dim['dimension_hash']}: {dim['uom_code_count']} UOM codes ({uom_display})\")\n\ndef validate_flag_calculations():\n    \"\"\"Validate the calculated flags against the raw data\"\"\"\n    logger.info(\"🔍 Validating flag calculations...\")\n    \n    with get_db_conn() as conn:\n        # Check a few dimensions manually to validate logic\n        validation_query = \"\"\"\n            SELECT \n                ds.dimension_hash,\n                ds.dimension_name_en,\n                ds.is_tree,\n                ds.is_hetero,\n                COUNT(dsm.member_id) as member_count,\n                COUNT(dsm.parent_member_id) as parent_count,\n                COUNT(DISTINCT dsm.member_uom_code) as distinct_uom_count\n            FROM processing.dimension_set ds\n            LEFT JOIN processing.dimension_set_members dsm ON ds.dimension_hash = dsm.dimension_hash\n            GROUP BY ds.dimension_hash, ds.dimension_name_en, ds.is_tree, ds.is_hetero\n            HAVING (ds.is_tree = true AND COUNT(dsm.parent_member_id) = 0)\n                OR (ds.is_tree = false AND COUNT(dsm.parent_member_id) > 0)\n                OR (ds.is_hetero = true AND COUNT(DISTINCT dsm.member_uom_code) <= 1)\n                OR (ds.is_hetero = false AND COUNT(DISTINCT dsm.member_uom_code) > 1)\n            LIMIT 5\n        \"\"\"\n        \n        validation_results = pd.read_sql(validation_query, conn)\n        \n        if len(validation_results) > 0:\n            logger.warning(f\"⚠️ Found {len(validation_results)} potential flag calculation inconsistencies:\")\n            for _, row in validation_results.iterrows():\n                logger.warning(f\"   • {row['dimension_hash']} ({row['dimension_name_en']}): \"\n                              f\"is_tree={row['is_tree']}, parent_count={row['parent_count']}, \"\n                              f\"is_hetero={row['is_hetero']}, uom_count={row['distinct_uom_count']}\")\n        else:\n            logger.success(\"✅ All flag calculations appear consistent with underlying data\")\n\n\n\ndef main():\n    \"\"\"Main metadata flag calculation function\"\"\"\n    try:\n        # Calculate metadata flags\n        dimension_flags = calculate_metadata_flags()\n        \n        # Generate summary statistics\n        generate_summary_statistics(dimension_flags)\n        \n        # Validate calculations\n        validate_flag_calculations()\n        \n        logger.success(\"🎉 Dimension metadata flag calculation complete!\")\n        logger.info(\"📝 Flags available in processing.dimension_set: is_tree, is_hetero\")\n        \n    except Exception as e:\n        logger.exception(f\"❌ Metadata flag calculation failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "15_calculate_tree_levels.py": {
            "size": 12510,
            "modified": "2025-06-20T14:18:42.840598",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Dimension Member Tree Level Calculator\n=======================================================\n\nScript:     15_calculate_tree_levels.py\nPurpose:    Calculate hierarchical tree levels for dimension members\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script calculates tree_level values for members in hierarchical dimensions:\n- Level 1: Root nodes (members with no parent_member_id)\n- Level 2: Children of level 1 nodes\n- Level 3: Children of level 2 nodes, etc.\n- NULL: Members in non-hierarchical dimensions (is_tree=false)\n\nThe script includes comprehensive validation to detect and handle:\n- Circular references in parent-child relationships\n- Orphaned members (parent_member_id points to non-existent member)\n- Maximum depth limits to prevent infinite recursion\n- Self-referencing members\n\nRequires: Scripts 10-14 to have run successfully first.\n\nKey Operations:\n--------------\n• Load hierarchical dimensions (is_tree=true) and their members\n• Build parent-child relationship graphs for each dimension\n• Detect and report circular references and orphaned members\n• Calculate tree levels using iterative depth-first traversal\n• Update processing.dimension_set_members with calculated levels\n• Generate comprehensive validation and summary reports\n\nProcessing Pipeline:\n-------------------\n1. Load dimensions marked as hierarchical (is_tree=true)\n2. For each hierarchical dimension, load all members\n3. Build parent-child relationship mapping\n4. Validate relationships (detect cycles, orphans, self-references)\n5. Calculate tree levels iteratively starting from root nodes\n6. Update database with calculated tree_level values\n7. Generate summary statistics and validation reports\n\"\"\"\n\nimport pandas as pd\nimport psycopg2\nfrom collections import defaultdict, deque\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/calculate_tree_levels.log\", rotation=\"1 MB\", retention=\"7 days\")\n\n# Maximum tree depth constant (not used but kept for reference)\nMAX_TREE_DEPTH = 50\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef check_required_tables():\n    \"\"\"Verify required tables and columns exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        # Check if tree_level column exists\n        cur.execute(\"\"\"\n            SELECT EXISTS (\n                SELECT FROM information_schema.columns \n                WHERE table_schema = 'processing' \n                AND table_name = 'dimension_set_members'\n                AND column_name = 'tree_level'\n            )\n        \"\"\")\n        \n        if not cur.fetchone()[0]:\n            raise Exception(\n                \"❌ Column tree_level does not exist in processing.dimension_set_members! \"\n                \"Please run the DDL script to add this column first.\"\n            )\n        \n        logger.info(\"✅ All required tables and columns exist\")\n\ndef load_hierarchical_dimensions():\n    \"\"\"Load dimensions that are marked as hierarchical\"\"\"\n    with get_db_conn() as conn:\n        hierarchical_dims = pd.read_sql(\"\"\"\n            SELECT dimension_hash, dimension_name_en\n            FROM processing.dimension_set \n            WHERE is_tree = true\n            ORDER BY dimension_hash\n        \"\"\", conn)\n        \n        logger.info(f\"📥 Found {len(hierarchical_dims)} hierarchical dimensions\")\n        return hierarchical_dims\n\ndef load_dimension_members(dimension_hash):\n    \"\"\"Load all members for a specific dimension\"\"\"\n    with get_db_conn() as conn:\n        members = pd.read_sql(\"\"\"\n            SELECT dimension_hash, member_id, parent_member_id, member_name_en\n            FROM processing.dimension_set_members\n            WHERE dimension_hash = %s\n            ORDER BY member_id\n        \"\"\", conn, params=(dimension_hash,))\n        \n        return members\n\ndef validate_member_relationships(members, dimension_hash):\n    \"\"\"Validate parent-child relationships and detect issues\"\"\"\n    issues = []\n    \n    # Get all member IDs in this dimension\n    all_member_ids = set(members['member_id'])\n    \n    # Check for self-references\n    self_refs = members[members['member_id'] == members['parent_member_id']]\n    if len(self_refs) > 0:\n        for _, member in self_refs.iterrows():\n            issues.append({\n                'type': 'self_reference',\n                'dimension_hash': dimension_hash,\n                'member_id': member['member_id'],\n                'message': f\"Member {member['member_id']} references itself as parent\"\n            })\n    \n    # Check for orphaned members (parent_member_id points to non-existent member)\n    members_with_parents = members[members['parent_member_id'].notna()]\n    for _, member in members_with_parents.iterrows():\n        if member['parent_member_id'] not in all_member_ids:\n            issues.append({\n                'type': 'orphaned_member',\n                'dimension_hash': dimension_hash,\n                'member_id': member['member_id'],\n                'parent_member_id': member['parent_member_id'],\n                'message': f\"Member {member['member_id']} has parent {member['parent_member_id']} that doesn't exist\"\n            })\n    \n    return issues\n\ndef detect_circular_references(members, dimension_hash):\n    \"\"\"Detect circular references using path tracking\"\"\"\n    # Build parent mapping\n    parent_map = {}\n    for _, member in members.iterrows():\n        if pd.notna(member['parent_member_id']):\n            parent_map[member['member_id']] = member['parent_member_id']\n    \n    circular_refs = []\n    visited_global = set()\n    \n    def check_path_for_cycles(start_member_id):\n        \"\"\"Follow parent path from a member to detect cycles\"\"\"\n        if start_member_id in visited_global:\n            return\n        \n        visited_in_path = set()\n        current_id = start_member_id\n        path = []\n        \n        while current_id is not None and current_id not in visited_global:\n            if current_id in visited_in_path:\n                # Found cycle - get the cycle portion\n                cycle_start = path.index(current_id)\n                cycle = path[cycle_start:] + [current_id]\n                circular_refs.append({\n                    'type': 'circular_reference',\n                    'dimension_hash': dimension_hash,\n                    'cycle': cycle,\n                    'message': f\"Circular reference: {' -> '.join(map(str, cycle))}\"\n                })\n                break\n            \n            visited_in_path.add(current_id)\n            path.append(current_id)\n            current_id = parent_map.get(current_id)\n        \n        # Mark all members in this path as visited\n        visited_global.update(visited_in_path)\n    \n    # Check each member's parent path\n    for member_id in members['member_id']:\n        check_path_for_cycles(member_id)\n    \n    return circular_refs\n\ndef calculate_tree_levels(members, dimension_hash):\n    \"\"\"Calculate tree levels for members in a dimension\"\"\"\n    # Skip if there are validation issues\n    validation_issues = validate_member_relationships(members, dimension_hash)\n    circular_refs = detect_circular_references(members, dimension_hash)\n    \n    if validation_issues or circular_refs:\n        return None, validation_issues + circular_refs\n    \n    # Build parent-to-children mapping\n    children_map = defaultdict(list)\n    for _, member in members.iterrows():\n        if pd.notna(member['parent_member_id']):\n            children_map[member['parent_member_id']].append(member['member_id'])\n    \n    # Find root nodes (members with no parent)\n    root_nodes = members[members['parent_member_id'].isna()]['member_id'].tolist()\n    \n    if not root_nodes:\n        return None, [{\n            'type': 'no_root_nodes',\n            'dimension_hash': dimension_hash,\n            'message': f\"No root nodes found in hierarchical dimension {dimension_hash}\"\n        }]\n    \n    # Calculate levels using breadth-first search\n    member_levels = {}\n    queue = deque([(root_id, 1) for root_id in root_nodes])\n    \n    while queue:\n        member_id, level = queue.popleft()\n        member_levels[member_id] = level\n        \n        # Add children to queue with next level\n        for child_id in children_map[member_id]:\n            queue.append((child_id, level + 1))\n    \n    return member_levels, []\n\ndef update_tree_levels(dimension_hash, member_levels):\n    \"\"\"Update tree_level values in the database\"\"\"\n    if not member_levels:\n        return 0\n    \n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        update_count = 0\n        \n        for member_id, level in member_levels.items():\n            cur.execute(\"\"\"\n                UPDATE processing.dimension_set_members \n                SET tree_level = %s\n                WHERE dimension_hash = %s AND member_id = %s\n            \"\"\", (level, dimension_hash, member_id))\n            update_count += cur.rowcount\n        \n        conn.commit()\n        return update_count\n\ndef clear_tree_levels_for_non_hierarchical():\n    \"\"\"Set tree_level to NULL for members in non-hierarchical dimensions\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        cur.execute(\"\"\"\n            UPDATE processing.dimension_set_members \n            SET tree_level = NULL\n            WHERE dimension_hash IN (\n                SELECT dimension_hash \n                FROM processing.dimension_set \n                WHERE is_tree = false OR is_tree IS NULL\n            )\n        \"\"\")\n        \n        cleared_count = cur.rowcount\n        conn.commit()\n        \n        logger.info(f\"Cleared tree_level for {cleared_count} members in non-hierarchical dimensions\")\n\ndef generate_summary_statistics():\n    \"\"\"Generate and log summary statistics\"\"\"\n    with get_db_conn() as conn:\n        # Overall statistics\n        stats = pd.read_sql(\"\"\"\n            SELECT \n                COUNT(*) as total_members,\n                COUNT(tree_level) as members_with_levels,\n                MIN(tree_level) as min_level,\n                MAX(tree_level) as max_level\n            FROM processing.dimension_set_members\n        \"\"\", conn)\n        \n        row = stats.iloc[0]\n        logger.success(f\"Updated {row['members_with_levels']:,} members with tree levels (range: {row['min_level']}-{row['max_level']})\")\n\ndef main():\n    \"\"\"Main tree level calculation function\"\"\"\n    try:\n        logger.info(\"🚀 Starting tree level calculation...\")\n        \n        check_required_tables()\n        \n        # Clear tree levels for non-hierarchical dimensions first\n        clear_tree_levels_for_non_hierarchical()\n        \n        # Load hierarchical dimensions\n        hierarchical_dims = load_hierarchical_dimensions()\n        \n        if len(hierarchical_dims) == 0:\n            logger.warning(\"⚠️ No hierarchical dimensions found (is_tree=true)\")\n            return\n        \n        total_updated = 0\n        total_issues = []\n        \n        # Process each hierarchical dimension\n        for _, dim_row in hierarchical_dims.iterrows():\n            dimension_hash = dim_row['dimension_hash']\n            \n            # Load members for this dimension\n            members = load_dimension_members(dimension_hash)\n            \n            if len(members) == 0:\n                continue\n            \n            # Calculate tree levels\n            member_levels, issues = calculate_tree_levels(members, dimension_hash)\n            \n            if issues:\n                total_issues.extend(issues)\n                continue\n            \n            # Update database\n            updated_count = update_tree_levels(dimension_hash, member_levels)\n            total_updated += updated_count\n        \n        # Generate summary statistics\n        generate_summary_statistics()\n        \n        # Report any issues found\n        if total_issues:\n            logger.warning(f\"Found {len(total_issues)} validation issues\")\n            # Only log first few issues to avoid spam\n            for issue in total_issues[:3]:\n                logger.warning(issue['message'])\n            if len(total_issues) > 3:\n                logger.warning(f\"... and {len(total_issues) - 3} more issues\")\n        \n        logger.success(f\"Tree level calculation complete! Updated {total_updated:,} members\")\n        \n    except Exception as e:\n        logger.exception(f\"❌ Tree level calculation failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "16_populate_cube_dimension_registry.py": {
            "size": 17986,
            "modified": "2025-06-20T15:13:40.153702",
            "type": "file",
            "content": "#!/usr/bin/env python3\n\"\"\"\nStatistics Canada Cube Dimension Registry Mapping Populator\n==========================================================\n\nScript:     16_populate_cube_dimension_registry.py\nPurpose:    Populate cube.cube_dimension_map with productid->dimension_hash mappings\nAuthor:     Paul Verbrugge with Claude Sonnet 4 (Anthropic)\nCreated:    2025\nUpdated:    June 2025\n\nOverview:\n--------\nThis script populates the cube.cube_dimension_map table by associating productid and \ndimension_position with the corresponding dimension_hash from processing.dimension_set.\nThe table serves as the canonical mapping between cubes and their normalized dimensions.\n\nThe script includes comprehensive data integrity checks and processes one productid at a time\nto ensure consistency and enable partial recovery from failures.\n\nRequires: Scripts 10-15 to have run successfully first.\n\nKey Operations:\n--------------\n• Load processed dimensions with their dimension_hash values\n• Validate that all dimension_positions for each productid have corresponding hashes\n• Delete existing mappings for productid before repopulating (update logic)\n• Insert new mappings with dimension metadata from canonical registry\n• Comprehensive validation and error reporting\n• Progress tracking for large datasets\n\nProcessing Pipeline:\n-------------------\n1. Load all processed dimensions from processing.processed_dimensions\n2. Validate data completeness for each productid\n3. For each productid:\n   a. Delete existing mappings\n   b. Insert new mappings with dimension metadata\n   c. Validate insertion completeness\n4. Generate summary statistics and validation reports\n5. Report any data integrity issues discovered\n\nData Integrity Checks:\n---------------------\n• Verify all dimension_positions for a productid have dimension_hashes\n• Check for orphaned mappings (dimension_hash not in canonical registry)\n• Validate that dimension metadata is correctly joined\n• Ensure no duplicate mappings exist\n• Detect missing or extra dimension positions\n\"\"\"\n\nimport pandas as pd\nimport psycopg2\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/populate_cube_dimension_registry.log\", rotation=\"1 MB\", retention=\"7 days\")\n\ndef get_db_conn():\n    return psycopg2.connect(**DB_CONFIG)\n\ndef check_required_tables():\n    \"\"\"Verify all required tables and columns exist\"\"\"\n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        required_tables = [\n            ('processing', 'processed_dimensions'),\n            ('processing', 'dimension_set'),\n            ('cube', 'cube_dimension_map')\n        ]\n        \n        for schema, table_name in required_tables:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_schema = %s AND table_name = %s\n                )\n            \"\"\", (schema, table_name))\n            \n            if not cur.fetchone()[0]:\n                raise Exception(\n                    f\"❌ Required table {schema}.{table_name} does not exist! \"\n                    \"Please ensure all prerequisite scripts have run successfully.\"\n                )\n        \n        # Verify that processed_dimensions has dimension_hash populated\n        cur.execute(\"\"\"\n            SELECT COUNT(*) as total, COUNT(dimension_hash) as with_hash\n            FROM processing.processed_dimensions\n        \"\"\")\n        \n        result = cur.fetchone()\n        total, with_hash = result\n        \n        if total == 0:\n            raise Exception(\n                \"❌ No data found in processing.processed_dimensions! \"\n                \"Please run scripts 10-11 first.\"\n            )\n        \n        if with_hash == 0:\n            raise Exception(\n                \"❌ No dimension_hash values found in processing.processed_dimensions! \"\n                \"Please run script 11 first.\"\n            )\n        \n        if with_hash < total:\n            logger.warning(f\"⚠️ {total - with_hash} dimension records missing dimension_hash values\")\n        \n        logger.info(\"✅ All required tables exist and contain data\")\n\ndef load_processed_dimensions():\n    \"\"\"Load processed dimensions with dimension metadata\"\"\"\n    with get_db_conn() as conn:\n        # Join processed dimensions with canonical dimension set for metadata\n        processed_dims = pd.read_sql(\"\"\"\n            SELECT \n                pd.productid,\n                pd.dimension_position,\n                pd.dimension_hash,\n                pd.dimension_name_en,\n                pd.dimension_name_fr,\n                ds.dimension_name_en_slug\n            FROM processing.processed_dimensions pd\n            LEFT JOIN processing.dimension_set ds ON pd.dimension_hash = ds.dimension_hash\n            WHERE pd.dimension_hash IS NOT NULL\n            ORDER BY pd.productid, pd.dimension_position\n        \"\"\", conn)\n        \n        logger.info(f\"📥 Loaded {len(processed_dims)} processed dimension records\")\n        \n        # Check for orphaned dimension_hashes (not in canonical registry)\n        orphaned = processed_dims[processed_dims['dimension_name_en_slug'].isna()]\n        if len(orphaned) > 0:\n            logger.warning(f\"⚠️ Found {len(orphaned)} dimension records with orphaned dimension_hash values\")\n            # Show a few examples\n            for _, row in orphaned.head(3).iterrows():\n                logger.warning(f\"   • Product {row['productid']}, position {row['dimension_position']}: hash {row['dimension_hash']}\")\n        \n        return processed_dims\n\ndef validate_productid_completeness(processed_dims):\n    \"\"\"Validate that all dimension_positions for each productid have dimension_hashes\"\"\"\n    logger.info(\"🔍 Validating dimension completeness by productid...\")\n    \n    validation_issues = []\n    \n    # Group by productid and check for gaps in dimension_position\n    productid_groups = processed_dims.groupby('productid')\n    \n    for productid, group in productid_groups:\n        positions = sorted(group['dimension_position'].tolist())\n        expected_positions = list(range(1, len(positions) + 1))\n        \n        # Check for gaps in dimension positions\n        if positions != expected_positions:\n            validation_issues.append({\n                'productid': productid,\n                'type': 'position_gaps',\n                'expected_positions': expected_positions,\n                'actual_positions': positions,\n                'message': f\"Product {productid} has gaps in dimension positions\"\n            })\n        \n        # Check for missing dimension_hashes\n        missing_hashes = group[group['dimension_hash'].isna()]\n        if len(missing_hashes) > 0:\n            validation_issues.append({\n                'productid': productid,\n                'type': 'missing_hash',\n                'missing_positions': missing_hashes['dimension_position'].tolist(),\n                'message': f\"Product {productid} has {len(missing_hashes)} positions without dimension_hash\"\n            })\n    \n    if validation_issues:\n        logger.warning(f\"⚠️ Found {len(validation_issues)} validation issues\")\n        # Log first few issues\n        for issue in validation_issues[:5]:\n            logger.warning(f\"   • {issue['message']}\")\n        if len(validation_issues) > 5:\n            logger.warning(f\"   • ... and {len(validation_issues) - 5} more issues\")\n    else:\n        logger.success(\"✅ All productids have complete dimension mappings\")\n    \n    return validation_issues\n\ndef populate_cube_dimension_mappings(processed_dims):\n    \"\"\"Populate cube.cube_dimension_map table one productid at a time\"\"\"\n    logger.info(\"🚀 Starting cube dimension registry population...\")\n    \n    # Get unique productids to process\n    productids = processed_dims['productid'].unique()\n    logger.info(f\"📊 Processing {len(productids)} unique productids\")\n    \n    successful_updates = 0\n    failed_updates = 0\n    total_mappings_inserted = 0\n    \n    with get_db_conn() as conn:\n        cur = conn.cursor()\n        \n        for i, productid in enumerate(productids, 1):\n            try:\n                # Get dimensions for this productid\n                productid_dims = processed_dims[processed_dims['productid'] == productid].copy()\n                \n                if len(productid_dims) == 0:\n                    logger.warning(f\"⚠️ No dimensions found for productid {productid}\")\n                    continue\n                \n                # Validate this productid has complete data\n                missing_hashes = productid_dims[productid_dims['dimension_hash'].isna()]\n                if len(missing_hashes) > 0:\n                    logger.warning(f\"⚠️ Skipping productid {productid}: {len(missing_hashes)} positions missing dimension_hash\")\n                    failed_updates += 1\n                    continue\n                \n                # Step 1: Delete existing mappings for this productid\n                cur.execute(\"\"\"\n                    DELETE FROM cube.cube_dimension_map \n                    WHERE productid = %s\n                \"\"\", (int(productid),))\n                \n                deleted_count = cur.rowcount\n                \n                # Step 2: Insert new mappings\n                inserted_count = 0\n                for _, row in productid_dims.iterrows():\n                    cur.execute(\"\"\"\n                        INSERT INTO cube.cube_dimension_map (\n                            productid, dimension_position, dimension_hash,\n                            dimension_name_en, dimension_name_fr, dimension_name_slug\n                        ) VALUES (%s, %s, %s, %s, %s, %s)\n                    \"\"\", (\n                        int(row['productid']),\n                        int(row['dimension_position']),\n                        row['dimension_hash'],\n                        row['dimension_name_en'],\n                        row['dimension_name_fr'],\n                        row['dimension_name_en_slug']\n                    ))\n                    inserted_count += 1\n                \n                # Step 3: Validate insertion\n                cur.execute(\"\"\"\n                    SELECT COUNT(*) FROM cube.cube_dimension_map \n                    WHERE productid = %s\n                \"\"\", (int(productid),))\n                \n                final_count = cur.fetchone()[0]\n                \n                if final_count != len(productid_dims):\n                    logger.warning(f\"⚠️ Validation failed for productid {productid}: \"\n                                   f\"expected {len(productid_dims)}, got {final_count}\")\n                    failed_updates += 1\n                else:\n                    successful_updates += 1\n                    total_mappings_inserted += inserted_count\n                \n                # Commit after each productid\n                conn.commit()\n                \n                # Progress logging\n                if i % 100 == 0 or i == len(productids):\n                    logger.info(f\"📈 Progress: {i}/{len(productids)} productids processed \"\n                               f\"({successful_updates} successful, {failed_updates} failed)\")\n                \n            except Exception as e:\n                logger.error(f\"❌ Failed to process productid {productid}: {e}\")\n                failed_updates += 1\n                conn.rollback()\n                continue\n    \n    logger.success(f\"✅ Cube dimension registry population complete!\")\n    logger.success(f\"   • {successful_updates:,} productids successfully updated\")\n    logger.success(f\"   • {total_mappings_inserted:,} total dimension mappings inserted\")\n    \n    if failed_updates > 0:\n        logger.warning(f\"⚠️ {failed_updates} productids failed to update\")\n    \n    return successful_updates, failed_updates, total_mappings_inserted\n\ndef validate_final_mappings():\n    \"\"\"Validate the final cube dimension mappings\"\"\"\n    logger.info(\"🔍 Validating final cube dimension mappings...\")\n    \n    with get_db_conn() as conn:\n        # Overall statistics\n        stats = pd.read_sql(\"\"\"\n            SELECT \n                COUNT(*) as total_mappings,\n                COUNT(DISTINCT productid) as unique_productids,\n                COUNT(DISTINCT dimension_hash) as unique_dimensions,\n                COUNT(DISTINCT (productid, dimension_position)) as unique_positions\n            FROM cube.cube_dimension_map\n        \"\"\", conn)\n        \n        row = stats.iloc[0]\n        logger.info(f\"📊 Final statistics:\")\n        logger.info(f\"   • {row['total_mappings']:,} total dimension mappings\")\n        logger.info(f\"   • {row['unique_productids']:,} unique productids\")\n        logger.info(f\"   • {row['unique_dimensions']:,} unique dimension_hashes\")\n        logger.info(f\"   • {row['unique_positions']:,} unique (productid, dimension_position) pairs\")\n        \n        # Check for duplicate mappings (should not exist due to primary key)\n        duplicates = pd.read_sql(\"\"\"\n            SELECT productid, dimension_position, COUNT(*) as count\n            FROM cube.cube_dimension_map\n            GROUP BY productid, dimension_position\n            HAVING COUNT(*) > 1\n        \"\"\", conn)\n        \n        if len(duplicates) > 0:\n            logger.error(f\"❌ Found {len(duplicates)} duplicate mappings!\")\n            for _, dup in duplicates.head(5).iterrows():\n                logger.error(f\"   • Product {dup['productid']}, position {dup['dimension_position']}: {dup['count']} entries\")\n        else:\n            logger.success(\"✅ No duplicate mappings found\")\n        \n        # Check for orphaned dimension_hashes\n        orphaned = pd.read_sql(\"\"\"\n            SELECT DISTINCT cdm.dimension_hash\n            FROM cube.cube_dimension_map cdm\n            LEFT JOIN processing.dimension_set ds ON cdm.dimension_hash = ds.dimension_hash\n            WHERE ds.dimension_hash IS NULL\n        \"\"\", conn)\n        \n        if len(orphaned) > 0:\n            logger.warning(f\"⚠️ Found {len(orphaned)} orphaned dimension_hashes in mappings\")\n        else:\n            logger.success(\"✅ All dimension_hashes have corresponding canonical definitions\")\n        \n        # Most common dimensions\n        top_dimensions = pd.read_sql(\"\"\"\n            SELECT \n                cdm.dimension_hash,\n                cdm.dimension_name_en,\n                COUNT(*) as usage_count\n            FROM cube.cube_dimension_map cdm\n            GROUP BY cdm.dimension_hash, cdm.dimension_name_en\n            ORDER BY usage_count DESC\n            LIMIT 5\n        \"\"\", conn)\n        \n        logger.info(\"🏆 Top 5 most used dimensions:\")\n        for _, dim in top_dimensions.iterrows():\n            logger.info(f\"   • {dim['dimension_name_en']}: {dim['usage_count']:,} uses\")\n\ndef generate_summary_report():\n    \"\"\"Generate a comprehensive summary report\"\"\"\n    logger.info(\"📋 Generating summary report...\")\n    \n    with get_db_conn() as conn:\n        # Coverage analysis\n        coverage = pd.read_sql(\"\"\"\n            SELECT \n                'Processing Dimensions' as source,\n                COUNT(DISTINCT productid) as productids,\n                COUNT(*) as dimension_count\n            FROM processing.processed_dimensions\n            WHERE dimension_hash IS NOT NULL\n            \n            UNION ALL\n            \n            SELECT \n                'Cube Registry' as source,\n                COUNT(DISTINCT productid) as productids,\n                COUNT(*) as dimension_count\n            FROM cube.cube_dimension_map\n        \"\"\", conn)\n        \n        logger.info(\"📈 Coverage Summary:\")\n        for _, row in coverage.iterrows():\n            logger.info(f\"   • {row['source']}: {row['productids']:,} productids, {row['dimension_count']:,} dimensions\")\n        \n        # Check for missing productids\n        missing_products = pd.read_sql(\"\"\"\n            SELECT DISTINCT pd.productid\n            FROM processing.processed_dimensions pd\n            WHERE pd.dimension_hash IS NOT NULL\n            AND pd.productid NOT IN (\n                SELECT DISTINCT productid FROM cube.cube_dimension_map\n            )\n            LIMIT 10\n        \"\"\", conn)\n        \n        if len(missing_products) > 0:\n            logger.warning(f\"⚠️ {len(missing_products)} productids from processing not in registry\")\n            if len(missing_products) <= 10:\n                logger.warning(f\"   Missing: {missing_products['productid'].tolist()}\")\n        else:\n            logger.success(\"✅ All processed productids are represented in the registry\")\n\ndef main():\n    \"\"\"Main cube dimension registry population function\"\"\"\n    try:\n        logger.info(\"🚀 Starting cube dimension registry mapping population...\")\n        \n        # Verify prerequisites\n        check_required_tables()\n        \n        # Load processed dimensions\n        processed_dims = load_processed_dimensions()\n        \n        # Validate data completeness\n        validation_issues = validate_productid_completeness(processed_dims)\n        \n        if validation_issues:\n            logger.warning(f\"⚠️ Proceeding with {len(validation_issues)} validation issues\")\n        \n        # Populate cube dimension mappings\n        successful, failed, total_inserted = populate_cube_dimension_mappings(processed_dims)\n        \n        # Validate final results\n        validate_final_mappings()\n        \n        # Generate summary report\n        generate_summary_report()\n        \n        logger.success(\"🎉 Cube dimension registry population complete!\")\n        logger.info(f\"📝 Registry table: cube.cube_dimension_map\")\n        logger.info(f\"📈 Total mappings: {total_inserted:,}\")\n        \n        if failed > 0:\n            logger.warning(f\"⚠️ Note: {failed} productids failed to update - check logs for details\")\n        \n    except Exception as e:\n        logger.exception(f\"❌ Cube dimension registry population failed: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "README.md": {
            "size": 8284,
            "modified": "2025-06-04T08:22:23.643861",
            "type": "file",
            "content": "4 June 2025\n\nThis project implements a modular ETL (Extract, Transform, Load) pipeline to download, track, and normalize publicly available data cubes from Statistics Canada. Cube data and metadata are retrieved via the Web Data Service (WDS) API, stored in a structured raw file system, and ingested into a PostgreSQL-based data warehouse. A dimension registry deduplicates and harmonizes dimension definitions and member labels across products, enabling cross-cube integration and long-term schema consistency. The system is designed for reproducibility, traceability, and scalability, with future extensions planned for harmonized time series, metadata enrichment, and client-facing analytics tools.\n\n---\n\n**01_fetch_spine.py — Retrieve and Archive Master Cube List**  \nThis script downloads the full list of available data cubes from Statistics Canada's `getAllCubesListLite` endpoint and stores it as a versioned JSON file in the metadata archive. It calculates a hash of the file contents to detect duplicates and prevent redundant storage. If the hash is new, the script saves the file to disk and logs its metadata into the `raw_files.manage_spine_raw_files` table, deactivating any previous entries. This ensures a traceable, deduplicated record of spine snapshots for downstream processing.\n\n**02_spine_etl.py — Load Spine Metadata into Warehouse**  \nThis ETL script ingests the most recent Statistics Canada spine metadata file into the warehouse. It retrieves the active metadata file path from the `manage_spine_raw_files` table, reads and stages the JSON content using DuckDB, and normalizes it into structured views (`cube`, `cube_subject`, `cube_survey`). These views are then loaded into the corresponding PostgreSQL `spine` schema tables after truncation. The script ensures consistent data structure and supports refreshes by replacing old records with the latest snapshot.\n\n**03_populate_cube_status.py — Initialize cube_status for New Cubes**  \nThis script populates the `raw_files.cube_status` table with entries for all `productid`s in the `spine.cube` table that are not already present. Each new entry is initialized with `download_pending = TRUE` to flag them for initial download. This ensures that newly added cubes are picked up by the cube fetcher script. Existing entries are left unchanged.\n\n**04_update_cube_status.py — Detect Cube Updates and Flag Downloads**  \nThis script checks for updates to StatCan data cubes by querying the `getChangedCubeList(date)` endpoint for each date since the last recorded download. Detected changes are logged in the `raw_files.changed_cubes_log` table without duplication. If a cube's `change_date` is more recent than its `last_download`, it is marked `download_pending = TRUE` in the `raw_files.cube_status` table. The script includes logic to account for StatCan’s 08:30 EST release time and introduces a delay between API calls to reduce server load.\n\n**05_fetch_cubes.py — Download and Log StatCan Cube Files**  \nThis script automates downloading StatCan data cubes using the `getFullTableDownloadCSV` endpoint. It checks the `raw_files.cube_status` table for cubes flagged `download_pending`, fetches each cube as a `.zip`, computes a hash for deduplication, and stores it under `/app/raw/cubes/`. Metadata (hash, path, timestamp) is recorded in `raw_files.manage_cube_raw_files`, while `cube_status` is updated to mark completion. Existing active files are deactivated if superseded. Duplicate files are skipped. A 2-second delay is added between downloads to avoid hammering the server.\n\n**06_verify_cube_files.py — Validate Raw Cube Files**  \nThis script checks that every active cube file listed in `raw_files.manage_cube_raw_files`:  \n1) Exists at the specified `storage_location`;  \n2) Matches the expected SHA-256 hash.  \nIf a file is missing or fails the hash check:  \n- It is deleted (if corrupt);  \n- Its database record is removed;  \n- The corresponding `download_pending` flag in `cube_status` is reset to TRUE, triggering a re-download.  \nThis ensures integrity of the cube archive and guards against partial or corrupted downloads.\n\n**07_populate_metadata_status.py — Initialize Metadata Download Status**  \nThis script populates the `raw_files.metadata_status` table by inserting all `productid`s from `spine.cube` that don’t already exist in `metadata_status`. Each new entry is marked with `download_pending = TRUE`, flagging it for metadata download. This ensures all cubes in the spine are tracked for metadata ingestion. Useful for bootstrapping or syncing the metadata status table after new cube entries are added.\n\n**08_fetch_metadata.py — Download Metadata for StatCan Cubes**  \nThis script fetches bilingual metadata for each StatCan cube using the `getCubeMetadata` endpoint. It checks `raw_files.metadata_status` for cubes marked `download_pending`, downloads their metadata in JSON format, stores the file with a content-based hash, and updates tracking tables:  \n- Logs each file in `raw_files.manage_metadata_raw_files`,  \n- Updates `last_file_hash` and `last_download` in `metadata_status`,  \n- Duplicate downloads are skipped.  \nFiles are saved to `/app/raw/metadata`. A 1-second delay is used between requests to avoid overloading the API.\n\n**09_load_raw_dimensions.py — Ingest Dimension Metadata**  \nThis script parses and ingests dimension definitions and member details from previously downloaded metadata files stored on disk.  \n- Loads `productid` and associated `last_file_hash` from `raw_files.metadata_status`.  \n- For each metadata JSON file:  \n  - Inserts dimension definitions into `dictionary.raw_dimension`;  \n  - Inserts dimension members into `dictionary.raw_member`;  \n  - Uses `ON CONFLICT DO NOTHING` to avoid duplicate insertions;  \n  - Supports partial failure by continuing to next file on error.  \nThis is the main pipeline step for getting raw dimension metadata into the warehouse for later normalization.\n\n**10_dimension_member_keys.py — Inspect Metadata Key Usage**  \nThis diagnostic script scans all downloaded StatCan cube metadata files and tallies the frequency of keys used within:  \n- Dimension definitions;  \n- Member entries.  \nThe script:  \n- Iterates over JSON files in the metadata directory;  \n- Extracts and counts the keys present in dimension and member dictionaries;  \n- Outputs a ranked list of observed keys and their frequencies.  \nUseful for:  \n- Schema discovery;  \n- Debugging inconsistencies in raw metadata;  \n- Informing the design of database tables and normalization logic.\n\n**11_build_dimension_registry.py — Construct Harmonized Dimension Registry**  \nThis script deduplicates and normalizes raw StatCan dimension metadata into a harmonized registry across cubes.\n\nKey steps:  \n1. Hashes code-label combinations (`member_id`, label, parent ID, UOM) to form `member_hash`.  \n2. Aggregates member hashes into a `dimension_hash` per dimension position and product.  \n3. Selects the most common English label per code to assign canonical names.  \n4. Computes metadata flags:  \n   - `is_total` (label contains “total”),  \n   - `is_grabbag` (name includes “characteristics” or “other”),  \n   - `is_tree` (any parent-child hierarchy),  \n   - `is_exclusive` (placeholder).  \n5. Inserts cleaned data into:  \n   - `dictionary.dimension_set`,  \n   - `dictionary.dimension_set_member`,  \n   - `cube.cube_dimension_map`.  \n\nThe result is a normalized set of reusable dimension definitions suitable for harmonizing cubes and enabling cross-cube analytics.\n\n**12_assign_base_names.py — Label Normalization for Deduplication**  \nThis script computes a normalized \"base name\" for each dimension member in the `dictionary.dimension_set_member` table to support deduplication and harmonization.\n\nKey steps:  \n- Tokenizes English labels using NLTK’s tokenizer;  \n- Filters out stopwords and non-alphabetic tokens;  \n- Normalizes remaining words to lowercase and forms a deterministic, sorted token string as the `base_name`;  \n- Updates the database with the computed `base_name` for each `(dimension_hash, member_id)` pair.\n\n**Purpose:** Enables comparison and grouping of semantically similar member labels across dimensions and cubes.  \nA foundation for harmonization and cross-cube integration.\n\n"
          },
          "dev_ingest_cube.py": {
            "size": 13513,
            "modified": "2025-06-06T11:13:10.367600",
            "type": "file",
            "content": "\"\"\"\nCube CSV Ingestion Script\nIngests StatCan cube data from downloaded ZIP files into cube-specific tables.\n\nProcess:\n1. Check processing.cube_ingestion_status for pending cubes\n2. Extract and parse CSV from ZIP files\n3. Create dynamic table schema based on cube dimensions\n4. Parse REF_DATE to standardized date format\n5. Transform VALUES using scalar factors and decimals\n6. Insert data and update ingestion status\n\"\"\"\n\nimport os\nimport re\nimport zipfile\nimport pandas as pd\nimport psycopg2\nfrom pathlib import Path\nfrom datetime import datetime, date\nfrom loguru import logger\nfrom statcan.tools.config import DB_CONFIG\n\nlogger.add(\"/app/logs/cube_csv_ingest.log\", rotation=\"10 MB\", retention=\"7 days\")\n\n# Scalar factor lookup based on StatCan documentation\nSCALAR_FACTORS = {\n    0: 1,           # units\n    1: 10,          # tens  \n    2: 100,         # hundreds\n    3: 1000,        # thousands\n    4: 10000,       # tens of thousands\n    5: 100000,      # hundreds of thousands\n    6: 1000000,     # millions\n    7: 10000000,    # tens of millions\n    8: 100000000,   # hundreds of millions\n    9: 1000000000   # billions\n}\n\ndef parse_ref_date(ref_date_str):\n    \"\"\"\n    Parse REF_DATE string to standardized date format.\n    Returns tuple: (date_obj, original_str, interval_type)\n    \"\"\"\n    ref_date_str = str(ref_date_str).strip()\n    \n    # Annual: YYYY\n    if re.match(r'^\\d{4}$', ref_date_str):\n        year = int(ref_date_str)\n        return date(year, 1, 1), ref_date_str, 'annual'\n    \n    # Monthly: YYYY-MM\n    if re.match(r'^\\d{4}-\\d{2}$', ref_date_str):\n        year, month = map(int, ref_date_str.split('-'))\n        return date(year, month, 1), ref_date_str, 'monthly'\n    \n    # Quarterly: YYYY-Q[1-4]\n    if re.match(r'^\\d{4}-Q[1-4]$', ref_date_str):\n        year = int(ref_date_str[:4])\n        quarter = int(ref_date_str[-1])\n        month = (quarter - 1) * 3 + 1\n        return date(year, month, 1), ref_date_str, 'quarterly'\n    \n    # Weekly: YYYY-W[01-53] (approximate - use first day of year + weeks)\n    if re.match(r'^\\d{4}-W\\d{2}$', ref_date_str):\n        year = int(ref_date_str[:4])\n        week = int(ref_date_str[6:])\n        # Approximate: year start + (week-1)*7 days\n        from datetime import timedelta\n        start_date = date(year, 1, 1)\n        week_date = start_date + timedelta(weeks=week-1)\n        return week_date, ref_date_str, 'weekly'\n    \n    # Fiscal year variations: YYYY/YYYY+1 or similar\n    if re.match(r'^\\d{4}/\\d{4}$', ref_date_str):\n        year = int(ref_date_str[:4])\n        return date(year, 4, 1), ref_date_str, 'fiscal_annual'  # Canadian fiscal starts April\n    \n    # Default: try to parse as-is, fall back to string\n    try:\n        parsed_date = pd.to_datetime(ref_date_str).date()\n        return parsed_date, ref_date_str, 'other'\n    except:\n        logger.warning(f\"Could not parse REF_DATE: {ref_date_str}\")\n        return None, ref_date_str, 'unparsed'\n\ndef get_cube_dimensions(productid):\n    \"\"\"Get dimension mapping for a cube from cube_dimension_map.\"\"\"\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        query = \"\"\"\n            SELECT dimension_position, dimension_name_slug, dimension_hash\n            FROM cube.cube_dimension_map \n            WHERE productid = %s \n            ORDER BY dimension_position\n        \"\"\"\n        df = pd.read_sql(query, conn, params=[productid])\n        return df.to_dict('records')\n\ndef create_cube_table(productid, dimensions):\n    \"\"\"Create dynamic cube table based on dimensions.\"\"\"\n    table_name = f\"c{productid}\"\n    \n    # Build column definitions\n    columns = [\n        \"ref_date DATE\",\n        \"ref_date_original TEXT\",\n        \"ref_date_interval_type TEXT\"\n    ]\n    \n    # Add dimension columns\n    for dim in dimensions:\n        col_name = f\"{dim['dimension_name_slug']}_member_id\"\n        columns.append(f\"{col_name} INTEGER\")\n    \n    columns.append(\"value NUMERIC\")\n    \n    # Create table with proper indexes\n    create_sql = f\"\"\"\n        CREATE TABLE IF NOT EXISTS cube_data.{table_name} (\n            {', '.join(columns)},\n            PRIMARY KEY (ref_date, {', '.join(f\"{d['dimension_name_slug']}_member_id\" for d in dimensions)})\n        );\n        \n        CREATE INDEX IF NOT EXISTS {table_name}_ref_date_idx \n        ON cube_data.{table_name} (ref_date);\n        \n        CREATE INDEX IF NOT EXISTS {table_name}_value_idx \n        ON cube_data.{table_name} (value) WHERE value IS NOT NULL;\n    \"\"\"\n    \n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            cur.execute(create_sql)\n            conn.commit()\n    \n    logger.info(f\"✅ Created/verified table cube_data.{table_name}\")\n    return table_name\n\ndef extract_csv_from_zip(zip_path, productid):\n    \"\"\"Extract the data CSV from zip file (ignore metadata CSV).\"\"\"\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        files = zip_ref.namelist()\n        \n        # Look for the main data CSV (not metadata)\n        data_csv = None\n        for file in files:\n            if file.endswith('.csv') and 'metadata' not in file.lower():\n                data_csv = file\n                break\n        \n        if not data_csv:\n            raise ValueError(f\"No data CSV found in {zip_path}\")\n        \n        # Extract to temporary location\n        temp_path = f\"/tmp/{productid}_data.csv\"\n        with zip_ref.open(data_csv) as source, open(temp_path, 'wb') as target:\n            target.write(source.read())\n        \n        return temp_path\n\ndef transform_value(raw_value, scalar_factor_code, decimals):\n    \"\"\"Apply scalar and decimal transformations to raw value.\"\"\"\n    if pd.isna(raw_value) or raw_value == '':\n        return None\n    \n    try:\n        raw_val = float(raw_value)\n        scalar_multiplier = SCALAR_FACTORS.get(scalar_factor_code, 1)\n        decimal_divisor = 10 ** decimals\n        \n        actual_value = raw_val * scalar_multiplier / decimal_divisor\n        return actual_value\n    except (ValueError, TypeError):\n        logger.warning(f\"Could not transform value: {raw_value}\")\n        return None\n\ndef parse_coordinate(coordinate_str, dimensions):\n    \"\"\"Parse coordinate string into dimension member IDs.\"\"\"\n    if pd.isna(coordinate_str):\n        return {}\n    \n    member_ids = coordinate_str.split('.')\n    result = {}\n    \n    for i, dim in enumerate(dimensions):\n        if i < len(member_ids):\n            try:\n                member_id = int(member_ids[i])\n                col_name = f\"{dim['dimension_name_slug']}_member_id\"\n                result[col_name] = member_id\n            except (ValueError, IndexError):\n                logger.warning(f\"Invalid member ID at position {i}: {member_ids}\")\n                result[f\"{dim['dimension_name_slug']}_member_id\"] = None\n        else:\n            result[f\"{dim['dimension_name_slug']}_member_id\"] = None\n    \n    return result\n\ndef get_pending_cubes(limit=None):\n    \"\"\"Get cubes pending ingestion.\"\"\"\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            # Get cubes where file has been downloaded but not ingested\n            cur.execute(\"\"\"\n                SELECT cs.productid, mcrf.storage_location, mcrf.file_hash\n                FROM raw_files.cube_status cs\n                JOIN raw_files.manage_cube_raw_files mcrf \n                    ON cs.productid = mcrf.productid AND mcrf.active = TRUE\n                LEFT JOIN processing.cube_ingestion_status cis \n                    ON cs.productid = cis.productid\n                WHERE cs.download_pending = FALSE \n                    AND cs.last_file_hash IS NOT NULL\n                    AND (cis.ingestion_pending = TRUE OR cis.productid IS NULL)\n                ORDER BY cs.productid\n                LIMIT %s\n            \"\"\", (limit,))\n            return cur.fetchall()\n\ndef update_ingestion_status(productid, file_hash, table_created=True, row_count=0, success=True, notes=None):\n    \"\"\"Update processing.cube_ingestion_status.\"\"\"\n    with psycopg2.connect(**DB_CONFIG) as conn:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                INSERT INTO processing.cube_ingestion_status \n                (productid, last_ingestion, ingestion_pending, last_file_hash, \n                 table_created, row_count, notes)\n                VALUES (%s, NOW(), %s, %s, %s, %s, %s)\n                ON CONFLICT (productid) DO UPDATE SET\n                    last_ingestion = NOW(),\n                    ingestion_pending = %s,\n                    last_file_hash = %s,\n                    table_created = %s,\n                    row_count = %s,\n                    notes = %s\n            \"\"\", (productid, not success, file_hash, table_created, row_count, notes,\n                  not success, file_hash, table_created, row_count, notes))\n            conn.commit()\n\ndef ingest_cube_data(productid, zip_path, file_hash):\n    \"\"\"Main ingestion logic for a single cube.\"\"\"\n    logger.info(f\"🔄 Starting ingestion for cube {productid}\")\n    \n    try:\n        # Get cube dimensions\n        dimensions = get_cube_dimensions(productid)\n        if not dimensions:\n            raise ValueError(f\"No dimensions found for productid {productid}\")\n        \n        # Create cube table\n        table_name = create_cube_table(productid, dimensions)\n        \n        # Extract CSV from ZIP\n        csv_path = extract_csv_from_zip(zip_path, productid)\n        \n        try:\n            # Read CSV data\n            logger.info(f\"📖 Reading CSV data for {productid}\")\n            df = pd.read_csv(csv_path)\n            \n            # Verify required columns exist\n            required_cols = ['REF_DATE', 'COORDINATE', 'VALUE', 'SCALAR_FACTOR', 'DECIMALS']\n            missing_cols = [col for col in required_cols if col not in df.columns]\n            if missing_cols:\n                raise ValueError(f\"Missing required columns: {missing_cols}\")\n            \n            # Transform data\n            logger.info(f\"🔄 Transforming {len(df)} rows for {productid}\")\n            \n            # Parse REF_DATE\n            df[['ref_date', 'ref_date_original', 'ref_date_interval_type']] = df['REF_DATE'].apply(\n                lambda x: pd.Series(parse_ref_date(x))\n            )\n            \n            # Parse coordinates into dimension columns\n            coordinate_data = df.apply(\n                lambda row: parse_coordinate(row['COORDINATE'], dimensions), axis=1\n            )\n            coordinate_df = pd.DataFrame(coordinate_data.tolist())\n            df = pd.concat([df, coordinate_df], axis=1)\n            \n            # Transform values\n            df['value'] = df.apply(\n                lambda row: transform_value(row['VALUE'], row.get('SCALAR_FACTOR', 0), row.get('DECIMALS', 0)),\n                axis=1\n            )\n            \n            # Select final columns for insertion\n            final_columns = ['ref_date', 'ref_date_original', 'ref_date_interval_type'] + \\\n                           [f\"{dim['dimension_name_slug']}_member_id\" for dim in dimensions] + \\\n                           ['value']\n            \n            df_final = df[final_columns].copy()\n            \n            # Remove rows with null dates or all null dimension values\n            df_final = df_final.dropna(subset=['ref_date'])\n            \n            # Insert data\n            logger.info(f\"💾 Inserting {len(df_final)} rows into {table_name}\")\n            \n            with psycopg2.connect(**DB_CONFIG) as conn:\n                # Truncate existing data for this cube\n                with conn.cursor() as cur:\n                    cur.execute(f\"TRUNCATE TABLE cube_data.{table_name}\")\n                \n                # Insert new data\n                df_final.to_sql(\n                    table_name, \n                    conn, \n                    schema='cube_data',\n                    if_exists='append', \n                    index=False,\n                    method='multi'\n                )\n                conn.commit()\n            \n            row_count = len(df_final)\n            logger.success(f\"✅ Ingested {row_count} rows for cube {productid}\")\n            \n            # Update status\n            update_ingestion_status(productid, file_hash, True, row_count, True)\n            \n        finally:\n            # Cleanup temp file\n            if os.path.exists(csv_path):\n                os.remove(csv_path)\n    \n    except Exception as e:\n        logger.error(f\"❌ Failed to ingest cube {productid}: {e}\")\n        update_ingestion_status(productid, file_hash, False, 0, False, str(e))\n        raise\n\ndef main():\n    logger.info(\"🚀 Starting cube CSV ingestion...\")\n    \n    try:\n        # Get pending cubes\n        pending_cubes = get_pending_cubes(limit=5)  # Start with small batch\n        \n        if not pending_cubes:\n            logger.info(\"🎉 No cubes pending ingestion\")\n            return\n        \n        logger.info(f\"📋 Found {len(pending_cubes)} cubes to ingest\")\n        \n        success_count = 0\n        for productid, zip_path, file_hash in pending_cubes:\n            try:\n                ingest_cube_data(productid, zip_path, file_hash)\n                success_count += 1\n            except Exception as e:\n                logger.error(f\"💥 Cube {productid} ingestion failed: {e}\")\n                continue\n        \n        logger.success(f\"✅ Ingestion complete: {success_count}/{len(pending_cubes)} successful\")\n        \n    except Exception as e:\n        logger.exception(f\"💥 Ingestion pipeline failed: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n"
          },
          "ingest_cube_csv.py": {
            "size": 2267,
            "modified": "2025-06-05T03:45:22.205780",
            "type": "file",
            "content": "import os\nimport zipfile\nimport pandas as pd\nimport psycopg2\nimport csv\nfrom io import StringIO\nfrom statcan.tools.config import DB_CONFIG\n\ndef get_zip_path_from_db(conn, product_id):\n    with conn.cursor() as cur:\n        cur.execute(\"\"\"\n            SELECT storage_location\n            FROM raw_files.manage_cube_raw_files\n            WHERE productid = %s AND active = true;\n        \"\"\", (product_id,))\n        row = cur.fetchone()\n        if not row:\n            raise ValueError(f\"No active file for productId {product_id}\")\n        return row[0]\n\ndef get_csv_from_zip(zip_path):\n    with zipfile.ZipFile(zip_path, 'r') as z:\n        csv_file = [f for f in z.namelist() if f.endswith('.csv')][0]\n        with z.open(csv_file) as f:\n            df = pd.read_csv(f)\n    return df\n\ndef infer_sql_type(dtype):\n    if pd.api.types.is_integer_dtype(dtype):\n        return \"BIGINT\"\n    elif pd.api.types.is_float_dtype(dtype):\n        return \"DOUBLE PRECISION\"\n    elif pd.api.types.is_bool_dtype(dtype):\n        return \"BOOLEAN\"\n    else:\n        return \"TEXT\"\n\ndef create_table(conn, df, product_id):\n    cols = df.columns\n    dtypes = [infer_sql_type(df[col]) for col in cols]\n    col_defs = \",\\n  \".join(f'\"{col}\" {dtype}' for col, dtype in zip(cols, dtypes))\n    ddl = f'''\n    DROP TABLE IF EXISTS cube_data.\"{product_id}\";\n    CREATE TABLE cube_data.\"{product_id}\" (\n      {col_defs}\n    );\n    '''\n    with conn.cursor() as cur:\n        cur.execute(ddl)\n    conn.commit()\n\ndef load_data(conn, df, product_id):\n    with conn.cursor() as cur:\n        output = StringIO()\n        df.to_csv(output, index=False, header=False, quoting=csv.QUOTE_MINIMAL)\n        output.seek(0)\n        cols = ','.join(f'\"{c}\"' for c in df.columns)\n        cur.copy_expert(f'COPY cube_data.\"{product_id}\" ({cols}) FROM STDIN WITH CSV', output)\n    conn.commit()\n\ndef ingest_by_product_id(product_id):\n    conn = psycopg2.connect(**DB_CONFIG)\n    zip_path = get_zip_path_from_db(conn, product_id)\n    df = get_csv_from_zip(zip_path)\n    create_table(conn, df, product_id)\n    load_data(conn, df, product_id)\n    conn.close()\n    print(f\"✅ Loaded cube {product_id} from {zip_path} with {len(df)} rows.\")\n\n# Example usage\nif __name__ == \"__main__\":\n    ingest_by_product_id(\"13100653\")\n\n"
          },
          "sample.txt": {
            "size": 123,
            "modified": "2025-06-04T23:56:40.135069",
            "type": "file",
            "content": "['13100653', '13100667', '27100024', '27100157', '33100701',\n '35100007', '36100208', '37100261', '46100053', '98100060']\n\n"
          }
        }
      },
      "tools": {
        "type": "directory",
        "contents": {
          "__init__.py": {
            "size": 0,
            "modified": "2025-05-26T05:15:44.034259",
            "type": "file",
            "content": ""
          },
          "file_logger.py": {
            "size": 1497,
            "modified": "2025-05-28T07:53:57.075875",
            "type": "file",
            "content": "import os\nimport hashlib\nimport datetime\n\ndef compute_sha256(data: bytes) -> str:\n    \"\"\"Compute SHA-256 hash of byte content.\"\"\"\n    return hashlib.sha256(data).hexdigest()\n\ndef file_exists_with_same_hash(path: str, new_bytes: bytes) -> bool:\n    \"\"\"Check if a file exists at `path` and matches the hash of `new_bytes`.\"\"\"\n    if not os.path.exists(path):\n        return False\n    with open(path, \"rb\") as f:\n        existing_bytes = f.read()\n    return hashlib.sha256(existing_bytes).hexdigest() == hashlib.sha256(new_bytes).hexdigest()\n\ndef save_file_if_changed(path: str, file_bytes: bytes) -> bool:\n    \"\"\"\n    Save file only if it's missing or has changed.\n    Returns True if file was saved or updated.\n    \"\"\"\n    if file_exists_with_same_hash(path, file_bytes):\n        return False\n    with open(path, \"wb\") as f:\n        f.write(file_bytes)\n    return True\n\ndef log_file_ingest(conn, productid: str, file_path: str, file_hash: str, source_url: str = None):\n    \"\"\"Insert a log record into statcan.ingest_log (skip if already exists).\"\"\"\n    with conn.cursor() as cur:\n        cur.execute(\"\"\"\n            INSERT INTO archive.ingest_log (productid, status, file_path, file_hash, notes, attempt_time)\n            VALUES (%s, %s, %s, %s, %s, %s)\n            ON CONFLICT (productid) DO NOTHING\n        \"\"\", (\n            productid,\n            \"success\",\n            file_path,\n            file_hash,\n            source_url,\n            datetime.datetime.now()\n        ))\n    conn.commit()\n\n"
          }
        }
      }
    }
  }
}