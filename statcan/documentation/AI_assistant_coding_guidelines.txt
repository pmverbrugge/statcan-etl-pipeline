# Coding Guidelines for AI Development Assistance
## Statistics Canada Public Data ETL Pipeline

This document provides coding standards and architectural guidelines for AI assistants working on the StatCan ETL pipeline. These guidelines ensure consistency, maintainability, and optimal performance across the codebase.

## Project Context

**Mission**: Build a comprehensive Canadian statistical intelligence platform that transforms all Statistics Canada public data into an AI-powered analytics ecosystem enabling cross-domain insights impossible through StatCan's native tools.

**Architecture**: Containerized ETL pipeline with PostgreSQL data warehouse, featuring sophisticated dimension registry for cross-cube analytics and harmonized time series analysis.

## Core Architectural Principles

### Database Processing Hierarchy
1. **PostgreSQL-First**: Use direct PostgreSQL operations for database-to-database ETL work
2. **DuckDB for Analytics**: Use DuckDB for file processing, complex analytics, or when working with external data sources
3. **Pandas Minimally**: Reserve pandas only for Python-specific operations that cannot be efficiently done in SQL

**Rationale**: PostgreSQL-first eliminates external dependencies, leverages existing database extensions (pgcrypto, PostGIS), and provides simpler error handling. DuckDB excels at file processing and memory-efficient analytics. Pandas adds overhead and complexity for database operations.

### Data Integration Strategy
- **Unified Fact Tables**: All ~9,000 cubes flow into normalized fact tables (not individual cube tables)
- **Dimension Normalization**: Create reusable dimension sets spanning multiple cubes
- **Hash-Based Change Detection**: Enable incremental updates while preventing duplicate processing
- **Coordinate Preservation**: Maintain StatCan's native addressing while adding normalized member IDs

### Container Architecture
- **ETL Container**: Cannot modify database structure (no DDL permissions)
- **Database Container**: Separate container with full schema control
- **Separation of Concerns**: ETL scripts focus on data processing; DDL changes require separate database scripts

## Code Standards

### Script Header Format
Every script must include this standardized header:

```python
#!/usr/bin/env python3
"""
Statcan Public Data ETL Pipeline
Script: [script_name].py
Date: [YYYY-MM-DD]
Author: Paul Verbrugge with [AI_Model_Name_Version]

[Brief description of script purpose and functionality]

[Detailed description of processing logic and approach]

Key Operations:
- [List key operations performed]
- [Processing steps]
- [Output descriptions]

Dependencies:
- [Required input tables/files]
- [Prerequisite scripts that must run first]
- [External dependencies]
"""
```

### Technology Preferences

#### Database Operations
```python
# ‚úÖ PREFERRED: Direct PostgreSQL operations
with psycopg2.connect(**DB_CONFIG) as conn:
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO target_table 
        SELECT processed_data FROM source_table
        WHERE conditions
    """)

# ‚ùå AVOID: Pandas for database-to-database operations
df = pd.read_sql("SELECT * FROM source_table", conn)
df.to_sql("target_table", conn)
```

#### Data Processing
```python
# ‚úÖ PREFERRED: SQL-based transformations
cur.execute("""
    UPDATE table SET 
    normalized_field = LOWER(TRIM(source_field)),
    hash_field = SUBSTR(ENCODE(DIGEST(key_fields, 'sha256'), 'hex'), 1, 12)
""")

# ‚úÖ ACCEPTABLE: DuckDB for complex analytics or file processing
duck_conn.execute("""
    CREATE TABLE processed AS
    SELECT * FROM read_csv_auto('/path/to/file.csv')
    WHERE complex_analytical_conditions
""")

# ‚ùå AVOID: Pandas unless absolutely necessary
df['normalized'] = df['source'].str.lower().str.strip()
df['hash'] = df.apply(lambda x: hash_function(x), axis=1)
```

## Minimal Logging Guidelines

### Philosophy
ETL scripts should log **only what operators need to know** - major milestones, failures, and actionable warnings. Excessive logging creates noise, impacts performance, and makes it harder to identify real issues.

### Core Logging Principles

#### 1. **Major Milestones Only**
Log significant process boundaries and completion states:
```python
logger.info("üöÄ Starting dimension metadata ingestion...")
logger.info("üìä Processing 1,247 metadata files...")
logger.success("‚úÖ Metadata ingestion complete: 1,235 processed, 12 failed")
```

#### 2. **Always Log Failures**
Every error should be logged with sufficient context for debugging:
```python
logger.error(f"‚ùå Failed to process product {productid}: {str(e)}")
logger.exception(f"‚ùå Database connection failed: {e}")  # Includes stack trace
```

#### 3. **Progress Only for Long Operations**
For processes taking >30 seconds, log progress at reasonable intervals:
```python
# Log every 1000 items OR every 30 seconds, whichever is less frequent
if processed_count % 1000 == 0:
    logger.info(f"üìà Progress: {processed_count:,}/{total_count:,} processed")
```

#### 4. **Actionable Warnings Only**
Only warn about issues that require operator attention:
```python
# ‚úÖ Good - actionable
logger.warning(f"‚ö†Ô∏è High failure rate: {failure_rate:.1f}% - check API connectivity")

# ‚ùå Avoid - not actionable
logger.warning(f"‚ö†Ô∏è Product {productid} has no French title")
```

### What NOT to Log

#### Avoid These Common Over-Logging Patterns:
- **Individual record processing**: Don't log each file/record processed
- **Successful validation steps**: Only log validation failures
- **Debug information**: No function entry/exit logging
- **Data content details**: Don't log specific field values
- **Batch details**: Don't log each batch unless it fails

```python
# ‚ùå EXCESSIVE - Don't do this
logger.info(f"üì• Processing file {filename}")
logger.debug(f"üîç Validating product {productid}")
logger.info(f"üì¶ Processing batch {batch_num}/{total_batches}")
logger.debug(f"‚úÖ Validated {productid}: {validation_result}")
logger.info(f"üíæ Inserted {record_count} records for {productid}")
```

### Standard Logging Configuration
```python
from loguru import logger

# Configure with appropriate rotation for minimal logging
logger.add("/app/logs/script_name.log", rotation="5 MB", retention="7 days")
```

### Emoji Standards for Logging
Use these sparingly and consistently:
- üöÄ **Process start**
- ‚úÖ **Successful completion**
- ‚ùå **Failures/errors**
- ‚ö†Ô∏è **Actionable warnings**
- üìä **Major statistics**
- üìà **Progress (long operations only)**

### Logging Template for ETL Scripts
```python
def main():
    logger.info("üöÄ Starting [process name]...")
    
    try:
        # Get work to do
        items = get_work_items()
        logger.info(f"üìä Processing {len(items)} [items]...")
        
        # Process with minimal logging
        results = process_items(items)
        
        # Log final results
        logger.success(f"‚úÖ [Process name] complete: {results['success']} processed, {results['failed']} failed")
        
        # Warn about concerning failure rates
        if results['failed'] > 0:
            failure_rate = results['failed'] / len(items) * 100
            if failure_rate > 10:  # Only warn if >10% failure rate
                logger.warning(f"‚ö†Ô∏è High failure rate: {failure_rate:.1f}% - investigate logs")
        
    except Exception as e:
        logger.exception(f"‚ùå [Process name] failed: {e}")
        raise

def process_items(items):
    success_count = 0
    failed_count = 0
    
    for i, item in enumerate(items):
        try:
            process_single_item(item)
            success_count += 1
            
            # Progress logging only for long operations
            if len(items) > 1000 and i % 1000 == 0:
                logger.info(f"üìà Progress: {i:,}/{len(items):,} processed")
                
        except Exception as e:
            # Always log individual failures
            logger.error(f"‚ùå Failed to process {item.id}: {str(e)}")
            failed_count += 1
            continue
    
    return {'success': success_count, 'failed': failed_count}
```

### Rule of Thumb for Logging
**If you wouldn't want to see this message in a daily operational summary, don't log it.**

Focus on the information a system operator needs to:
1. Know the process completed successfully
2. Identify and fix failures
3. Monitor system health
4. Understand performance trends

Everything else is just noise.

## Error Handling and Validation

### Comprehensive Validation
```python
def validate_output():
    """Validate processed output for data quality"""
    with psycopg2.connect(**DB_CONFIG) as conn:
        cur = conn.cursor()
        
        # Basic counts
        cur.execute("SELECT COUNT(*) FROM processed_table")
        processed_count = cur.fetchone()[0]
        
        # Data quality checks
        cur.execute("""
            SELECT 
                COUNT(*) FILTER (WHERE key_field IS NULL) as null_keys,
                COUNT(*) FILTER (WHERE hash_field IS NULL) as null_hashes,
                COUNT(DISTINCT hash_field) as unique_hashes
            FROM processed_table
        """)
        
        null_keys, null_hashes, unique_hashes = cur.fetchone()
        
        # Report issues
        if null_keys > 0:
            logger.warning(f"‚ö†Ô∏è Found {null_keys} records with NULL keys")
        
        if null_hashes > 0:
            logger.error(f"‚ùå Found {null_hashes} records with NULL hashes")
            raise Exception("Hash generation failed for some records")
        
        # Success metrics
        dedup_rate = ((processed_count - unique_hashes) / processed_count * 100) if processed_count > 0 else 0
        logger.info(f"üìà Deduplication rate: {dedup_rate:.1f}%")
        logger.success(f"‚úÖ Validation passed: {processed_count:,} records processed")
```

### Performance Optimization

#### Batch Processing
```python
# ‚úÖ Process large datasets in batches
BATCH_SIZE = 1000

for i in range(0, len(items), BATCH_SIZE):
    batch = items[i:i + BATCH_SIZE]
    process_batch(batch)
    
    # Progress logging only if needed
    if len(items) > 5000 and i % 5000 == 0:
        logger.info(f"üìà Progress: {i:,}/{len(items):,} items processed")
```

#### Atomic Operations
```python
# ‚úÖ Use transactions for data consistency
with psycopg2.connect(**DB_CONFIG) as conn:
    try:
        cur = conn.cursor()
        
        # Clear existing data
        cur.execute("TRUNCATE TABLE target_table")
        
        # Insert new data
        cur.execute("INSERT INTO target_table SELECT * FROM staging_table")
        
        # Validate results
        cur.execute("SELECT COUNT(*) FROM target_table")
        final_count = cur.fetchone()[0]
        
        if final_count == 0:
            raise Exception("No data inserted")
        
        conn.commit()
        logger.success(f"‚úÖ Transaction completed: {final_count:,} records")
        
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå Transaction rolled back: {e}")
        raise
```

## ETL Pipeline Patterns

### Hash-Based Deduplication
```python
# Standard pattern for generating consistent hashes
hash_query = """
    SUBSTR(
        ENCODE(
            DIGEST(
                CONCAT(
                    LOWER(TRIM(COALESCE(CAST(key1 AS TEXT), ''))), '|',
                    LOWER(TRIM(COALESCE(key2, ''))), '|',
                    LOWER(TRIM(COALESCE(CAST(key3 AS TEXT), '')))
                ), 
                'sha256'
            ), 
            'hex'
        ), 
        1, 12
    ) as hash_field
"""
```

### File Processing Patterns
```python
def process_files_in_batches(file_list, batch_size=100):
    """Process files in batches for memory efficiency"""
    total_batches = (len(file_list) + batch_size - 1) // batch_size
    
    for i in range(0, len(file_list), batch_size):
        batch = file_list[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        
        try:
            process_file_batch(batch)
        except Exception as e:
            logger.error(f"‚ùå Batch {batch_num} failed: {e}")
            continue  # Continue with next batch
```

### StatCan API Integration
```python
# Rate limiting and error handling for StatCan API
import time
import requests

RATE_LIMIT_DELAY = 2  # seconds
API_TIMEOUT = 30      # seconds
MAX_RETRIES = 3

def call_statcan_api(url, payload=None):
    """Call StatCan API with proper error handling and rate limiting"""
    for attempt in range(MAX_RETRIES):
        try:
            if payload:
                response = requests.post(url, json=payload, timeout=API_TIMEOUT)
            else:
                response = requests.get(url, timeout=API_TIMEOUT)
            
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.Timeout:
            if attempt < MAX_RETRIES - 1:
                time.sleep(RATE_LIMIT_DELAY * (attempt + 1))
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå API request failed: {e}")
            if attempt < MAX_RETRIES - 1:
                time.sleep(RATE_LIMIT_DELAY)
            else:
                raise
    
    # Rate limiting between successful calls
    time.sleep(RATE_LIMIT_DELAY)
```

## Documentation Standards

### Function Documentation
```python
def process_dimensions(productid: int) -> dict:
    """Process dimension metadata for a specific product
    
    Args:
        productid: StatCan 8-digit product identifier
        
    Returns:
        dict: Processing statistics including counts and validation results
        
    Raises:
        Exception: If product has no dimensions or validation fails
        
    Example:
        stats = process_dimensions(13100001)
        logger.info(f"Processed {stats['dimension_count']} dimensions")
    """
```

### Configuration Management
```python
# Use environment-aware configuration
from statcan.tools.config import DB_CONFIG

# Document expected configuration keys
# DB_CONFIG should contain:
# - host: Database hostname
# - database: Database name  
# - user: Database username
# - password: Database password
```

## Testing and Validation Guidelines

### Data Validation Checks
- **Completeness**: Verify all expected records are processed
- **Uniqueness**: Check for duplicate hashes or keys
- **Referential Integrity**: Validate foreign key relationships
- **Data Types**: Ensure proper type conversion
- **Range Validation**: Check for reasonable value ranges

### Performance Benchmarks
- **Processing Speed**: Log processing rates (records/second)
- **Memory Usage**: Monitor for memory leaks in long-running processes
- **Database Load**: Validate query performance on large datasets
- **Error Rates**: Track and minimize processing failures

## Monitoring Integration

Scripts should be **monitorable by external tools** with minimal logs:
- **Exit codes**: 0 for success, non-zero for failure
- **Log patterns**: Consistent emoji patterns for automated parsing
- **Metrics**: Key counts in final success messages
- **Health checks**: Clear success/failure indicators

## AI Assistant Guidelines

When working on this codebase:

1. **Follow the hierarchy**: PostgreSQL-first, then DuckDB, pandas only when necessary
2. **Maintain consistency**: Use established patterns for logging, error handling, and validation
3. **Preserve functionality**: Ensure optimizations produce identical results to original code
4. **Document decisions**: Explain why specific approaches were chosen
5. **Validate thoroughly**: Include comprehensive data quality checks
6. **Think integration**: Consider how changes affect downstream processes
7. **Log minimally**: Focus on operational necessity, not development convenience

## Common Pitfalls to Avoid

- **Over-engineering**: Keep solutions simple and maintainable
- **Pandas overuse**: Don't default to pandas for database operations
- **Missing validation**: Always validate input data and output results
- **Poor error handling**: Ensure graceful failure and meaningful error messages
- **Excessive logging**: Use standardized minimal logging patterns
- **Ignoring constraints**: Respect container separation and DDL restrictions

## Resources

- **StatCan WDS API Documentation**: Web Data Service endpoints and response formats
- **PostgreSQL Documentation**: Focus on pgcrypto and performance optimization
- **DuckDB Documentation**: File processing and analytical functions
- **Project Repository**: `/docs` directory for additional technical documentation

---

*These guidelines evolve with the project. Suggest improvements through issues or pull requests.*
